<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>正则表达式记录</title>
      <link href="/2018/12/04/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%80%BB%E7%BB%93/"/>
      <url>/2018/12/04/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;正则表达式可以在文本数据中通过一系列的规则去寻找自己想要的信息，之前弄过几次，但是掌握的都不是很牢靠，所以想记录一下正则表达式的相关知识，以备后查。</p><h2 id="正则表达式模式"><a href="#正则表达式模式" class="headerlink" title="正则表达式模式"></a>正则表达式模式</h2><p>&emsp;&emsp;正则表达式模式字符串使用特殊的语法来表示一个正则表达式：</p><ul><li>字母和数字表示他们自身。一个正则表达式模式中的字母和数字匹配同样的字符串。</li><li>多数字母和数字前加一个反斜杠时会拥有不同的含义。</li><li>标点符号只有被转义时才匹配自身，否则它们表示特殊的含义。</li><li>反斜杠本身需要使用反斜杠转义。</li><li>由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 <code>r&#39;\t&#39;</code>，等价于 <code>&#39;\\t&#39;</code>)匹配相应的特殊字符。<br>&emsp;&emsp;下表列出了正则表达式模式语法中的特殊元素。如果你使用模式的同时提供了可选的标志参数，某些模式元素的含义会改变。</li></ul><table><thead><tr><th>模式</th><th>描述</th></tr></thead><tbody><tr><td>^</td><td>匹配字符串的开头</td></tr><tr><td>$</td><td>匹配字符串的末尾。</td></tr><tr><td>.</td><td>匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。</td></tr><tr><td>[…]</td><td>用来表示一组字符,单独列出：[amk] 匹配 ‘a’，’m’或’k’</td></tr><tr><td>[^…]</td><td>不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符。</td></tr><tr><td>re*</td><td>匹配0个或多个的表达式。</td></tr><tr><td>re+</td><td>匹配1个或多个的表达式。</td></tr><tr><td>re?</td><td>匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式</td></tr><tr><td>re{n}</td><td>精确匹配 n 个前面表达式。例如， o{2} 不能匹配 “Bob” 中的 “o”，但是能匹配 “food” 中的两个 o。</td></tr><tr><td>re{n,}</td><td>匹配 n 个前面表达式。例如， o{2,} 不能匹配”Bob”中的”o”，但能匹配 “foooood”中的所有 o。”o{1,}” 等价于 “o+”。”o{0,}” 则等价于 “o*”。</td></tr><tr><td>re{n,m}</td><td>匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式</td></tr><tr><td>a｜b</td><td>匹配a或b</td></tr><tr><td>(re)</td><td>匹配括号内的表达式，也表示一个组</td></tr><tr><td>(?imx)</td><td>正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域。</td></tr><tr><td>(?-imx)</td><td>正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域。</td></tr><tr><td>(?: re)</td><td>类似 (…), 但是不表示一个组</td></tr><tr><td>(?imx: re)</td><td>在括号中使用i, m, 或 x 可选标志</td></tr><tr><td>(?-imx: re)</td><td>在括号中不使用i, m, 或 x 可选标志</td></tr><tr><td>(?#…)</td><td>注释.</td></tr><tr><td>(?= re)</td><td>前向肯定界定符。如果所含正则表达式，以 … 表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。</td></tr><tr><td>(?! re)</td><td>前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功</td></tr><tr><td>(?&gt; re)</td><td>匹配的独立模式，省去回溯。</td></tr><tr><td>\w</td><td>匹配字母数字及下划线</td></tr><tr><td>\W</td><td>匹配非字母数字及下划线</td></tr><tr><td>\s</td><td>匹配任意空白字符，等价于 <code>[\t\n\r\f]</code>.</td></tr><tr><td>\S</td><td>匹配任意非空字符</td></tr><tr><td>\d</td><td>匹配任意数字，等价于 [0-9].</td></tr><tr><td>\D</td><td>匹配任意非数字</td></tr><tr><td>\A</td><td>匹配字符串开始</td></tr><tr><td>\Z</td><td>匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。</td></tr><tr><td>\z</td><td>匹配字符串结束</td></tr><tr><td>\G</td><td>匹配最后匹配完成的位置。</td></tr><tr><td>\b</td><td>匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。</td></tr><tr><td>\B</td><td>匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。</td></tr><tr><td>\n, \t, 等.</td><td>匹配一个换行符。匹配一个制表符。等</td></tr><tr><td>\1…\9</td><td>匹配第n个分组的内容。</td></tr><tr><td>\10</td><td>匹配第n个分组的内容，如果它经匹配。否则指的是八进制字符码的表达式。</td></tr></tbody></table><h2 id="正则表达式常用函数"><a href="#正则表达式常用函数" class="headerlink" title="正则表达式常用函数"></a>正则表达式常用函数</h2><h3 id="re-match函数"><a href="#re-match函数" class="headerlink" title="re.match函数"></a>re.match函数</h3><p>&emsp;&emsp;<code>re.match</code>尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。</p><p><strong>函数语法</strong><br><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.<span class="keyword">match</span>(<span class="built_in">pattern</span>, string, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p><em>函数参数说明</em></p><ul><li>pattern:匹配的正则表达式</li><li>string:要匹配的字符串</li><li>flags:标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。</li></ul><blockquote><p>匹配成功re.match方法返回一个匹配的对象，否则返回None。</p></blockquote><p>我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。</p><table><thead><tr><th>匹配对象方法</th><th>描述</th></tr></thead><tbody><tr><td>group(num=0)</td><td>匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。</td></tr><tr><td>groups()</td><td>返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。</td></tr></tbody></table><p><strong>实例</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line">line = <span class="string">"Cats are smarter than dogs"</span></span><br><span class="line"> </span><br><span class="line">matchObj = re.match( <span class="string">r'(.*) are (.*?) .*'</span>, line, re.M|re.I)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> matchObj:</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"matchObj.group() : "</span>, matchObj.group()</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"matchObj.group(1) : "</span>, matchObj.group(<span class="number">1</span>)</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"matchObj.group(2) : "</span>, matchObj.group(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"No match!!"</span></span><br></pre></td></tr></table></figure></p><p><strong>结果</strong><br><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">match</span>Obj.<span class="keyword">group</span>() :  Cats are smarter than dogs</span><br><span class="line"><span class="built_in">match</span>Obj.<span class="keyword">group</span>(<span class="number">1</span>) :  Cats</span><br><span class="line"><span class="built_in">match</span>Obj.<span class="keyword">group</span>(<span class="number">2</span>) :  smarter</span><br></pre></td></tr></table></figure></p><h3 id="re-search方法"><a href="#re-search方法" class="headerlink" title="re.search方法"></a>re.search方法</h3><p>&emsp;&emsp;re.search 扫描整个字符串并返回第一个成功的匹配。</p><p><strong>函数语法</strong><br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.search(pattern, <span class="type">string</span>, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p><em>函数参数说明</em></p><ul><li>pattern:匹配的正则表达式</li><li>string:要匹配的字符串</li><li>flags:标志位，用于控制正则表达式的匹配方式</li></ul><blockquote><p>匹配成功re.search方法返回一个匹配的对象，否则返回None。</p></blockquote><p><strong>实例</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line">line = <span class="string">"Cats are smarter than dogs"</span>;</span><br><span class="line"> </span><br><span class="line">searchObj = re.search( <span class="string">r'(.*) are (.*?) .*'</span>, line, re.M|re.I)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> searchObj:</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"searchObj.group() : "</span>, searchObj.group()</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"searchObj.group(1) : "</span>, searchObj.group(<span class="number">1</span>)</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"searchObj.group(2) : "</span>, searchObj.group(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"Nothing found!!"</span></span><br></pre></td></tr></table></figure></p><p><strong>结果</strong><br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">searchObj.group() :  <span class="type">Cats</span> are smarter than dogs</span><br><span class="line">searchObj.group(<span class="number">1</span>) :  <span class="type">Cats</span></span><br><span class="line">searchObj.group(<span class="number">2</span>) :  <span class="type">smarter</span></span><br></pre></td></tr></table></figure></p><p><strong>re.match与re.search的区别</strong><br>&emsp;&emsp;re.match只匹配<code>字符串的开始</code>，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search会遍历整个字符串，直到找到一个匹配。</p><h3 id="re-sub方法"><a href="#re-sub方法" class="headerlink" title="re.sub方法"></a>re.sub方法</h3><p>&emsp;&emsp;Python 的 re 模块提供了re.sub用于替换字符串中的匹配项。</p><p><strong>语法</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.sub(pattern, repl, string, <span class="attribute">count</span>=0, <span class="attribute">flags</span>=0)</span><br></pre></td></tr></table></figure></p><p><em>参数</em></p><ul><li>pattern:正则中的模式字符串</li><li>repl:替换的字符串，也可以为一个函数</li><li>string:要被替换的原始字符串</li><li>count:模式匹配后替换的最大次数，默认为0替换所有匹配</li></ul><p><strong>实例</strong><br><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/python</span></span><br><span class="line"><span class="meta"># -*- coding: UTF-8 -*-</span></span><br><span class="line"> </span><br><span class="line">import re</span><br><span class="line"> </span><br><span class="line"><span class="meta"># 将匹配的数字乘以 2</span></span><br><span class="line"><span class="function">def <span class="title">double</span>(<span class="params">matched</span>):</span></span><br><span class="line"><span class="function">    <span class="keyword">value</span> </span>= <span class="keyword">int</span>(matched.<span class="keyword">group</span>(<span class="string">'value'</span>))</span><br><span class="line">    <span class="keyword">return</span> str(<span class="keyword">value</span> * <span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">s = <span class="string">'A23G4HFD567'</span></span><br><span class="line">print(re.sub(<span class="string">'(?P&lt;value&gt;\d+)'</span>, <span class="keyword">double</span>, s))</span><br></pre></td></tr></table></figure></p><p><strong>结果</strong><br><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A<span class="number">46</span><span class="name">G8</span>HFD<span class="number">1134</span></span><br></pre></td></tr></table></figure></p><h3 id="re-compile-函数"><a href="#re-compile-函数" class="headerlink" title="re.compile 函数"></a>re.compile 函数</h3><p>&emsp;&emsp;compile 函数用于编译正则表达式，生成一个正则表达式（ Pattern ）对象，供 match() 和 search() 这两个函数使用。</p><p><strong>语法</strong><br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">re</span><span class="selector-class">.compile</span>(<span class="selector-tag">pattern</span> <span class="selector-attr">[, flags]</span>)</span><br></pre></td></tr></table></figure></p><p><em>参数</em></p><ul><li>pattern:一个字符串形式的正则表达式</li><li>flags:可选，表示匹配模式，比如忽略大小写，多行模式等，具体参数为：<ul><li>re.I 忽略大小写</li><li>re.L 表示特殊字符集 \w, \W, \b, \B, \s, \S 依赖于当前环境</li><li>re.M 多行模式</li><li>re.S 即为 . 并且包括换行符在内的任意字符（. 不包括换行符）</li><li>re.U 表示特殊字符集 \w, \W, \b, \B, \d, \D, \s, \S 依赖于 Unicode 字符属性数据库</li><li>re.X 为了增加可读性，忽略空格和 # 后面的注释</li></ul></li></ul><p><strong>实例</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pattern = re.compile(<span class="string">r'\d+'</span>)                    <span class="comment"># 用于匹配至少一个数字</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = pattern.match(<span class="string">'one12twothree34four'</span>)        <span class="comment"># 查找头部，没有匹配</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> m</span><br><span class="line"><span class="keyword">None</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = pattern.match(<span class="string">'one12twothree34four'</span>, <span class="number">2</span>, <span class="number">10</span>) <span class="comment"># 从'e'的位置开始匹配，没有匹配</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> m</span><br><span class="line"><span class="keyword">None</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = pattern.match(<span class="string">'one12twothree34four'</span>, <span class="number">3</span>, <span class="number">10</span>) <span class="comment"># 从'1'的位置开始匹配，正好匹配</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> m                                         <span class="comment"># 返回一个 Match 对象</span></span><br><span class="line">&lt;_sre.SRE_Match object at <span class="number">0x10a42aac0</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.group(<span class="number">0</span>)   <span class="comment"># 可省略 0</span></span><br><span class="line"><span class="string">'12'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.start(<span class="number">0</span>)   <span class="comment"># 可省略 0</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.end(<span class="number">0</span>)     <span class="comment"># 可省略 0</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.span(<span class="number">0</span>)    <span class="comment"># 可省略 0</span></span><br><span class="line">(<span class="number">3</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure></p><p>在上面，当匹配成功时返回一个Match对象，其中：</p><ul><li>group([group1, …]) 方法用于获得一个或多个分组匹配的字符串，当要获得整个匹配的子串时，可直接使用 group() 或 group(0)；</li><li>start([group]) 方法用于获取分组匹配的子串在整个字符串中的起始位置（子串第一个字符的索引），参数默认值为 0；</li><li>end([group]) 方法用于获取分组匹配的子串在整个字符串中的结束位置（子串最后一个字符的索引+1），参数默认值为 0；</li><li>span([group]) 方法返回 (start(group), end(group))。<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt;import re</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; pattern = re.compile(r<span class="string">'([a-z]+) ([a-z]+)'</span>, re.I)   <span class="comment"># re.I 表示忽略大小写</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; m = pattern.match(<span class="string">'Hello World Wide Web'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; print m                               <span class="comment"># 匹配成功，返回一个 Match 对象</span></span><br><span class="line">&lt;_sre.SRE_Match object at <span class="number">0x10bea83e8</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; m.group(<span class="number">0</span>)                            <span class="comment"># 返回匹配成功的整个子串</span></span><br><span class="line"><span class="string">'Hello World'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; m.span(<span class="number">0</span>)                             <span class="comment"># 返回匹配成功的整个子串的索引</span></span><br><span class="line">(<span class="number">0</span>, <span class="number">11</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; m.group(<span class="number">1</span>)                            <span class="comment"># 返回第一个分组匹配成功的子串</span></span><br><span class="line"><span class="string">'Hello'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; m.span(<span class="number">1</span>)                             <span class="comment"># 返回第一个分组匹配成功的子串的索引</span></span><br><span class="line">(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; m.group(<span class="number">2</span>)                            <span class="comment"># 返回第二个分组匹配成功的子串</span></span><br><span class="line"><span class="string">'World'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; m.span(<span class="number">2</span>)                             <span class="comment"># 返回第二个分组匹配成功的子串</span></span><br><span class="line">(<span class="number">6</span>, <span class="number">11</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; m.groups()                            <span class="comment"># 等价于 (m.group(1), m.group(2), ...)</span></span><br><span class="line">(<span class="string">'Hello'</span>, <span class="string">'World'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; m.group(<span class="number">3</span>)                            <span class="comment"># 不存在第三个分组</span></span><br><span class="line">Traceback (most recent call last)<span class="symbol">:</span></span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;<span class="class"><span class="keyword">module</span>&gt;</span></span><br><span class="line"><span class="symbol">IndexError:</span> no such group</span><br></pre></td></tr></table></figure></li></ul><h3 id="re-findall"><a href="#re-findall" class="headerlink" title="re.findall"></a>re.findall</h3><p>&emsp;&emsp;在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。</p><blockquote><p>match和search是匹配一次，而findall是匹配所有</p></blockquote><p><strong>语法格式</strong><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">findall</span><span class="params">(string[, pos[, endpos]])</span></span></span><br></pre></td></tr></table></figure></p><p><em>参数</em></p><ul><li>string : 待匹配的字符串。</li><li>pos : 可选参数，指定字符串的起始位置，默认为 0。</li><li>endpos : 可选参数，指定字符串的结束位置，默认为字符串的长度。</li></ul><p><strong>实例</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:UTF8 -*-</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line">pattern = re.compile(<span class="string">r'\d+'</span>)   <span class="comment"># 查找数字</span></span><br><span class="line">result1 = pattern.findall(<span class="string">'runoob 123 google 456'</span>)</span><br><span class="line">result2 = pattern.findall(<span class="string">'run88oob123google456'</span>, <span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line">print(result1)</span><br><span class="line">print(result2)</span><br></pre></td></tr></table></figure></p><p><strong>结果</strong><br><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="symbol">'123</span>', <span class="symbol">'456</span>']</span><br><span class="line">[<span class="symbol">'88</span>', <span class="symbol">'12</span>']</span><br></pre></td></tr></table></figure></p><h3 id="re-finditer"><a href="#re-finditer" class="headerlink" title="re.finditer"></a>re.finditer</h3><p>&emsp;&emsp;和 findall 类似，在字符串中找到正则表达式所匹配的所有子串，并把它们作为一个迭代器返回。</p><p><strong>语法格式</strong><br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.finditer(pattern, <span class="type">string</span>, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p><em>参数</em></p><ul><li>pattern: 匹配的正则表达式</li><li>string: 要匹配的字符串。</li><li>flags: 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。</li></ul><p><strong>实例</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line">it = re.finditer(<span class="string">r"\d+"</span>,<span class="string">"12a32bc43jf3"</span>) </span><br><span class="line"><span class="keyword">for</span> match <span class="keyword">in</span> it: </span><br><span class="line">    <span class="keyword">print</span> (match.group() )</span><br></pre></td></tr></table></figure></p><p><strong>结果</strong><br><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">12 </span></span><br><span class="line"><span class="symbol">32 </span></span><br><span class="line"><span class="symbol">43 </span></span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure></p><h3 id="re-split"><a href="#re-split" class="headerlink" title="re.split"></a>re.split</h3><p>&emsp;&emsp;split 方法按照能够匹配的子串将字符串分割后返回列表，它的使用形式如下：</p><p><strong>语法格式</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.split(pattern, string[, <span class="attribute">maxsplit</span>=0, <span class="attribute">flags</span>=0])</span><br></pre></td></tr></table></figure></p><p><strong>参数</strong></p><ul><li>pattern：匹配的正则表达式</li><li>string：要匹配的字符串。</li><li>maxsplit：分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数。</li><li>flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。</li></ul><p><strong>实例</strong><br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt;import re</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.split(<span class="string">'\W+'</span>, <span class="string">'runoob, runoob, runoob.'</span>)</span><br><span class="line">[<span class="string">'runoob'</span>, <span class="string">'runoob'</span>, <span class="string">'runoob'</span>, <span class="string">''</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.split(<span class="string">'(\W+)'</span>, <span class="string">' runoob, runoob, runoob.'</span>) </span><br><span class="line">[<span class="string">''</span>, <span class="string">' '</span>, <span class="string">'runoob'</span>, <span class="string">', '</span>, <span class="string">'runoob'</span>, <span class="string">', '</span>, <span class="string">'runoob'</span>, <span class="string">'.'</span>, <span class="string">''</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.split(<span class="string">'\W+'</span>, <span class="string">' runoob, runoob, runoob.'</span>, <span class="number">1</span>) </span><br><span class="line">[<span class="string">''</span>, <span class="string">'runoob, runoob, runoob.'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.split(<span class="string">'a*'</span>, <span class="string">'hello world'</span>)   <span class="comment"># 对于一个找不到匹配的字符串而言，split 不会对其作出分割</span></span><br><span class="line">[<span class="string">'hello world'</span>]</span><br></pre></td></tr></table></figure></p><h2 id="正则表达式对象"><a href="#正则表达式对象" class="headerlink" title="正则表达式对象"></a>正则表达式对象</h2><h3 id="re-RegexObject"><a href="#re-RegexObject" class="headerlink" title="re.RegexObject"></a>re.RegexObject</h3><p>re.compile() 返回 RegexObject 对象。</p><h3 id="re-MatchObject"><a href="#re-MatchObject" class="headerlink" title="re.MatchObject"></a>re.MatchObject</h3><p>group() 返回被 RE 匹配的字符串。</p><ul><li>start() 返回匹配开始的位置</li><li>end() 返回匹配结束的位置</li><li>span() 返回一个元组包含匹配 (开始,结束) 的位置</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正则表达式 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>[LeetCode]Problem 1-3</title>
      <link href="/2018/12/02/LeetCode-1-3/"/>
      <url>/2018/12/02/LeetCode-1-3/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;近日思想较为纠结，也不想看书什么的，所以就没事干看了看LeetCode上面的题目。未来如何发展很迷茫呀，还是忙好当下应该做的事情！</p><h2 id="Two-Sum-easy"><a href="#Two-Sum-easy" class="headerlink" title="Two Sum [easy]"></a>Two Sum [easy]</h2><h3 id="题面"><a href="#题面" class="headerlink" title="题面"></a>题面</h3><p>Given an array of integers, return indices of the two numbers such that they add up to a specific target.<br>You may assume that each input would have exactly one solution, and you may not use the same element twice.</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Given nums = [<span class="number">2</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">15</span>], target = <span class="number">9</span>,</span><br><span class="line"></span><br><span class="line">Because nums[<span class="number">0</span>] + nums[<span class="number">1</span>] = <span class="number">2</span> + <span class="number">7</span> = <span class="number">9</span>,</span><br><span class="line">return [<span class="number">0</span>, <span class="number">1</span>].</span><br></pre></td></tr></table></figure><h3 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h3><p>&emsp;&emsp;该题目的题意是从所给定的数组中挑选哪两个数值相加可以得到我们想要的<code>target</code>，然后输出两个数值所在的索引。</p><figure class="highlight pony"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    def twoSum(self, nums, target):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        dic=&#123;&#125;</span><br><span class="line">        for i,<span class="meta">val</span> <span class="keyword">in</span> enumerate(nums):</span><br><span class="line">            <span class="keyword">if</span>(target-<span class="meta">val</span>) <span class="keyword">in</span> dic:</span><br><span class="line">                <span class="keyword">return</span> [dic[target-<span class="meta">val</span>],i]</span><br><span class="line">            dic[<span class="meta">val</span>]=i</span><br></pre></td></tr></table></figure><h2 id="Add-Two-numbers-Medium"><a href="#Add-Two-numbers-Medium" class="headerlink" title="Add Two numbers [Medium]"></a>Add Two numbers [Medium]</h2><h3 id="题面-1"><a href="#题面-1" class="headerlink" title="题面"></a>题面</h3><p>You are given two <strong>non-empty</strong> linked lists representing two non-negative integers. The digits are stored in <strong>reverse order</strong> and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.</p><p>You may assume the two numbers do not contain any leading zero, except the number 0 itself.</p><h3 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h3><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: (<span class="number">2</span> -&gt; <span class="number">4</span> -&gt; <span class="number">3</span>) + (<span class="number">5</span> -&gt; <span class="number">6</span> -&gt; <span class="number">4</span>)</span><br><span class="line">Output: <span class="number">7</span> -&gt; <span class="number">0</span> -&gt; <span class="number">8</span></span><br><span class="line">Explanation: <span class="number">342</span> + <span class="number">465</span> = <span class="number">807.</span></span><br></pre></td></tr></table></figure><h3 id="题解-1"><a href="#题解-1" class="headerlink" title="题解"></a>题解</h3><p>&emsp;&emsp;可以看出来，两个链状链表表示的是两个多位数，例如样例中的<code>input</code>就是代表了两个数值，分别是342与465.可以看出该链表中，代表的位置越深代表的数字位数越高，如<code>Input</code>中所代表的3为百分位，2为个位。<br>&emsp;&emsp;我的想法是将两个数组先分别生成两个数，然后将其相加，之后再将解生成为一个链表即可。</p><figure class="highlight pony"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    def addTwoNumbers(self, l1, l2):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type l1: ListNode</span></span><br><span class="line"><span class="string">        :type l2: ListNode</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ans=[]</span><br><span class="line">        tmp=<span class="number">0</span></span><br><span class="line">        ll1=<span class="number">0</span></span><br><span class="line">        ll2=<span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span>(l1):</span><br><span class="line">            ll1+=l1.<span class="meta">val</span>*(<span class="number">10</span>**tmp)</span><br><span class="line">            l1=l1.next</span><br><span class="line">            tmp+=<span class="number">1</span></span><br><span class="line">        tmp=<span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span>(l2):</span><br><span class="line">            ll2+=l2.<span class="meta">val</span>*(<span class="number">10</span>**tmp)</span><br><span class="line">            l2=l2.next</span><br><span class="line">            tmp+=<span class="number">1</span></span><br><span class="line">        tmp=ll1+ll2</span><br><span class="line">        <span class="keyword">while</span>(tmp<span class="comment">//10 or tmp):</span></span><br><span class="line">            ans.append(tmp%<span class="number">10</span>)</span><br><span class="line">            tmp=tmp<span class="comment">//10</span></span><br><span class="line">        <span class="keyword">if</span>(len(ans)==<span class="number">0</span>):</span><br><span class="line">            ans.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h2 id="Longest-Substring-Without-Repeating-Characters"><a href="#Longest-Substring-Without-Repeating-Characters" class="headerlink" title="Longest Substring Without Repeating Characters"></a>Longest Substring Without Repeating Characters</h2><h3 id="题面-2"><a href="#题面-2" class="headerlink" title="题面"></a>题面</h3><p>Given a string, find the length of the longest substring without repeating characters.</p><h3 id="Example-2"><a href="#Example-2" class="headerlink" title="Example"></a>Example</h3><p><strong>Example 1</strong>:<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: <span class="string">"abcabcbb"</span></span><br><span class="line">Output: <span class="number">3</span> </span><br><span class="line">Explanation: The answer <span class="keyword">is</span> <span class="string">"abc"</span>, <span class="keyword">with</span> <span class="keyword">the</span> <span class="built_in">length</span> <span class="keyword">of</span> <span class="number">3.</span></span><br></pre></td></tr></table></figure></p><p><strong>Example 2</strong>:<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: <span class="string">"bbbbb"</span></span><br><span class="line">Output: <span class="number">1</span></span><br><span class="line">Explanation: The answer <span class="keyword">is</span> <span class="string">"b"</span>, <span class="keyword">with</span> <span class="keyword">the</span> <span class="built_in">length</span> <span class="keyword">of</span> <span class="number">1.</span></span><br></pre></td></tr></table></figure></p><p><strong>Example 3</strong>:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input: <span class="string">"pwwkew"</span></span><br><span class="line">Output: 3</span><br><span class="line">Explanation: The answer is <span class="string">"wke"</span>, with the length of 3. </span><br><span class="line">            <span class="built_in"> Note </span>that the answer must be a substring, <span class="string">"pwke"</span> is a subsequence <span class="keyword">and</span> <span class="keyword">not</span> a substring.</span><br></pre></td></tr></table></figure></p><h3 id="题解-2"><a href="#题解-2" class="headerlink" title="题解"></a>题解</h3><p>&emsp;&emsp;本题应该注意的是他所需要的是<code>最长不重复子串</code>的长度，而不是<code>最长不重复子序列</code>的长度。两者有什么区别呢？类似 <em>Example 3</em> 中所示如果是最长不重复子序列，那么解应该是<code>pwke</code>，但是子串必须是连续的，因此最长的只有<code>wke</code>。<br>&emsp;&emsp;首先我们需要一个dict的<code>usedChar</code>，用它来保存我们现在已经读到的所有字符以及最近的距离。用<code>start</code>保存当前保存的最长不重复子串的开始地点，然后遍历字符串，若该字符已出现且<code>start</code>小于该字符之前的位置；否则就重新规划最长长度，也就是现在的长度和之前的最长长度的最大值。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    <span class="comment"># @return an integer</span></span><br><span class="line">    def lengthOfLongestSubstring(self, s):</span><br><span class="line">        <span class="keyword">start</span> = maxLength = <span class="number">0</span></span><br><span class="line">        usedChar = &#123;&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">range</span>(<span class="keyword">len</span>(s)):</span><br><span class="line">            <span class="keyword">if</span> s[i] <span class="keyword">in</span> usedChar <span class="keyword">and</span> <span class="keyword">start</span> &lt;= usedChar[s[i]]:</span><br><span class="line">                <span class="keyword">start</span> = usedChar[s[i]] + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                maxLength = <span class="keyword">max</span>(maxLength, i - <span class="keyword">start</span> + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            usedChar[s[i]] = i</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> maxLength</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>&emsp;&emsp;看看题什么的主要就是调整一下自己的心情，因为真的不知道未来该如何发展，太纠结了，对未来充满了迷茫呀！不过就算是再迷茫还是要做好当前的事情。</p>]]></content>
      
      
      <categories>
          
          <category> 刷题记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow学习手册（四）</title>
      <link href="/2018/11/17/TensorFlow%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
      <url>/2018/11/17/TensorFlow%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;本节将描述一下循环神经网络与长短期记忆网络两种模型。</p><h2 id="循环神经网络（RNN）"><a href="#循环神经网络（RNN）" class="headerlink" title="循环神经网络（RNN）"></a>循环神经网络（RNN）</h2><p>&emsp;&emsp;循环神经网络主要用于处理和预测序列化数据的。循环神经网络的来源就是为了刻画一个序列当前的输出和之前信息的关系，它的隐藏层之间的结点是有连接的，隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出。</p><h3 id="循环神经网络的经典形式"><a href="#循环神经网络的经典形式" class="headerlink" title="循环神经网络的经典形式"></a>循环神经网络的经典形式</h3><p><img src="/2018/11/17/TensorFlow学习手册（四）/WX20181117-145027.png" alt="图1 循环神经网络经典结构示意图"></p><p>&emsp;&emsp;图1展示的是一个典型的循环神经网络，在每一时刻t，循环神经网络会针对该时刻的输入结合当前模型的状态给出一个输出，并更新模型的状态。在每一个时刻，循环神经网络的模块A在读取了$x_t$和$h_{t-1}$之后会产生本时刻的输出$O_t$。<strong>由于模块A中的运算和变量在不同时刻是相同的</strong>，因此循环神经网络理论上可以被看作是同一神经网络结构被无限复制的结果。循环神经网络是在不同时间位置共享参数，从而能够使用有限的参数处理任意长度的序列。</p><h3 id="循环神经网络的展开形式"><a href="#循环神经网络的展开形式" class="headerlink" title="循环神经网络的展开形式"></a>循环神经网络的展开形式</h3><p>&emsp;&emsp;如果将完整的输入输出序列展开，就可以看到如图2所示的结构。从下图可以看出循环神经网络当前状态$h_t$和当前的输入$x_t$共同决定的。在时刻t，状态$h_{t-1}$浓缩了前面序列$x_0,x_1,…,x_{t-1}$的信息，用于作为输出$o_t$的参考。由于序列的长度可以无限延长，维度有限的h状态不可能将序列的全部信息都保存下来，因此模型必须学习只保留与后面任务$o_t,o_{t+1},…$相关的最重要的信息。</p><p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_2.jpg" alt="图2 循环神经网络按时间展开后的结构"></p><p>&emsp;&emsp;循环神经网络对长度为N的序列展开之后，可以视为一个<strong>有N个中间层的前馈神经网络</strong>。这个前馈神经网络没有训练链接，可以使用反向传播算法进行训练。循环神经网络要求每一个时刻都有一个输入，但是不一定每个时刻都需要有输出。</p><p>&emsp;&emsp;在循环神经网络中，被复制多次的结构称为循环体。图3是一个最简单的循环体结构。循环神经网络中的状态是通过一个向量来表示的，这个向量的维度也称为循环神经网络隐藏层的大小，假设其为n。从下图可以看出循环体中的神经网络的输入有两部分，一部分为上一时刻的状态，一部分为当前时刻的输入样本。对于时间序列数据来说，每一时刻的输入样例可以是该时刻的数值或者是单词向量。</p><p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_3_gaitubao_com_289x278.jpg" alt="图3 使用单层全连接神经网络作为循环体的循环神经网络结构图"></p><p>&emsp;&emsp;循环神经网络唯一的区别在于因为它每个时刻都有一个输出，所以循环神经网络的总损失为所有时刻（或者部分时刻上）的损失总和。</p><h2 id="长短期记忆网络（LSTM）"><a href="#长短期记忆网络（LSTM）" class="headerlink" title="长短期记忆网络（LSTM）"></a>长短期记忆网络（LSTM）</h2><p>&emsp;&emsp;与单一tanh循环体结构不同，LSTM是一种拥有三个“门”结构的特殊网络结构，如图4所示。为了使循环神经网络更有效的保存长期记忆，“遗忘门”和“输入门”至关重要，它们是LSTM结构的核心。“遗忘门”会根据当前的输入$x_t$和上一时刻输出$h_{t-1}$决定哪一部分记忆需要被遗忘。“输入门”会根据$x_t$和$h_{t-1}$决定哪些信息加入到状态$c_{t-1}$中生成新的状态$c_t$。</p><p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_4.jpg" alt="图4 LSTM单元结构示意图"></p><p>&emsp;&emsp;图5是用流程图形式表示了LSTM单元的细节。</p><p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_5.jpg" alt="图5 LSTM单元细节图"></p><h2 id="循环神经网络的变种"><a href="#循环神经网络的变种" class="headerlink" title="循环神经网络的变种"></a>循环神经网络的变种</h2><h3 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h3><p>&emsp;&emsp;在之前介绍的经典的循环神经网络中，状态的传输都是从前往后单向的，然而在有些问题中，当前时刻的输出不仅和之前的状态有关，也和之后的状态有关。双向循环神经网络是由两个独立的循环神经网络叠加在一起组成的。输出由这两个循环神经网络的输出拼接而成。如下图所示：</p><p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_6.jpg" alt="图6 双向循环神经网络"></p><h3 id="深层循环神经网络"><a href="#深层循环神经网络" class="headerlink" title="深层循环神经网络"></a>深层循环神经网络</h3><p>&emsp;&emsp;深层循环神经网络是循环神经网络的另外一种变种。为了增强模型的表达能力，可以在网络中设置多个循环层，将每层循环网络的输出传给下一层进行处理。</p><p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_7.jpg" alt="图7 深层循环神经网络"></p><h2 id="循环神经网络样例程序"><a href="#循环神经网络样例程序" class="headerlink" title="循环神经网络样例程序"></a>循环神经网络样例程序</h2><p>&emsp;&emsp;以时序预测为例，利用循环神经网络实现对函数sin x取值的预测。在以下程序中每隔SAMPLE_ITERVAL对sin函数进行一次采样，采样得到的序列就是sin函数离散化之后的结果。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Author   : netycc</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/16 8:44 PM</span></span><br><span class="line"><span class="comment"># @File     : 循环神经网络样例.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm Community Edition</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> numpy as np</span><br><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> matplotlib as mpl</span><br><span class="line"><span class="comment"># mpl.use("Agg")</span></span><br><span class="line">from matplotlib <span class="built_in">import</span> pyplot as plt</span><br><span class="line"></span><br><span class="line"><span class="attr">HIDDEN_SIZE=30</span>  <span class="comment">#LSTM中的隐藏节点数</span></span><br><span class="line"><span class="attr">NUM_LAYERS=2</span>    <span class="comment">#LSTM的层数</span></span><br><span class="line"></span><br><span class="line"><span class="attr">TIMESTEPS=10</span>    <span class="comment">#循环神经网络的训练长度</span></span><br><span class="line"><span class="attr">TRAINING_STEPS=10000</span>    <span class="comment">#训练轮数</span></span><br><span class="line"><span class="attr">BATCH_SIZE=32</span>   <span class="comment">#batch大小</span></span><br><span class="line"></span><br><span class="line"><span class="attr">TRAINING_EXAMPLES=10000</span> <span class="comment">#训练数据个数</span></span><br><span class="line"><span class="attr">TESTING_EXAMPLES=1000</span>   <span class="comment">#测试数据个数</span></span><br><span class="line"><span class="attr">SAMPLE_GAP=0.01</span>         <span class="comment">#采样间隔</span></span><br><span class="line"></span><br><span class="line">def generate_data(seq):</span><br><span class="line">    <span class="attr">X=[]</span></span><br><span class="line">    <span class="attr">y=[]</span></span><br><span class="line"></span><br><span class="line">    for i <span class="keyword">in</span> range(len(seq)-TIMESTEPS):</span><br><span class="line">        X.append([seq[i:i+TIMESTEPS]])  <span class="comment">#i:i+TIMESTEPS这组数据作为训练数据</span></span><br><span class="line">        y.append([seq[i+TIMESTEPS]])    <span class="comment">#为了预测第i+TIMESTEPS这一数据</span></span><br><span class="line">    return np.array(X,<span class="attr">dtype=np.float32),np.array(y,dtype=np.float32)</span></span><br><span class="line"></span><br><span class="line">def lstm_model(X,y,is_training):</span><br><span class="line">    <span class="comment"># 多层LSTM</span></span><br><span class="line">    <span class="attr">cell</span> = tf.nn.rnn_cell.MultiRNNCell([</span><br><span class="line">        tf.nn.rnn_cell.LSTMCell(HIDDEN_SIZE)   for _ <span class="keyword">in</span> range(NUM_LAYERS)])</span><br><span class="line"></span><br><span class="line">    outputs,<span class="attr">_=tf.nn.dynamic_rnn(cell,X,dtype=tf.float32)</span></span><br><span class="line">    <span class="attr">output=outputs[:,-1,:]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对LSTM网络的输出再加一层全连接层并计算损失</span></span><br><span class="line">    <span class="attr">predictions</span> = tf.contrib.layers.fully_connected(output,<span class="number">1</span>,<span class="attr">activation_fn=None)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> not is_training:</span><br><span class="line">        return predictions,None,None</span><br><span class="line"></span><br><span class="line">    <span class="attr">loss=tf.losses.mean_squared_error(labels=y,predictions=predictions)</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">train_op=tf.contrib.layers.optimize_loss(</span></span><br><span class="line">        loss,tf.train.get_global_step(),<span class="attr">optimizer="Adagrad",learning_rate=0.1)</span></span><br><span class="line"></span><br><span class="line">    return predictions,loss,train_op</span><br><span class="line"></span><br><span class="line">def train(sess,train_X,train_y):</span><br><span class="line">    <span class="attr">ds=tf.data.Dataset.from_tensor_slices((train_X,train_y))</span></span><br><span class="line">    <span class="attr">ds=ds.repeat().shuffle(1000).batch(BATCH_SIZE)</span></span><br><span class="line">    X,<span class="attr">y=ds.make_one_shot_iterator().get_next()</span></span><br><span class="line">    <span class="comment"># 调用模型</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>):</span><br><span class="line">        predictions,loss,<span class="attr">train_op=lstm_model(X,y,True)</span></span><br><span class="line"></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    for i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">        _,<span class="attr">l=sess.run([train_op,loss])</span></span><br><span class="line">        <span class="keyword">if</span> i %<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"train step:"</span>+str(i),<span class="string">", loss:"</span>+str(l))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def run_eval(sess,test_X,test_y):</span><br><span class="line">    <span class="attr">ds=tf.data.Dataset.from_tensor_slices((test_X,test_y))</span></span><br><span class="line">    <span class="attr">ds=ds.batch(1)</span></span><br><span class="line">    X,<span class="attr">y=ds.make_one_shot_iterator().get_next()</span></span><br><span class="line">    <span class="comment"># 调用模型计算结果</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>,<span class="attr">reuse=True):</span></span><br><span class="line">        prediction,_,<span class="attr">_=lstm_model(X,[0.0],False)</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">predictions=[]</span></span><br><span class="line">    <span class="attr">labels=[]</span></span><br><span class="line">    for i <span class="keyword">in</span> range(TESTING_EXAMPLES):</span><br><span class="line">        p,<span class="attr">l=sess.run([prediction,y])</span></span><br><span class="line">        predictions.append(p)</span><br><span class="line">        labels.append(l)</span><br><span class="line"></span><br><span class="line">    <span class="attr">predictions=np.array(predictions).squeeze()</span></span><br><span class="line">    <span class="attr">labels=np.array(labels).squeeze()</span></span><br><span class="line">    <span class="attr">rmse=np.sqrt(((predictions-labels)**2).mean(axis=0))</span></span><br><span class="line">    print(<span class="string">"Mean Square Error is :%.2f"</span>%rmse)</span><br><span class="line"></span><br><span class="line">    return predictions,labels</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attr">__name__</span> == '__main__':</span><br><span class="line">    <span class="attr">test_start=(TRAINING_EXAMPLES+TIMESTEPS)*SAMPLE_GAP</span></span><br><span class="line">    <span class="attr">test_end=test_start+(TESTING_EXAMPLES+TIMESTEPS)*SAMPLE_GAP</span></span><br><span class="line">    train_X,<span class="attr">train_y=generate_data(np.sin(np.linspace(</span></span><br><span class="line">        <span class="number">0</span>,test_start,TRAINING_EXAMPLES+TIMESTEPS,<span class="attr">dtype=np.float32</span></span><br><span class="line">    )))</span><br><span class="line">    test_X,<span class="attr">test_y=generate_data(np.sin(np.linspace(</span></span><br><span class="line">        test_start,test_end,TESTING_EXAMPLES+TIMESTEPS,<span class="attr">dtype=np.float32</span></span><br><span class="line">    )))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() as sess:</span><br><span class="line">        train(sess,train_X,train_y)</span><br><span class="line">        predictions,<span class="attr">labels=run_eval(sess,test_X,test_y)</span></span><br><span class="line">        <span class="comment"># plt.figure()</span></span><br><span class="line">        plt.plot(predictions, <span class="attr">label="predictions")</span></span><br><span class="line">        plt.plot(labels, <span class="attr">label='real_sin')</span></span><br><span class="line">        plt.legend()</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/2018/11/17/TensorFlow学习手册（四）/Figure_1.png" alt="预测结果"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> RNN </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>通过proxychains在终端使用ss代理</title>
      <link href="/2018/11/16/%E9%80%9A%E8%BF%87proxychains%E5%9C%A8%E7%BB%88%E7%AB%AF%E4%BD%BF%E7%94%A8ss%E4%BB%A3%E7%90%86/"/>
      <url>/2018/11/16/%E9%80%9A%E8%BF%87proxychains%E5%9C%A8%E7%BB%88%E7%AB%AF%E4%BD%BF%E7%94%A8ss%E4%BB%A3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;由于需要下载数据集，所以需要在终端科学*上网，于是就开始了这个博客的内容。</p><h2 id="下载并安装proxychains"><a href="#下载并安装proxychains" class="headerlink" title="下载并安装proxychains"></a>下载并安装proxychains</h2><p>&emsp;&emsp;在mac上，可以直接使用<code>sudo brew install proxychains</code>，或者通过源代码安装，如下：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone http<span class="variable">s:</span>//github.<span class="keyword">com</span>/rofl0r/proxychains-ng.git</span><br><span class="line"><span class="keyword">cd</span> proxychains-ng</span><br><span class="line">./configure</span><br><span class="line"><span class="keyword">make</span> &amp;&amp; <span class="keyword">make</span> install</span><br><span class="line"><span class="keyword">cd</span> .. &amp;&amp; rm -rf proxychains-ng</span><br></pre></td></tr></table></figure></p><h2 id="修改proxychains配置"><a href="#修改proxychains配置" class="headerlink" title="修改proxychains配置"></a>修改proxychains配置</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim <span class="regexp">/usr/</span>local<span class="regexp">/etc/</span>proxychains.conf</span><br></pre></td></tr></table></figure><p>将文件末尾的<code>socks4 127.0.0.1 9095</code>改为：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">socks5 <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> <span class="number">1080</span></span><br></pre></td></tr></table></figure></p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>&emsp;&emsp;开启ssr的全局模式以后，在需要科学*上网的命令前加上<code>proxychain4</code>即可，如下：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">proxychains4</span> <span class="selector-tag">wget</span> <span class="selector-tag">google</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure></p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><p><a href="https://blog.fazero.me/2015/08/31/%E5%88%A9%E7%94%A8proxychains%E5%9C%A8%E7%BB%88%E7%AB%AF%E4%BD%BF%E7%94%A8socks5%E4%BB%A3%E7%90%86/" target="_blank" rel="noopener">利用proxychains在终端使用socks5代理</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> osx </tag>
            
            <tag> ssr </tag>
            
            <tag> proxychains </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>安装cuda9.0和cudnn7.4以及tensorflow-gpu==1.11.0</title>
      <link href="/2018/11/14/%E5%AE%89%E8%A3%85cuda9-0%E5%92%8Ccudnn7-4%E4%BB%A5%E5%8F%8Atensorflow-gpu-1-11-0/"/>
      <url>/2018/11/14/%E5%AE%89%E8%A3%85cuda9-0%E5%92%8Ccudnn7-4%E4%BB%A5%E5%8F%8Atensorflow-gpu-1-11-0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;因为需要更新到TensorFlow 1.11.0，因此需要更改一下之前配置的服务器。TensorFlow 1.11.0版本仅仅只支持cuda 9.0，因此需要卸载之前安装的cuda 8.0，以下是本次修改的步骤。</p><h2 id="卸载cuda-8-0"><a href="#卸载cuda-8-0" class="headerlink" title="卸载cuda 8.0"></a>卸载cuda 8.0</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭图形界面</span></span><br><span class="line">$ sudo<span class="built_in"> service </span>lightdm stop</span><br><span class="line">$ sudo /usr/local/cuda-8.0/bin/uninstall_cuda_8.0.pl</span><br><span class="line">$ sudo rm -rf /usr/local/cuda-8.0</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;以上步骤完成了删除cuda-8.0的操作。</p><h2 id="安装cuda-9-0"><a href="#安装cuda-9-0" class="headerlink" title="安装cuda 9.0"></a>安装cuda 9.0</h2><p>&emsp;&emsp;现在cuda已经更新到了10.0，但是我们需要TensorFlow只支持到9.0，因此我们需要下载9.0版本，可以到<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">官网的历史版本</a>中寻找。下载完后传输到服务器上，然后就可以进行安装了。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo bash cuda_9<span class="number">.0</span><span class="number">.176</span>_384<span class="number">.81</span>_linux.run</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;需要注意的是，<strong>不要安装驱动</strong>，因为驱动已经安装过了，因此不需要再安装了。</p><blockquote><p>注：如果不小心把驱动弄没了的话，可以使用cuda9.2版本安装一下驱动，然后删除cuda即可，因为cuda9.0版本中内置的驱动内核不匹配，所以不能使用这个版本的驱动。</p></blockquote><h2 id="安装cudnn-7-4"><a href="#安装cudnn-7-4" class="headerlink" title="安装cudnn 7.4"></a>安装cudnn 7.4</h2><p>&emsp;&emsp;去<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">官网</a>下载最新的cudnn，如果没有账号的话需要注册账号。我下载的是liunx源代码版本的，解压移动到cuda文件夹中即可。</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>tar -zxvf cudnn-<span class="number">9.0</span>-linux-x64-v7.<span class="number">0</span>.tgz</span><br><span class="line"><span class="variable">$ </span>cd cuda</span><br><span class="line"><span class="variable">$ </span>sudo cp lib64/* <span class="regexp">/usr/local</span><span class="regexp">/cuda/lib</span>64/    </span><br><span class="line"><span class="variable">$ </span>sudo cp <span class="keyword">include</span>/* <span class="regexp">/usr/local</span><span class="regexp">/cuda/include</span><span class="regexp">/</span></span><br></pre></td></tr></table></figure><hr><h2 id="服务器下各账户要处理的事情"><a href="#服务器下各账户要处理的事情" class="headerlink" title="服务器下各账户要处理的事情"></a>服务器下各账户要处理的事情</h2><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>&emsp;&emsp;弄好完整的cuda和cudnn后，每个账户需要修改自己的环境变量。<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>sudo vim ~<span class="regexp">/.bashrc</span></span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;在环境变量的最后，加上如下代码，如果之前有，则只需要修改一下就行：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=/usr/local/cuda-9.0/bin$&#123;PATH:+:<span class="variable">$&#123;PATH&#125;</span>&#125;</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">LD_LIBRARY_PATH</span>=/usr/local/cuda-9.0/lib64$&#123;LD_LIBRARY_PATH:+:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&#125;</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;激活环境变量<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> ~/.bashrc</span></span><br></pre></td></tr></table></figure></p><h3 id="安装TensorFlow"><a href="#安装TensorFlow" class="headerlink" title="安装TensorFlow"></a>安装TensorFlow</h3><p>&emsp;&emsp;安装TensorFlow 1.11.0版本<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果需要卸载之前的话：pip uninstall tensorflow-gpu</span></span><br><span class="line">$ pip install <span class="attribute">tensorflow-gpu</span>==1.11.0</span><br></pre></td></tr></table></figure></p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><p><a href="https://blog.csdn.net/fdqw_sph/article/details/78745375" target="_blank" rel="noopener">史上最全的ubuntu16.04安装nvidia驱动+cuda9.0+cuDnn7.0</a><br><a href="https://blog.csdn.net/weixin_39513374/article/details/80997912" target="_blank" rel="noopener">ImportError: libcublas.so.9.0: cannot open shared object file: No such file…问题原因及解决方法</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> 显卡驱动配置 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow学习手册（三）</title>
      <link href="/2018/11/14/TensorFlow%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/2018/11/14/TensorFlow%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;本节将叙述一下如何将训练好的模型保存下来，并且在使用的时候还原出来，以及卷积神经网络的构建方法以及TensorFlow实现。</p><h2 id="TensorFlow模型持久化"><a href="#TensorFlow模型持久化" class="headerlink" title="TensorFlow模型持久化"></a>TensorFlow模型持久化</h2><p>&emsp;&emsp;在TensorFlow中，提供了tf.train.Saver类用来保存TensorFlow计算图，以下代码给出了保存TensorFlow计算图的方法。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line"></span><br><span class="line"><span class="attr">v1</span> = tf.Variable(tf.constant(<span class="number">1.0</span>, <span class="attr">shape=[1]),</span> <span class="attr">name</span> = <span class="string">"v1"</span>)</span><br><span class="line"><span class="attr">v2</span> = tf.Variable(tf.constant(<span class="number">2.0</span>, <span class="attr">shape=[1]),</span> <span class="attr">name</span> = <span class="string">"v2"</span>)</span><br><span class="line"><span class="attr">result</span> = v1 + v2</span><br><span class="line"></span><br><span class="line"><span class="attr">init_op</span> = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># 可以指定saver保存什么变量，如果只保存v1，那么下边要恢复result就会出错，因为缺少v2</span></span><br><span class="line"><span class="attr">saver</span> = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() as sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    saver.save(sess, <span class="string">"Saved_model/model.ckpt"</span>)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;以上代码实现了持久化一个简单的TensorFlow模型的功能。在这段代码中，saver.save函数将TensorFlow模型保存到了<code>Saved_model/model.ckpt</code>文件中， 但是在该文件夹下，会产生三个文件，第一个文件为<code>model.ckpt.meta</code>他保存了计算图的结构；第二个文件为<code>model.ckpt</code>，这个文件保存了TensorFlow程序中每一个变量的取值；最后一个文件为<code>checkpoint</code>文件，这个文件中保存了一个目录下所有的模型文件列表。<br>&emsp;&emsp;以下代码给出了加载这个已经保存的TensorFlow模型的方法。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line"></span><br><span class="line">v1 = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name = <span class="string">"v1"</span>)</span><br><span class="line">v2 = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name = <span class="string">"v2"</span>)</span><br><span class="line">result = v1 + v2</span><br><span class="line"></span><br><span class="line">saver = <span class="keyword">tf</span>.train.Saver()</span><br><span class="line"></span><br><span class="line">with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></span><br><span class="line">    saver.restore(sess, <span class="string">"Saved_model/model.ckpt"</span>)</span><br><span class="line">    <span class="keyword">print</span>(result.<span class="built_in">eval</span>())</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;可以看出这个代码与保存模型的代码差不多。在加载模型的代码中没有运行变量的初始化过程，而是将变量的值通过已经保存的模型加载下来。如果不希望重复定义图上的运算，也可以直接加载已经持久化的图。如下：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line"></span><br><span class="line">saver = <span class="keyword">tf</span>.train.import_meta_graph(<span class="string">"Saved_model/model.ckpt.meta"</span>)</span><br><span class="line">with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></span><br><span class="line">    saver.restore(sess, <span class="string">"Saved_model/model.ckpt"</span>)</span><br><span class="line">    <span class="keyword">print</span>(sess.run(<span class="keyword">tf</span>.get_default_graph().get_tensor_by_name(<span class="string">"add:0"</span>)))</span><br></pre></td></tr></table></figure></p><h2 id="TensorFlow最佳实践样例程序"><a href="#TensorFlow最佳实践样例程序" class="headerlink" title="TensorFlow最佳实践样例程序"></a>TensorFlow最佳实践样例程序</h2><h3 id="mnist-inference-py"><a href="#mnist-inference-py" class="headerlink" title="mnist_inference.py"></a>mnist_inference.py</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line"></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line">LAYER1_NODE = <span class="number">500</span></span><br><span class="line"></span><br><span class="line">def get_weight_variable(shape, regularizer):</span><br><span class="line">    weights = <span class="keyword">tf</span>.get_variable(<span class="string">"weights"</span>, shape, initializer=<span class="keyword">tf</span>.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">    <span class="keyword">if</span> regularizer != None: <span class="keyword">tf</span>.add_to_collection(<span class="string">'losses'</span>, regularizer(weights))</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line">def inference(input_tensor, regularizer):</span><br><span class="line">    with <span class="keyword">tf</span>.variable_scope(<span class="string">'layer1'</span>):</span><br><span class="line"></span><br><span class="line">        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)</span><br><span class="line">        biases = <span class="keyword">tf</span>.get_variable(<span class="string">"biases"</span>, [LAYER1_NODE], initializer=<span class="keyword">tf</span>.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        layer1 = <span class="keyword">tf</span>.<span class="keyword">nn</span>.relu(<span class="keyword">tf</span>.matmul(input_tensor, weights) + biases)</span><br><span class="line"></span><br><span class="line">    with <span class="keyword">tf</span>.variable_scope(<span class="string">'layer2'</span>):</span><br><span class="line">        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)</span><br><span class="line">        biases = <span class="keyword">tf</span>.get_variable(<span class="string">"biases"</span>, [OUTPUT_NODE], initializer=<span class="keyword">tf</span>.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        layer2 = <span class="keyword">tf</span>.matmul(layer1, weights) + biases</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> layer2</span><br></pre></td></tr></table></figure><h3 id="mnist-train-py"><a href="#mnist-train-py" class="headerlink" title="mnist_train.py"></a>mnist_train.py</h3><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist <span class="built_in">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> mnist_inference</span><br><span class="line"></span><br><span class="line"><span class="attr">BATCH_SIZE</span> = <span class="number">100</span></span><br><span class="line"><span class="attr">LEARNING_RATE_BASE</span> = <span class="number">0.8</span></span><br><span class="line"><span class="attr">LEARNING_RATE_DECAY</span> = <span class="number">0.99</span></span><br><span class="line"><span class="attr">REGULARIZATION_RATE</span> = <span class="number">0.0001</span></span><br><span class="line"><span class="attr">TRAINING_STEPS</span> = <span class="number">30000</span></span><br><span class="line"><span class="attr">MOVING_AVERAGE_DECAY</span> = <span class="number">0.99</span></span><br><span class="line"><span class="attr">MODEL_SAVE_PATH="MNIST_model/"</span></span><br><span class="line"><span class="attr">MODEL_NAME="mnist_model"</span></span><br><span class="line"></span><br><span class="line">def train(mnist):</span><br><span class="line"></span><br><span class="line">    <span class="attr">x</span> = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], <span class="attr">name='x-input')</span></span><br><span class="line">    <span class="attr">y_</span> = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], <span class="attr">name='y-input')</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">regularizer</span> = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)</span><br><span class="line">    <span class="attr">y</span> = mnist_inference.inference(x, regularizer)</span><br><span class="line">    <span class="attr">global_step</span> = tf.Variable(<span class="number">0</span>, <span class="attr">trainable=False)</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">variable_averages</span> = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    <span class="attr">variables_averages_op</span> = variable_averages.apply(tf.trainable_variables())</span><br><span class="line">    <span class="attr">cross_entropy</span> = tf.nn.sparse_softmax_cross_entropy_with_logits(<span class="attr">logits=y,</span> <span class="attr">labels=tf.argmax(y_,</span> <span class="number">1</span>))</span><br><span class="line">    <span class="attr">cross_entropy_mean</span> = tf.reduce_mean(cross_entropy)</span><br><span class="line">    <span class="attr">loss</span> = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))</span><br><span class="line">    <span class="attr">learning_rate</span> = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,</span><br><span class="line">        <span class="attr">staircase=True)</span></span><br><span class="line">    <span class="attr">train_step</span> = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, <span class="attr">global_step=global_step)</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, variables_averages_op]):</span><br><span class="line">        <span class="attr">train_op</span> = tf.no_op(<span class="attr">name='train')</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">saver</span> = tf.train.Saver()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() as sess:</span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        <span class="built_in">import</span> time</span><br><span class="line">        <span class="attr">start_time=time.time()</span></span><br><span class="line">        for i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">            xs, <span class="attr">ys</span> = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            _, loss_value, <span class="attr">step</span> = sess.run([train_op, loss, global_step], <span class="attr">feed_dict=&#123;x:</span> xs, y_: ys&#125;)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># print("spend"+str(time.time()-start_time))</span></span><br><span class="line">                print(<span class="string">"Spend %.2f s,after %d training step(s), loss on training batch is %g."</span> % (time.time()-start_time,step, loss_value))</span><br><span class="line">                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), <span class="attr">global_step=global_step)</span></span><br><span class="line">                <span class="attr">start_time=time.time()</span></span><br><span class="line"></span><br><span class="line">def main(<span class="attr">argv=None):</span></span><br><span class="line">    <span class="attr">mnist</span> = input_data.read_data_sets(<span class="string">"data"</span>, <span class="attr">one_hot=True)</span></span><br><span class="line">    train(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attr">__name__</span> == '__main__':</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure><h3 id="mnist-eval-py"><a href="#mnist-eval-py" class="headerlink" title="mnist_eval.py"></a>mnist_eval.py</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line"></span><br><span class="line">import mnist_train</span><br><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">import mnist_inference</span><br><span class="line"></span><br><span class="line"># 加载的时间间隔。</span><br><span class="line">EVAL_INTERVAL_SECS = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">def evaluate(mnist):</span><br><span class="line">    with <span class="keyword">tf</span>.Graph().as_default() <span class="keyword">as</span> <span class="variable">g:</span></span><br><span class="line">        <span class="keyword">x</span> = <span class="keyword">tf</span>.placeholder(<span class="keyword">tf</span>.float32, [None, mnist_inference.INPUT_NODE], name=<span class="string">'x-input'</span>)</span><br><span class="line">        y_ = <span class="keyword">tf</span>.placeholder(<span class="keyword">tf</span>.float32, [None, mnist_inference.OUTPUT_NODE], name=<span class="string">'y-input'</span>)</span><br><span class="line">        validate_feed = &#123;<span class="keyword">x</span>: mnist.validation.images, y_: mnist.validation.labels&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">y</span> = mnist_inference.inference(<span class="keyword">x</span>, None)</span><br><span class="line">        correct_prediction = <span class="keyword">tf</span>.equal(<span class="keyword">tf</span>.argmax(<span class="keyword">y</span>, <span class="number">1</span>), <span class="keyword">tf</span>.argmax(y_, <span class="number">1</span>))</span><br><span class="line">        accuracy = <span class="keyword">tf</span>.reduce_mean(<span class="keyword">tf</span>.cast(correct_prediction, <span class="keyword">tf</span>.float32))</span><br><span class="line"></span><br><span class="line">        variable_averages = <span class="keyword">tf</span>.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)</span><br><span class="line">        variables_to_restore = variable_averages.variables_to_restore()</span><br><span class="line">        saver = <span class="keyword">tf</span>.train.Saver(variables_to_restore)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> True:</span><br><span class="line">            with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></span><br><span class="line">                ckpt = <span class="keyword">tf</span>.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)</span><br><span class="line">                <span class="keyword">if</span> ckpt <span class="built_in">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">                    saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">                    <span class="keyword">for</span> v in <span class="keyword">tf</span>.global_variables():</span><br><span class="line">                        <span class="keyword">print</span>(v.name, <span class="string">":"</span>, v.<span class="built_in">eval</span>())</span><br><span class="line">                    <span class="keyword">print</span>(<span class="string">"#####################"</span>)</span><br><span class="line">                    global_step = ckpt.model_checkpoint_path.<span class="keyword">split</span>(<span class="string">'/'</span>)[-<span class="number">1</span>].<span class="keyword">split</span>(<span class="string">'-'</span>)[-<span class="number">1</span>]</span><br><span class="line">                    accuracy_score = sess.run(accuracy, feed_dict=validate_feed)</span><br><span class="line">                    <span class="keyword">print</span>(<span class="string">"After %s training step(s), validation accuracy = %g"</span> % (global_step, accuracy_score))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">print</span>(<span class="string">'No checkpoint file found'</span>)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">            time.<span class="keyword">sleep</span>(EVAL_INTERVAL_SECS)</span><br><span class="line"></span><br><span class="line">def main(<span class="built_in">argv</span>=None):</span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"data"</span>, one_hot=True)</span><br><span class="line">    evaluate(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>&emsp;&emsp;理论知识不再赘述，该文只叙述如何使用TensorFlow构建CNN结构，一般构建卷积层是通过以下几个步骤：</p><ol><li><p>构建过滤器参数尺寸，参数为四维，前两个代表了过滤器的尺寸，第三个表示当前层的深度，第四个表示过滤器的深度。</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">filter_weight = tf.get_variable('weights',[<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">16</span>],</span><br><span class="line">                    initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br></pre></td></tr></table></figure></li><li><p>创建偏执项，偏执项尺寸为过滤器的深度。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">biases</span> = tf.get_variable(<span class="string">'biases'</span>,[<span class="number">16</span>],initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br></pre></td></tr></table></figure></li><li><p>利用tf.nn.conv2d()创建卷积层，参数依次为：输入的batch，过滤器权值，各维度步长，填充方法。<strong>其中在各维度步长的参数中，第一位和最后一维的数字一定要是1，因为卷积的步长只与矩阵的长宽有效。</strong>填充参数有两种选择，分别是SAME（全0填充）、VALID（表示不添加）。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv = tf<span class="selector-class">.nn</span><span class="selector-class">.conv2d</span>(<span class="selector-tag">input</span>,filter_weight,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],<span class="attribute">padding</span>=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure></li><li><p>利用tf.nn.bias_add()完成加偏执项，第一个参数是该卷积层，第二个参数是偏执。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bias = tf<span class="selector-class">.nn</span><span class="selector-class">.bias_add</span>(conv,biases)</span><br></pre></td></tr></table></figure></li><li><p>将计算结果通过激活函数去线性化。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">actived_conv = tf<span class="selector-class">.nn</span><span class="selector-class">.relu</span>(bias)</span><br></pre></td></tr></table></figure></li></ol><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>&emsp;&emsp;池化层可以有效地缩小矩阵的尺寸，从而减少最后全连接层的参数。使用池化层既可以加快计算速度也有防止过拟合问题的作用。<br>&emsp;&emsp;池化层采用更加简单的最大值或者平均值计算，也叫作最大池化层(max pooling)或者平均池化层(average pooling。卷积层与池化层中过滤器移动的方式是相似的，唯一的区别在于卷积层使用的过滤器是横跨整个深度的，而池化层使用的过滤器只影响一个深度的节点。以下TensorFlow程序实现了最大池化层的前向传播算法：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pool = tf.nn.max_pool(actived_conv,ksize=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding='SAME')</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;在<code>tf.nn.max_pool</code>函数中，首先需要传入当前层的节点矩阵，第二个参数为过滤器的尺寸，且第一个和最后一个数字必须为1，第三个参数为步长信息且第一个和最后一个数字必须为1，padding也是填充选项。</p><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p>&emsp;&emsp;可以用以下正则表达式来表示一些经典的用于图片分类的问题的卷积神经网络结构：$$输入层\to（卷积层+ \to 池化层？)+\to 全连接层+$$<br>&emsp;&emsp;在以上公式中，“卷积层+”表示一层或多层卷积层，大部分卷积神经网络中一般最多连续使用三层卷积层。“池化层？”表示没有或者一层池化层。LeNet-5模型就可以表示为以下结构:$$输入层\to卷积层\to池化层\to卷积层\to池化层\to全连接层\to全连接层\to输出层$$</p><h4 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h4><p>&emsp;&emsp;解决MNIST手写数字分类的源代码：</p><p><strong>LeNet5_infernece.py:</strong><br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入为28*28的图像[28, 28]</span></span><br><span class="line"><span class="attr">INPUT_NODE</span> = <span class="number">784</span></span><br><span class="line"><span class="comment"># 输出为1~10的可能性[10]</span></span><br><span class="line"><span class="attr">OUTPUT_NODE</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像尺寸</span></span><br><span class="line"><span class="attr">IMAGE_SIZE</span> = <span class="number">28</span></span><br><span class="line"><span class="comment"># 图像的颜色通道数，这里只有黑白一种通道</span></span><br><span class="line"><span class="attr">NUM_CHANNELS</span> = <span class="number">1</span></span><br><span class="line"><span class="comment"># 标签的数量</span></span><br><span class="line"><span class="attr">NUM_LABELS</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一层卷积的深度</span></span><br><span class="line"><span class="attr">CONV1_DEEP</span> = <span class="number">32</span></span><br><span class="line"><span class="comment"># 第一层卷积的过滤器尺寸</span></span><br><span class="line"><span class="attr">CONV1_SIZE</span> = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二层卷积的深度</span></span><br><span class="line"><span class="attr">CONV2_DEEP</span> = <span class="number">64</span></span><br><span class="line"><span class="comment"># 第二层卷积的过滤器尺寸</span></span><br><span class="line"><span class="attr">CONV2_SIZE</span> = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全连接层的节点个数</span></span><br><span class="line"><span class="attr">FC_SIZE</span> = <span class="number">512</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 常见的卷积模型</span></span><br><span class="line"><span class="comment"># 本例子卷积模型 输入 -&gt; 卷积层 -&gt; 池化层 -&gt; 卷积层 -&gt; 池化层 -&gt; 全连接层 -&gt; 全连接层</span></span><br><span class="line"><span class="comment"># 输入 -&gt; (卷积层+ -&gt; 池化层?)+ -&gt; 全连接层+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def inference(input_tensor, train, regularizer):</span><br><span class="line">    <span class="comment"># 第一层卷积1</span></span><br><span class="line">    <span class="comment"># 输入为[x-size=28, y-size=28, channel=1]的图像</span></span><br><span class="line">    <span class="comment"># 过滤器尺寸[x-size=5, y-size=5, channel=1, deep=32]</span></span><br><span class="line">    <span class="comment"># 过滤器步长=1</span></span><br><span class="line">    <span class="comment"># 输出为[x-size=28, y-size=28, deep=32]的矩阵</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope('layer1-conv1'):</span><br><span class="line">        <span class="attr">conv1_weights</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="weight",</span></span><br><span class="line">            <span class="attr">shape=[CONV1_SIZE,</span> CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],</span><br><span class="line">            <span class="attr">initializer=tf.truncated_normal_initializer(stddev=0.1)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="attr">conv1_biases</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="bias",</span></span><br><span class="line">            <span class="attr">shape=[CONV1_DEEP],</span></span><br><span class="line">            <span class="attr">initializer=tf.constant_initializer(0.0)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="attr">conv1</span> = tf.nn.conv2d(input_tensor, conv1_weights, <span class="attr">strides=[1,</span> <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="attr">padding='SAME')</span></span><br><span class="line">        <span class="attr">relu1</span> = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二层池化1</span></span><br><span class="line">    <span class="comment"># 输入为[x-size=28, y-size=28, deep=32]的矩阵</span></span><br><span class="line">    <span class="comment"># 过滤器尺寸[x-size=2, y-size=2]</span></span><br><span class="line">    <span class="comment"># 过滤器步长=2</span></span><br><span class="line">    <span class="comment"># 输出为[x-size=14, y-size=14, deep=32]的矩阵</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"layer2-pool1"</span>):</span><br><span class="line">        <span class="attr">pool1</span> = tf.nn.max_pool(relu1, <span class="attr">ksize</span> = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],<span class="attr">strides=[1,2,2,1],padding="SAME")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第三层卷积2</span></span><br><span class="line">    <span class="comment"># 输入为[x-size=14, y-size=14, deep=32]的矩阵</span></span><br><span class="line">    <span class="comment"># 过滤器尺寸[x-size=5, y-size=5, channel=1, deep=64]</span></span><br><span class="line">    <span class="comment"># 过滤器步长=1</span></span><br><span class="line">    <span class="comment"># 输出为[x-size=14, y-size=14, deep=64]的矩阵</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer3-conv2"</span>):</span><br><span class="line">        <span class="attr">conv2_weights</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="weight",</span></span><br><span class="line">            <span class="attr">shape=[CONV2_SIZE,</span> CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],</span><br><span class="line">            <span class="attr">initializer=tf.truncated_normal_initializer(stddev=0.1)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="attr">conv2_biases</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="bias",</span></span><br><span class="line">            <span class="attr">shape=[CONV2_DEEP],</span></span><br><span class="line">            <span class="attr">initializer=tf.constant_initializer(0.0)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="attr">conv2</span> = tf.nn.conv2d(pool1, conv2_weights, <span class="attr">strides=[1,</span> <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="attr">padding='SAME')</span></span><br><span class="line">        <span class="attr">relu2</span> = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第四层池化2</span></span><br><span class="line">    <span class="comment"># 输入为[x-size=14, y-size=14, deep=64]的矩阵</span></span><br><span class="line">    <span class="comment"># 过滤器尺寸[x-size=2, y-size=2]</span></span><br><span class="line">    <span class="comment"># 过滤器步长=2</span></span><br><span class="line">    <span class="comment"># 输出为[x-size=7, y-size=7, deep=64]的矩阵</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"layer4-pool2"</span>):</span><br><span class="line">        <span class="attr">pool2</span> = tf.nn.max_pool(relu2, <span class="attr">ksize=[1,</span> <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], <span class="attr">strides=[1,</span> <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], <span class="attr">padding='SAME')</span></span><br><span class="line">        <span class="comment"># 把[batch, x-size, y-size, deep]4维矩阵转化为[batch, vector]2维矩阵，长*宽*深度转换为1维向量</span></span><br><span class="line">        <span class="attr">pool_shape</span> = pool2.get_shape().as_list()</span><br><span class="line">        <span class="attr">nodes</span> = pool_shape[<span class="number">1</span>] * pool_shape[<span class="number">2</span>] * pool_shape[<span class="number">3</span>]</span><br><span class="line">        <span class="attr">reshaped</span> = tf.reshape(pool2, [pool_shape[<span class="number">0</span>], nodes])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全连接层</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope('layer5-fc1'):</span><br><span class="line">        <span class="attr">fc1_weights</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="weight",</span></span><br><span class="line">            <span class="attr">shape=[nodes,</span> FC_SIZE],</span><br><span class="line">            <span class="attr">initializer=tf.truncated_normal_initializer(stddev=0.1)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 只有全连接的权重需要加入正则化</span></span><br><span class="line">        <span class="keyword">if</span> regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))</span><br><span class="line">        <span class="attr">fc1_biases</span> = tf.get_variable(<span class="string">"bias"</span>, [FC_SIZE], <span class="attr">initializer=tf.constant_initializer(0.1))</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">fc1</span> = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)</span><br><span class="line">        <span class="comment"># dropout在训练数据的时候，会随机把部分输出改为0</span></span><br><span class="line">        <span class="comment"># dropout可以避免过度拟合，dropout一般只在全连接层，而不是在卷积层或者池化层使用</span></span><br><span class="line">        <span class="keyword">if</span> train: <span class="attr">fc1</span> = tf.nn.dropout(fc1, <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全连接层</span></span><br><span class="line">    <span class="comment"># 输入为[512]的向量</span></span><br><span class="line">    <span class="comment"># 输出为[10]的向量</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope('layer6-fc2'):</span><br><span class="line">        <span class="attr">fc2_weights</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="weight",</span></span><br><span class="line">            <span class="attr">shape=[FC_SIZE,</span> NUM_LABELS],</span><br><span class="line">            <span class="attr">initializer=tf.truncated_normal_initializer(stddev=0.1)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))</span><br><span class="line">        <span class="attr">fc2_biases</span> = tf.get_variable(<span class="string">"bias"</span>, [NUM_LABELS], <span class="attr">initializer=tf.constant_initializer(0.1))</span></span><br><span class="line">        <span class="attr">logit</span> = tf.matmul(fc1, fc2_weights) + fc2_biases</span><br><span class="line"></span><br><span class="line">    return logit</span><br></pre></td></tr></table></figure></p><hr><p><strong>LeNet5_train.py</strong><br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist <span class="built_in">import</span> input_data</span><br><span class="line"><span class="built_in">import</span> LeNet5_infernece</span><br><span class="line"><span class="built_in">import</span> os</span><br><span class="line"><span class="built_in">import</span> numpy as np</span><br><span class="line"></span><br><span class="line"><span class="attr">BATCH_SIZE</span> = <span class="number">1000</span></span><br><span class="line"><span class="attr">LEARNING_RATE_BASE</span> = <span class="number">0.01</span></span><br><span class="line"><span class="attr">LEARNING_RATE_DECAY</span> = <span class="number">0.99</span></span><br><span class="line"><span class="attr">REGULARIZATION_RATE</span> = <span class="number">0.0001</span></span><br><span class="line"><span class="attr">TRAINING_STEPS</span> = <span class="number">6000</span></span><br><span class="line"><span class="attr">MOVING_AVERAGE_DECAY</span> = <span class="number">0.99</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train(mnist):</span><br><span class="line">    <span class="comment"># 定义输出为4维矩阵的placeholder</span></span><br><span class="line">    <span class="attr">x</span> = tf.placeholder(tf.float32, [</span><br><span class="line">        BATCH_SIZE,</span><br><span class="line">        LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">        LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">        LeNet5_infernece.NUM_CHANNELS],</span><br><span class="line">                       <span class="attr">name='x-input')</span></span><br><span class="line">    <span class="attr">y_</span> = tf.placeholder(tf.float32, [None, LeNet5_infernece.OUTPUT_NODE], <span class="attr">name='y-input')</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">regularizer</span> = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)</span><br><span class="line">    <span class="attr">y</span> = LeNet5_infernece.inference(x, False, regularizer)</span><br><span class="line">    <span class="attr">global_step</span> = tf.Variable(<span class="number">0</span>, <span class="attr">trainable=False)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义损失函数、学习率、滑动平均操作以及训练过程。</span></span><br><span class="line">    <span class="attr">variable_averages</span> = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    <span class="attr">variables_averages_op</span> = variable_averages.apply(tf.trainable_variables())</span><br><span class="line">    <span class="attr">cross_entropy</span> = tf.nn.sparse_softmax_cross_entropy_with_logits(<span class="attr">logits=y,</span> <span class="attr">labels=tf.argmax(y_,</span> <span class="number">1</span>))</span><br><span class="line">    <span class="attr">cross_entropy_mean</span> = tf.reduce_mean(cross_entropy)</span><br><span class="line">    <span class="attr">loss</span> = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))</span><br><span class="line">    <span class="attr">learning_rate</span> = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,</span><br><span class="line">        <span class="attr">staircase=True)</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">train_step</span> = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, <span class="attr">global_step=global_step)</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, variables_averages_op]):</span><br><span class="line">        <span class="attr">train_op</span> = tf.no_op(<span class="attr">name='train')</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化TensorFlow持久化类。</span></span><br><span class="line">    <span class="attr">saver</span> = tf.train.Saver()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() as sess:</span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        <span class="built_in">import</span> time</span><br><span class="line">        <span class="attr">start_time=time.time()</span></span><br><span class="line">        for i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">            xs, <span class="attr">ys</span> = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">            <span class="attr">reshaped_xs</span> = np.reshape(xs, (</span><br><span class="line">                BATCH_SIZE,</span><br><span class="line">                LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">                LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">                LeNet5_infernece.NUM_CHANNELS))</span><br><span class="line">            _, loss_value, <span class="attr">step</span> = sess.run([train_op, loss, global_step], <span class="attr">feed_dict=&#123;x:</span> reshaped_xs, y_: ys&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Spend %.2f s, after %d training step(s), loss on training batch is %g."</span> % (time.time()-start_time,step, loss_value))</span><br><span class="line">                <span class="attr">start_time=time.time()</span></span><br><span class="line"></span><br><span class="line">def main(<span class="attr">argv=None):</span></span><br><span class="line">    <span class="attr">mnist</span> = input_data.read_data_sets(<span class="string">"data"</span>, <span class="attr">one_hot=True)</span></span><br><span class="line">    train(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attr">__name__</span> == '__main__':</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow学习手册（二）</title>
      <link href="/2018/11/10/TensorFlow%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2018/11/10/TensorFlow%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;在本部分中，将从深度学习与深层神经网络概念介绍、如何设定神经网络的优化目标、更加详细的介绍神经网络的反向传播算法等方面来进一步介绍如何运用TensorFlow来构建神经网络。</p><h2 id="深度学习与深层神经网络"><a href="#深度学习与深层神经网络" class="headerlink" title="深度学习与深层神经网络"></a>深度学习与深层神经网络</h2><p>&emsp;&emsp;在维基百科中，深度学习的定义为“一类通过多层非线性变换对高复杂性数据建模算法的合集”。因为神经网络是实现“多层非线性变换”最常用的一种方法，所以在实际中基本可以认为深度学习就是深层神经网络的代名词。从以上可以看出，深度学习的两个重要特性——<strong>多层、非线性</strong></p><h3 id="线性模型的局限性"><a href="#线性模型的局限性" class="headerlink" title="线性模型的局限性"></a>线性模型的局限性</h3><p>&emsp;&emsp;在线性模型中，模型的输出为输入的加权和。假设一个模型的输出y和输入$x_i$满足以下关系，那么这个模型就是一个线性模型。$$y=\sum_i{w_ix_i}+b$$<br>&emsp;&emsp;其中$w_i,b\in R$为模型的参数。上面的公式就是一个线性变换，即便深层神经网络有着多层结构，也只是多个W进行矩阵乘法，与单层网络没有区别。只通过线性变换，<strong>任意层的全连接网络和单层神经网络模型的表达能力没有任何区别，而且他们都是线性模型</strong>。</p><h3 id="激活函数实现去线性化"><a href="#激活函数实现去线性化" class="headerlink" title="激活函数实现去线性化"></a>激活函数实现去线性化</h3><p>&emsp;&emsp;一般的线性神经元构成的模型都是线性模型，如果将每一个神经元的输出通过一个非线性函数，那么整个神经网络的模型也就不再是线性的了。这个非线性函数就是激活函数，下图显示了加入激活函数和偏置项之后的神经元结构。<br><img src="/2018/11/10/TensorFlow学习手册（二）/QQ20181110-132040.png" alt=""><br>&emsp;&emsp;以下公式给出了加上激活函数和偏置项后的前向传播算法的数学定义$$A_1=[a_{11},a_{12},a_{13}]=f(xW^{(1)}+b)$$<br>&emsp;&emsp;相对于之前的定义，新的公式增加了偏置项（bias），偏置项是神经网络中非常常用的一种结构；其次就是每个节点的取值不再是单纯的甲醛和。每个节点在加权和的基础上还做了一个<strong>非线性变换</strong>。下图显示了几种常用的非线性激活函数的函数图像。<br><img src="/2018/11/10/TensorFlow学习手册（二）/QQ20181110-133205.png" alt=""><br>&emsp;&emsp;目前TensorFlow提供了7种不同的非线性激活函数，<code>tf.nn.relu</code>、<code>tf.sigmoid</code>、<code>tf.tanh</code>是比较常用的几个，当然TensorFlow也支持使用自己定义的激活函数。以下代码展示了TensorFlow实现神经网络中的前向算法。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">a</span> = tf<span class="selector-class">.nn</span><span class="selector-class">.relu</span>(tf.matmul(x, w1) + biases1)</span><br><span class="line">y = tf<span class="selector-class">.nn</span><span class="selector-class">.relu</span>(tf.matmul(<span class="selector-tag">a</span>, w2) + biases2)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;TensorFlow可以很好地支持使用了激活函数和偏置项的神经网络。</p><h3 id="多层网络解决异或运算"><a href="#多层网络解决异或运算" class="headerlink" title="多层网络解决异或运算"></a>多层网络解决异或运算</h3><p>&emsp;&emsp;感知机可以简单的理解为单层的神经网络，这在我之前的博客里也<a href="https://netycc.com/2018/10/10/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/">实现</a>了。感知机会先将输入进行加权和，然后使用激活函数最后得到输出。这个结构就是一个没有隐藏层的神经网络。<br>&emsp;&emsp;深层神经网络其实是有<strong>组合特征提取</strong>的功能的，这个特性对于解决不易提取特征向量的问题有很大帮助，也是深度学习在这些问题上更加容易取得突破性进展的原因。</p><h2 id="损失函数定义"><a href="#损失函数定义" class="headerlink" title="损失函数定义"></a>损失函数定义</h2><p>&emsp;&emsp;神经网络的效果以及优化的目标是通过损失函数(loss function)来定义的。</p><h3 id="经典损失函数"><a href="#经典损失函数" class="headerlink" title="经典损失函数"></a>经典损失函数</h3><p>&emsp;&emsp;分类问题和回归问题是监督学习的两大种类。本节将会分别介绍分类问题和回归问题中使用到的经典损失函数。</p><h4 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h4><p>&emsp;&emsp;通过神经网络解决多分类问题最常用的方法是设置n个输出节点，其中n为类别的个数。对于每一个样例，神经网络可以得到一个n维数组作为输出结果。在理想情况下，如果一个样本输入类别k，那么这个类别所对应的输出节点的值应该为1，而其他节点的输出都为0.<br>&emsp;&emsp;判断一个输出的向量和期望向量有多接近有什么方法？交叉熵(cross entropy)是常用的评判方法之一，<strong>交叉熵</strong>刻画了两个概率分布之间的距离，它是分类问题中使用比较广的一种损失函数。<br>&emsp;&emsp;交叉熵是一个信息论中的概念，它原本是用来估算平均编码长度的。给定两个概率分布p和q，通过q来表示p的交叉熵为：$$H(p,q)=-\sum_x{p(x)log&ensp;{q(x)}}$$<br>&emsp;&emsp;注意交叉熵刻画的是<strong>两个概率分布之间的距离</strong>，然而神经网络的输出却不一定是一个概率分布。概率分布刻画了不同事件发生的概率。因此可以通过Softmax将神经网络的输出值转化为概率:$$softmax(y)_i=y_i`=\frac{e^{y_i}}{\sum_{j=1}^{n}{e^{y_j}}}$$<br>&emsp;&emsp;从以上公式可以看出，原始神经网络的输出被用作置信度来生成新的输出。这样就把神经网络的输出也变成了一个概率分布，从而可以通过交叉熵来计算预测的概率分布和真实答案的概率分布之间的距离了。<br>&emsp;&emsp;交叉熵函数<strong>不是对称的</strong>（$H(p,q)\ne H(q,p)$），它刻画的是通过概率分布q来表达概率分布p的困难程度。因为正确答案是希望得到的结果，所以当交叉熵作为神经网络的损失函数时，p代表的是正确答案，q代表的是预测值。<br>&emsp;&emsp;在TensorFlow中实现交叉熵代码为:<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = -<span class="keyword">tf</span>.reduce_mean(</span><br><span class="line">                    y_ * <span class="keyword">tf</span>.<span class="built_in">log</span>(<span class="keyword">tf</span>.clip_by_value(<span class="keyword">y</span>, <span class="number">1</span><span class="keyword">e</span>-<span class="number">10</span>, <span class="number">1.0</span>)))</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;其中y_代表正确结果，y代表预测结果，这一行代码包含了4个不同的TensorFlow运算。通过<code>tf.clip_by_value</code>函数可以将一个张量中的数值限制在一个范围内。如上就是限制在(1e-10,1.0)之内。<br>&emsp;&emsp;因为交叉熵一般会和softmax回归一起使用，所以TensorFlow对这两个功能进行了封装，并提供了<code>tf.nn.softmax_cross_entropy_with_logits</code>函数。比如可以直接通过以下代码来实现先softmax回归然后交叉熵的损失函数：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf<span class="selector-class">.nn</span><span class="selector-class">.softmax_cross_entropy_with_logits</span>(labels=y_,logits=y)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;其中y代表了原始神经网络的输出结果，而y_给出了标准答案。这样通过一个命令可以得到使用了Softmax回归之后的交叉熵。而在只有一个正确答案的分类问题中，TensorFlow提供了<code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>函数来进一步加速计算过程。</p><h4 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h4><p>&emsp;&emsp;与分类问题不同，回归问题解决的是对具体数值的预测。这些问题需要预测的不是一个事先定义好的类别，而是一个任意实数。解决回归问题的神经网络一般只有一个输出节点，这个节点的输出值就是预测值。对于回归问题，最常用的损失函数是均方误差（MSE）。它的定义如下：$$MSE(y,y’)=\frac{\sum_{i=1}^{n}{(y_i-y_i’)^2}}{n}$$<br>&emsp;&emsp;其中$y_i$为一个batch中第i个数据的正确答案，而$y_i’$为神经网络给出的预测值，以下代码展示了TensorFlow实现均方误差损失函数：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mse</span> = tf.reduce_mean(tf.square(y_ - y))</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;其中y代表了神经网络的输出答案，y_代表了标准答案。这里的减法运算符代表两个矩阵中对应元素的减法。</p><h3 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h3><p>&emsp;&emsp;在TensorFlow中也支持自定义损失函数，它可以使得神经网络优化的结果更加接近实际问题的需求。例如如下损失函数：<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">loss</span> = tf.reduce_sum(tf.where(tf.greater(<span class="built_in">v1</span>,<span class="built_in">v2</span>),</span><br><span class="line">                        (<span class="built_in">v1</span> - <span class="built_in">v2</span>) * a,(<span class="built_in">v2</span> - <span class="built_in">v1</span>) * <span class="keyword">b))</span></span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;以上代码用到了<code>tf.greater</code>和<code>tf.where</code>来实现选择操作。<code>tf.greater</code>的输入是两个张量，此函数会比较这两个输入张量中的每一个元素的大小，并返回比较结果。当<code>tf.greater</code>的输入张量维度不一样时，TensorFlow会进行类似numpy的广播操作的处理，<code>tf.where</code>函数有三个参数，第一个为选择条件根据，当选择条件为True时，<code>tf.where</code>函数会选择第二个参数中的值，否则使用第三个参数中的值。注意这两个操作都是元素级别进行。</p><h2 id="神经网络优化"><a href="#神经网络优化" class="headerlink" title="神经网络优化"></a>神经网络优化</h2><p>&emsp;&emsp;本部分将讨论通过<code>反向传播算法</code>和<code>梯度下降算法</code>调整神经网络中参数的取值。梯度下降法主要用于优化单个参数的取值，而反向传播算法给出了一个高效的方式在所有参数上使用梯度下降法，从而使神经网络模型在训练数据上的损失函数尽可能小。<br>&emsp;&emsp;反向传播算法是训练神经网络的核心算法，它可以根据定义好的损失函数优化神经网络中参数的取值，从而使神经网络模型在训练数据集上的损失函数达到一个较小值。神经网络模型中参数的优化过程直接决定了模型的质量，是使用神经网络时非常重要的一步。<br>&emsp;&emsp;假设用$\theta$表示神经网络中的参数，$J(\theta)$表示在给定的参数取值下，训练数据集上损失函数的大小，那么整个优化过程可以抽象为寻找一个参数$\theta$，使得$J(\theta)$最小。梯度下降算法会迭代式更新参数$\theta$，不断沿着梯度的反方向让参数朝着总损失更小的方向更新。<br>&emsp;&emsp;参数的梯度可以通过求偏导的方式计算，对于参数$\theta$，其梯度为$$\frac{\delta}{\delta\theta}J(\theta)$$<br>&emsp;&emsp;神经网络的优化过程可以分为两个阶段，第一个阶段先通过前向传播算法计算得到预测值，并将预测值和真实值做对比得出两者之间的差距。然后在第二个阶段通过反向传播算法计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数。<br>&emsp;&emsp;在训练神经网络时，参数的初始值会很大程度影响最后得到的结果。只有损失函数为凸函数时，梯度下降算法才能保证达到全局最优解。<br>&emsp;&emsp;除了不一定能达到全局最优，梯度下降算法的另一个问题就是<strong>计算时间太长</strong>，因为需要计算全部训练数据的损失函数。<br>&emsp;&emsp;为了加速训练算法，可以使用<code>随机梯度下降</code>算法。这个算法优化的不是在全部训练数据上的损失函数，而是在每一轮迭代中，随机优化某一条训练数据上的损失函数；但是随机梯度下降优化的神经网络可能无法达到局部最优。<br>&emsp;&emsp;为了综合梯度下降算法和随机梯度下降算法的优缺点，在实际应用中一般采用这两个算法的折中——每次计算一小部分训练数据的损失函数。这一小部分被称之为<code>**batch**</code>。通过矩阵运算，每次在一个<strong>batch</strong>上优化神经网络的参数并不会比单个数据慢太多。另一方面，每次使用一个<strong>batch</strong>可以大大减小收敛所需要的迭代次数，同时可以使收敛的结果更加接近梯度下降的效果。以下代码给出了在TensorFlow中如何实现神经网络的训练过程。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">batch_size = n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次读取一小部分作为当前的训练数据来执行反向传播算法。</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(batch_size, 2),name=<span class="string">"x_input"</span>)</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(batch_size, 1),name=<span class="string">"y_input"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络结构和优化算法</span></span><br><span class="line">loss = ......</span><br><span class="line">train_step = tf.train.AdamOptimizer(0.001).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练神经网络</span></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># 迭代的更新参数</span></span><br><span class="line">    current_X, current_Y = ...</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: current_X, y_: current_Y&#125;)</span><br></pre></td></tr></table></figure></p><h2 id="神经网络进一步优化"><a href="#神经网络进一步优化" class="headerlink" title="神经网络进一步优化"></a>神经网络进一步优化</h2><p>&emsp;&emsp;本部分将介绍神经网络优化过程中的可能遇到的问题，比如设置梯度下降法中的<strong>学习率</strong>、<strong>过拟合问题</strong>、<strong>滑动平均模型</strong>。</p><h3 id="学习率的设置"><a href="#学习率的设置" class="headerlink" title="学习率的设置"></a>学习率的设置</h3><p>&emsp;&emsp;<strong>学习率决定了参数每次更新的幅度。如果幅度过大，那么可能导致参数在极优值的两侧来回移动。</strong><br>&emsp;&emsp;学习率既不能过大，也不能过小。为了解决设定学习率的问题，TensorFlow提供了一种更加灵活的学习率设置方法——指数衰减法。<code>tf.train.exponential_decay</code>函数实现了指数衰减学习率。通过这个函数，可以先使用较大的学习率获得一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后期更加稳定。<code>exponential_decay</code>函数会指数级地减小学习率，它实现了以下代码的功能：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># decayed_learning_rate为每一轮batch优化时使用的学习率，learning_rate为事先规定的初始学习率</span></span><br><span class="line"><span class="comment"># decay_rate为衰减系数，decay_steps衰减速度</span></span><br><span class="line"><span class="attr">decayed_learning_rate</span> = learning_rate * decay_rate ^ (global_step / decay_steps)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;在<code>tf.train.exponential_decay</code>函数中提供了<code>staircase</code>这一参数，默认时为False，此时学习率为连续衰减形式，设置为True时为阶梯状衰减学习率。以下就是一段代码示范如何在TensorFlow中使用<code>tf.train.exponential_decay</code>函数：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过exponential_decay函数生成学习率</span></span><br><span class="line">learning_rate = tf.train.exponential_decay(0.1, global_step, 100, 0.96, staircase=True)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用指数衰减的学习率。在minimize函数中传入global_step将自动更新</span></span><br><span class="line"><span class="comment"># global_step参数，从而使得学习率也得到相应更新。</span></span><br><span class="line">learning_step = tf.train.GradientDescentOptimizer(learning_rate)\</span><br><span class="line">                    .minimize(myloss,global_step=global_step)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;以上代码设定了初始学习率为0.1，因为制定了<code>staircase=True</code>，所以每训练100轮后学习率乘以0.96。若<code>staircase=False</code>，则训练每条数据的时候学习率都会乘以0.96。</p><h3 id="过拟合问题（正则化项）"><a href="#过拟合问题（正则化项）" class="headerlink" title="过拟合问题（正则化项）"></a>过拟合问题（正则化项）</h3><p>&emsp;&emsp;之前的博客中也多次讲到了过拟合问题，一般是将loss函数后加上正则化项，常用的刻画模型复杂度的正则化项有两种，分别为L1和L2，这里就不再赘述其数学原理，在TensorFlow中，可以实现如下一个简单的带L2正则化的损失函数定义：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置初始参数，2行1列，标准差为1，随机种子1，正态分布</span></span><br><span class="line"><span class="attr">w</span> = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 设置w为从x到y中间的隐藏层</span></span><br><span class="line"><span class="attr">y</span> = tf.matmul(x,w)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置loss为y_与y的平方差均值加上w的L2范数*lambda系数,tf.square函数是为求平方</span></span><br><span class="line"><span class="attr">loss</span> = tf.reduce_mean(tf.square(y_ - y)) + tf.contrib.layers.l2_regularizer(lambda)(w)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;类似的，<code>tf.contrib.layers.l1_regularizer</code>函数可以计算L1正则化项的值。但是当网络结构复杂之后定义网络结构的部分和计算损失函数的部分可能不在同一个函数中，这样通过变量这种方式计算损失函数就不行了，在TensorFlow中可以是集合（collection），它可以在一个计算图中保存一组实体，下面代码实现了计算一个5层神经网络带L2正则化的损失函数的计算方法：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</span><br><span class="line"><span class="built_in">import</span> numpy as np</span><br><span class="line"></span><br><span class="line"><span class="attr">dataset_size</span> = <span class="number">200</span></span><br><span class="line"><span class="attr">data</span> = []</span><br><span class="line"><span class="attr">label</span> = []</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以原点为圆心，半径为1的圆把散点划分成红蓝两部分，并加入随机噪音。</span></span><br><span class="line">for i <span class="keyword">in</span> range(dataset_size):</span><br><span class="line">    <span class="attr">x1</span> = np.random.uniform(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="attr">x2</span> = np.random.uniform(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> x1 ** <span class="number">2</span> + x2 ** <span class="number">2</span> &lt;= <span class="number">1</span>:</span><br><span class="line">        data.append([np.random.normal(x1, <span class="number">0.1</span>), np.random.normal(x2, <span class="number">0.1</span>)])</span><br><span class="line">        label.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data.append([np.random.normal(x1, <span class="number">0.1</span>), np.random.normal(x2, <span class="number">0.1</span>)])</span><br><span class="line">        label.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.hstack()函数是将list在水平方向上平铺，然后.reshape将维度更改</span></span><br><span class="line"><span class="attr">data</span> = np.hstack(data).reshape(-<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="attr">label</span> = np.hstack(label).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 绘制原散点图</span></span><br><span class="line"><span class="comment"># plt.scatter(data[:, 0], data[:, 1], c=np.squeeze(label),</span></span><br><span class="line"><span class="comment">#             cmap="RdBu", vmin=-0.2, vmax=1.2, edgecolor="white")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置layer参数，入口为矩阵维度和正则化项系数</span></span><br><span class="line">def get_weight(shape, var_lambda):</span><br><span class="line">    <span class="comment"># 声明一层网络</span></span><br><span class="line">    <span class="attr">w</span> = tf.Variable(tf.random_normal(shape), <span class="attr">dtype=tf.float32)</span></span><br><span class="line">    <span class="comment"># 添加L2正则化项到losses集合中</span></span><br><span class="line">    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(var_lambda)(w))</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明batch</span></span><br><span class="line"><span class="attr">x</span> = tf.placeholder(tf.float32, <span class="attr">shape=(None,</span> <span class="number">2</span>))</span><br><span class="line"><span class="attr">y_</span> = tf.placeholder(tf.float32, <span class="attr">shape=(None,</span> <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每层节点的个数</span></span><br><span class="line"><span class="attr">layer_dimension</span> = [<span class="number">2</span>,<span class="number">10</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 声明网络结构层数</span></span><br><span class="line"><span class="attr">n_layers</span> = len(layer_dimension)</span><br><span class="line"><span class="comment"># 前一个layer</span></span><br><span class="line"><span class="attr">cur_layer</span> = x</span><br><span class="line"><span class="comment"># 前一个layer的节点数</span></span><br><span class="line"><span class="attr">in_dimension</span> = layer_dimension[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环生成网络结构，输入层X[None, 2]，隐藏层W1[2, 10]、W2[10, 5]、W3[5, 3]，输出层Y[None, 1]</span></span><br><span class="line">for i <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">    <span class="attr">out_dimension</span> = layer_dimension[i]  <span class="comment"># 该layer输出的节点数量</span></span><br><span class="line">    <span class="attr">weight</span> = get_weight([in_dimension, out_dimension], <span class="number">0.003</span>)   <span class="comment"># 设置layer，并且正则化项系数为0.003</span></span><br><span class="line">    <span class="attr">bias</span> = tf.Variable(tf.constant(<span class="number">0.1</span>, <span class="attr">shape=[out_dimension]))</span> <span class="comment"># 设置偏执项，维度按照输出维度提供</span></span><br><span class="line">    <span class="attr">cur_layer</span> = tf.nn.relu(tf.matmul(cur_layer, weight) + bias)  <span class="comment"># 该层的输出为relu(w_i*x+b)</span></span><br><span class="line">    <span class="attr">in_dimension</span> = layer_dimension[i]   <span class="comment"># 设置下层的输入节点数量</span></span><br><span class="line"></span><br><span class="line"><span class="attr">y=</span> cur_layer    <span class="comment"># 最后的输出层</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数的定义。</span></span><br><span class="line"><span class="attr">mse_loss</span> = tf.reduce_sum(tf.pow(y_ - y, <span class="number">2</span>)) / dataset_size</span><br><span class="line">tf.add_to_collection('losses', mse_loss)    <span class="comment"># 正常的损失函数加入losses集合中</span></span><br><span class="line"><span class="attr">loss</span> = tf.add_n(tf.get_collection('losses'))    <span class="comment"># 获得losses集合所有项并求和</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练的目标函数loss，训练次数及训练模型</span></span><br><span class="line"><span class="attr">train_op</span> = tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"><span class="attr">TRAINING_STEPS</span> = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    for i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">        sess.run(train_op, <span class="attr">feed_dict=&#123;x:</span> data, y_: label&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">1000</span> - <span class="number">1</span>:</span><br><span class="line">            print(<span class="string">"After %d steps, loss: %f"</span> % (i, sess.run(loss, <span class="attr">feed_dict=&#123;x:</span> data, y_: label&#125;)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 画出训练后的分割曲线</span></span><br><span class="line">    xx, <span class="attr">yy</span> = np.mgrid[-<span class="number">1</span>:<span class="number">1</span>:.<span class="number">01</span>, <span class="number">0</span>:<span class="number">2</span>:.<span class="number">01</span>]    <span class="comment"># 分别创建两个密集型网格，起始：终点：步长</span></span><br><span class="line">    print(<span class="string">"xx.ravel()"</span>+<span class="string">"*"</span>*<span class="number">100</span>,'\n',xx.ravel(),<span class="string">"yy.ravel()"</span>+'*'*<span class="number">100</span>,yy.ravel())</span><br><span class="line">    <span class="attr">grid</span> = np.c_[xx.ravel(), yy.ravel()]    <span class="comment"># 形成多个n*2的array，每组里是(x,y)</span></span><br><span class="line">                                            <span class="comment"># np.c_()函数是将两个array按照行连接起来，np.r_()是按照列连接</span></span><br><span class="line">                                            <span class="comment"># array.ravel()函数是将array变成一维，然后返回一维数据</span></span><br><span class="line">    <span class="attr">probs</span> = sess.run(y, <span class="attr">feed_dict=&#123;x:grid&#125;)</span> <span class="comment"># 将x作为输入去运行y这个计算图</span></span><br><span class="line">    <span class="attr">probs</span> = probs.reshape(xx.shape)         <span class="comment"># 按照网格重新排列</span></span><br><span class="line"></span><br><span class="line">plt.scatter(data[:,<span class="number">0</span>], data[:,<span class="number">1</span>], <span class="attr">c=np.squeeze(label),</span></span><br><span class="line">           <span class="attr">cmap="RdBu",</span> <span class="attr">vmin=-.2,</span> <span class="attr">vmax=1.2,</span> <span class="attr">edgecolor="white")</span></span><br><span class="line"><span class="comment"># 填充网格中的等高线</span></span><br><span class="line">plt.contour(xx, yy, probs, <span class="attr">levels=[.5],</span> <span class="attr">cmap="Greys",</span> <span class="attr">vmin=0,</span> <span class="attr">vmax=.1)</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/2018/11/10/TensorFlow学习手册（二）/jieguo.png" alt="分类结果"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow学习手册（一）</title>
      <link href="/2018/11/09/TensorFlow%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2018/11/09/TensorFlow%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;从老师那里得知，未来我们使用的比较多的都是深度学习，因此想要系统地学习一下TensorFlow的使用，之前都是半半拉拉的初学而已。</p><h2 id="TensorFlow入门"><a href="#TensorFlow入门" class="headerlink" title="TensorFlow入门"></a>TensorFlow入门</h2><h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><p>&emsp;&emsp;在TensorFlow中，张量可以被简单地理解为多维数组。其中零阶张量可以理解为标量，也就是一个数；第一阶张量为向量，也就是一个一维数组；第n阶张量可以理解为一个n维数组。TensorFlow中张量中没有真正的保存数字，它保存的是如何得到这些数字的计算过程，<strong>获得是对结果的一个引用</strong>。</p><h3 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h3><p>&emsp;&emsp;在TensorFlow中，是使用会话（session）来执行定义好的运算。会话拥有并管理TensorFlow程序运行时的所有资源。所有计算完成之后需要关闭会话来帮助系统回收资源，<strong>否则可能出现资源泄露</strong>。<br>&emsp;&emsp;可以使用python的上下文管理器来使用会话。<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.<span class="built_in">run</span>()</span><br><span class="line"><span class="comment"># 不需要调用"session.close()"函数关闭会话</span></span><br><span class="line"><span class="comment"># 当上下文退出时会话关闭和资源释放叶自动完成了</span></span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;TensorFlow中会有着这样的机制，它不会自动生成默认的会话，而是手动指定。当默认的会话被指定之后可以通过tf.Tensor.eval函数来计算一个张量的取值。以下代码展示了通过设定默认会话计算张量的取值。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sess</span>=tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下两个命令有着相同的功能</span></span><br><span class="line"><span class="builtin-name">print</span>(sess.<span class="builtin-name">run</span>(result))</span><br><span class="line"><span class="builtin-name">print</span>(result.eval(<span class="attribute">session</span>=sess))</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;TensorFlow提供了一种在交互式环境下直接构建默认会话的函数。这个函数就是tf.InteractiveSession。使用这个函数会自动将生成的会话注册为默认会话。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sess</span>=tf.InteractiveSession()</span><br><span class="line"><span class="builtin-name">print</span>(result.eval())</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;并且也可以通过ConfigProto Protocol Buffer来配置需要生成的会话，下面给出了通过ConfigProto配置会话的方法；<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">config</span> = tf.ConfigProto(allow_soft_placement=<span class="literal">True</span>,log_device_placement=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="attr">sess1</span>=tf.InteractiveSession(config=config)</span><br><span class="line"><span class="attr">sess2</span>=tf.Session(config=config)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;通过ConfigProto可以配置类似并行的线程数、GPU分配策略、运算超时时间等参数，在这些参数中，最常用的有两个。第一个是allow_soft_placement，这是一个布尔型的参数，当它为True时，在以下任意一个条件成立时，GPU上的运算可以放到CPU上进行：</p><ol><li>运算无法在GPU上执行。</li><li>没有GPU资源（比如运算被放在第二个GPU上运行，但是机器上只有一个GPU）</li><li>运算输入包含对CPU计算结果的引用</li></ol><p>&emsp;&emsp;这个参数一般默认为False，但是为了使得代码的可移植性更强，在有GPU的环境下，这个参数一般会被设置为True。<br>&emsp;&emsp;第二个使用的比较多的配置参数是log_device_placement。这也是一个布尔型的参数，当它为True是日志中会详细记录以方便调试。而在生产环境中将该参数设置为Flase可以减少日志量。</p><h2 id="前向传播算法介绍"><a href="#前向传播算法介绍" class="headerlink" title="前向传播算法介绍"></a>前向传播算法介绍</h2><p><img src="/2018/11/09/TensorFlow学习手册（一）/QQ20181109-203545.png" alt="神经网络前向传播算法示意图"></p><p>&emsp;&emsp;前向传播算法通过矩阵乘法的方式表达。在TensorFlow中矩阵乘法是非常容易实现的。以下TensorFlow程序实现了如图所示的前向传播算法过程。<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">a</span> = tf.matmul(x,w1)</span><br><span class="line"><span class="attr">y</span> = tf.matmul(a,w2)</span><br></pre></td></tr></table></figure></p><h2 id="TensorFlow变量"><a href="#TensorFlow变量" class="headerlink" title="TensorFlow变量"></a>TensorFlow变量</h2><p>&emsp;&emsp;在TensorFlow中变量(tf.Variable)的作用就是保存和更新神经网络中的参数。变量一定是要初始化的，一般使用随机数给变量进行初始化，下面一段代码就是声明一个$2\times3$的矩阵变量的方法：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 产生<span class="number">2</span>*<span class="number">3</span>的矩阵，矩阵中的元素是均值为<span class="number">0</span>，标准差为<span class="number">2</span>的随机数。</span><br><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">2</span>))</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;这段代码调用了TensorFlow变量的声明函数tf.Variable。在变量声明函数中给出了初始化这个变量的方法。<code>tf.random_normal</code>函数是通过参数<code>mean</code>来指定平均值，当没有指定的时候默认为0。下表为Tensorflow支持的部分随机数生成函数<strong>（随机数生成函数）</strong>。</p><table><thead><tr><th>函数名称</th><th>随机数分布</th><th>主要参数</th></tr></thead><tbody><tr><td>tf.random_normal</td><td>正态分布</td><td>平均值、标准差、取值类型</td></tr><tr><td>tf.truncated_normal</td><td>正态分布，但如果随机出来的值偏离平均值超过2个标准差，那么这个数将会被重新随机</td><td>平均值、标准差、取值类型</td></tr><tr><td>tf.random_uniform</td><td>均匀分布</td><td>最小、最大取值、取值类型</td></tr><tr><td>tf.random_gamma</td><td>Gamma分布</td><td>形状参数alpha、尺度参数beta、取值类型</td></tr></tbody></table><p>&emsp;&emsp;TensorFlow也支持通过常数来初始化一个变量。下表给出了TensorFlow中常用的变量声明方法<strong>（常数生成函数）</strong>。</p><table><thead><tr><th>函数名称</th><th>功能</th><th>样例</th></tr></thead><tbody><tr><td>tf.zeros</td><td>产生全0的数组</td><td>tf.zeros([2,3],int32)-&gt;[[0,0,0][0,0,0]]</td></tr><tr><td>tf.ones</td><td>产生全1的数组</td><td>tf.ones([2,3],int32)-&gt;[[1,1,1][1,1,1]]</td></tr><tr><td>tf.fill</td><td>产生全固定数字的数组</td><td>tf.fill([2,3],9)-&gt;[[9,9,9][9,9,9]]</td></tr><tr><td>tf.constant</td><td>产生一个给定值的常量</td><td>tf.constant([1,2,3])-&gt;[1,2,3]</td></tr></tbody></table><p>&emsp;&emsp;虽然直接调用每个变量的初始化过程是一个可行的方案，但是当变量增多，或者变量之间存在依赖关系，单个调用的方案就比较麻烦了。为了解决这个问题，TensorFlow提供了一种更加便捷的方式来完成变量初始化过程。通过<code>tf.global_variables_initalizer</code>函数实现初始化所有变量的过程：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line">sess.run(init_op)</span><br></pre></td></tr></table></figure></p><h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p>&emsp;&emsp;反向传播算法实现了一个迭代的过程。在每次迭代的开始首先需要选取一小部分训练数据，这一小部分数据叫做一个batch。然后，这个batch的样例会通过前向传播算法得到神经网络模型的预测结果。因为训练数据都有数据标注，所以可以计算出当前神经网络模型的预测答案和标准答案之间的差距(LOSS)，最后，基于loss反向传播算法会相应更新神经网络参数的取值，使得在这个batch上神经网络模型的预测结果和真实答案更加接近。</p><p>&emsp;&emsp;为了避免数据过大而导致的计算图过大，TensorFlow提供了placeholder机制用于提供输入数据。placeholder相当于定义了一个位置，这个位置中的数据在程序运行时再指定。在placeholder定义时，这个位置上的数据类型是需要指定的。和其他张量一样，placeholder的类型是不可改变的。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32,shape=(None, 2),<span class="attribute">name</span>=<span class="string">"input"</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span></span><br><span class="line"></span><br><span class="line"><span class="builtin-name">print</span>(sess.<span class="builtin-name">run</span>(y,feed_dict=&#123;x:[ [] , [] , [] ]&#125;))</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;在得到一个batch的前向传播结果之后，需要定义一个损失函数来刻画当前的预测值和真实答案之间的差距。然后通过反向传播算法来调整神经网络参数的取值使得差距可以被缩小。以下代码定义了一个简单的损失函数，并通过TensorFlow定义了反向传播的算法：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用sigmoid函数使y转化为0~1之间的概率，y为预测是正样本的概率</span></span><br><span class="line">y = tf.sigmoid(y)</span><br><span class="line"><span class="comment"># 定义损失函数来刻画预测值与真实值之间的差距</span></span><br><span class="line">cross_entropy = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)),</span><br><span class="line">                                    +(1-y)*tf.log(tf.clip_by_value(1-y,1e-10,1.0)))</span><br><span class="line"><span class="comment"># 定义学习率</span></span><br><span class="line">learning_rate = 0.001</span><br><span class="line"><span class="comment"># 定义反向传播算法来优化神经网络中的参数</span></span><br><span class="line">train_step=tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;在以上代码中，cross_entropy定义了真实值和预测值之间的交叉熵，这是分类问题中的一个损失函数。第二行train_step定义了反向传播的优化方法。常用的优化方法有三种：</p><ol><li><code>tf.train.GradientDescentOptimizer</code></li><li><code>tf.train.AdamOptimizer</code></li><li><code>tf.train.MomentumOptimizer</code></li></ol><p>&emsp;&emsp;在定义了反向传播算法之后，通过运行sess.run(train_step)就可以对所有在<code>GraphKeys.TRAINABLE_VARIABLES</code>集合中的变量进行优化，使得在当前batch下损失函数更小。</p><h2 id="神经网络样例程序"><a href="#神经网络样例程序" class="headerlink" title="神经网络样例程序"></a>神经网络样例程序</h2><p>&emsp;&emsp;最后，叙述一下训练神经网络的三个步骤：</p><ol><li>定义神经网络的结构和前向传输的输出结果</li><li>定义损失函数以及选择反向传播优化的算法</li><li>声称会话并且在训练数据上反复运行反向传播优化算法</li></ol><p>&emsp;&emsp;这里列出我们的样例程序。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Numpy是一个科学计算的工具包，可以通过NumPy工具包生成模拟数据集</span></span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> RandomState</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练数据batch的大小</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的参数，这里还是沿用3.4.2小节中给出的神经网络结构</span></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>),name=<span class="string">"w1"</span>)</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>),name=<span class="string">"w2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">#在shape的一个维度上使用None可以方便使用不同的batch大小。在训练时需要把数据分成比较小的</span></span><br><span class="line"><span class="string">batch，但是在测试时，可以一次性使用全部数据，当数据集比较小的时候这样也比较方便，</span></span><br><span class="line"><span class="string">但是当数据集比较大的时候，将大量的数据放入一个batch可能会导致内存溢出。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>),name=<span class="string">'x-input'</span>)</span><br><span class="line">y_ = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>),name=<span class="string">'y-input'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络前向传播的过程</span></span><br><span class="line">a = tf.matmul(x,w1)</span><br><span class="line">y = tf.matmul(a,w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和反向传播算法的过程。</span></span><br><span class="line">y = tf.sigmoid(y)</span><br><span class="line">cross_entropy = -tf.reduce_mean(</span><br><span class="line">    y_*tf.log(tf.clip_by_value(<span class="number">1</span>-y,<span class="number">1e-10</span>,<span class="number">1.0</span>))</span><br><span class="line">             +(<span class="number">1</span>-y)*tf.log(tf.clip_by_value(<span class="number">1</span>-y,<span class="number">1e-10</span>,<span class="number">1.0</span>)))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过随机数生成一个模拟数据集</span></span><br><span class="line">rdm = RandomState(<span class="number">1</span>)</span><br><span class="line">dataset_size = <span class="number">6400</span></span><br><span class="line">X = rdm.rand(dataset_size,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 定义规则来给出样本的标签，在这里所有x1+x2&lt;1的样例都被认为是正样本（比如零件合格），</span></span><br><span class="line"><span class="comment"># 而其他为负样本（比如零件不合格）。0表示负样本，1表示正样本</span></span><br><span class="line">Y = [[int(x1+x2&lt;<span class="number">1</span>)] <span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个回话来运行TensorFlow程序</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op=tf.global_variables_initializer()</span><br><span class="line">    <span class="comment">#初始化变量</span></span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"After w1:\n"</span>,sess.run(w1))</span><br><span class="line">    print(<span class="string">"After w2:\n"</span>,sess.run(w2))</span><br><span class="line">    print(<span class="string">"="</span>*<span class="number">80</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#设定训练的轮数</span></span><br><span class="line">    STEPS=<span class="number">5000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        <span class="comment"># 每次选取batch_size个体样本进行训练</span></span><br><span class="line">        start = (i*batch_size) % dataset_size</span><br><span class="line">        end = min(start+batch_size,dataset_size)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#通过选取的样本训练神经网络并更新参数</span></span><br><span class="line">        sess.run(train_step,</span><br><span class="line">                feed_dict=&#123;x:X[start:end],y_:Y[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment">#每隔一段时间计算在所有数据上的交叉熵并输出</span></span><br><span class="line">            total_cross_entropy = sess.run(</span><br><span class="line">                cross_entropy,feed_dict=&#123;x:X,y_:Y&#125;)</span><br><span class="line">            print(<span class="string">"After %d training step(s),cross entropy on all data is %g"</span> %</span><br><span class="line">                 (i,total_cross_entropy))</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"="</span>*<span class="number">80</span>)</span><br><span class="line">    print(<span class="string">"final w1: \n"</span>,sess.run(w1))</span><br><span class="line">    print(<span class="string">"final w2: \n"</span>,sess.run(w2))</span><br></pre></td></tr></table></figure></p><p><img src="/2018/11/09/TensorFlow学习手册（一）/QQ20181109-184739.png" alt="运行输出"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Ubuntu16.04显卡驱动安装及环境配置</title>
      <link href="/2018/10/31/ubuntu16-04%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%E5%AE%89%E8%A3%85%E5%8F%8A%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/10/31/ubuntu16-04%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%E5%AE%89%E8%A3%85%E5%8F%8A%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在新的Ubuntu电脑上进行环境配置，由于系统不是自己弄的，因此也不知道工作人员弄了什么情况的系统，所以持续排坑，装环境。</p><h2 id="安装ssh"><a href="#安装ssh" class="headerlink" title="安装ssh"></a>安装ssh</h2><p>这一步是基础步骤，为了能够远程访问该计算机。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install openssh-server</span><br><span class="line">sudo<span class="built_in"> service </span>ssh restart</span><br></pre></td></tr></table></figure></p><h2 id="安装Anaconda3"><a href="#安装Anaconda3" class="headerlink" title="安装Anaconda3"></a>安装Anaconda3</h2><p>从清华源中获得Anaconda3的<strong>5.2.0版本</strong>，因为5.3.0版本是基于python3.7的，暂时没有tensorflow等库的适配。<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">bash</span> <span class="selector-tag">Anaconda3-5</span><span class="selector-class">.2</span><span class="selector-class">.0-Linux-x86_64</span><span class="selector-class">.sh</span></span><br><span class="line"># 注意，需要同意它写入<span class="selector-tag">path</span>中去，然后更新环境变量</span><br><span class="line"><span class="selector-tag">source</span> <span class="selector-class">.bashrc</span></span><br></pre></td></tr></table></figure></p><h2 id="更换python源"><a href="#更换python源" class="headerlink" title="更换python源"></a>更换python源</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> ~/.pip</span><br><span class="line"><span class="keyword">vim</span> ~/.pip/pip.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure><p>在pip.conf中添加如下内容：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = https:<span class="comment">//pypi.tuna.tsinghua.edu.cn/simple</span></span><br><span class="line">trusted-host = pypi<span class="selector-class">.tuna</span><span class="selector-class">.tsinghua</span><span class="selector-class">.edu</span><span class="selector-class">.cn</span></span><br></pre></td></tr></table></figure></p><h2 id="卸载原生显卡驱动"><a href="#卸载原生显卡驱动" class="headerlink" title="卸载原生显卡驱动"></a>卸载原生显卡驱动</h2><p>因为系统中本身的显卡驱动是工作人员双击安装的app，因此版本很老，并且不适配cuda，因此我们需要重新安装。<br>卸载NVIDIA驱动<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> <span class="builtin-name">remove</span> --purge nvidia-*</span><br><span class="line">sudo apt-<span class="builtin-name">get</span> autoremove</span><br></pre></td></tr></table></figure></p><h2 id="安装官网新的驱动"><a href="#安装官网新的驱动" class="headerlink" title="安装官网新的驱动"></a>安装官网新的驱动</h2><p>先说一下，我们需要的几个软件包，NVIDIA驱动、<a href="https://developer.nvidia.com/cuda-80-ga2-download-archive" target="_blank" rel="noopener">cuda8.0</a>，都可以在官网在下载到。</p><ol><li><p>修改/etc/modprobe.d/blacklist.conf，添加如下内容：</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">blacklist </span>vag16fb</span><br><span class="line"><span class="keyword">blacklist </span>nouveau  </span><br><span class="line"><span class="keyword">blacklist </span>rivafb</span><br><span class="line"><span class="keyword">blacklist </span>rivatv</span><br><span class="line"><span class="keyword">blacklist </span>nvidiafb</span><br></pre></td></tr></table></figure></li><li><p>新建blacklist-nouveau.conf文件，sudo nano /etc/modprobe.d/blacklist-nouveau.conf,并在文件中输入命令，保存并退出。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">blacklist </span>nouveau</span><br><span class="line"><span class="keyword">blacklist </span><span class="keyword">lbm-nouveau</span></span><br><span class="line"><span class="keyword">options </span>nouveau modeset=<span class="number">0</span></span><br><span class="line">alias nouveau off</span><br><span class="line">alias <span class="keyword">lbm-nouveau </span>off</span><br></pre></td></tr></table></figure></li><li><p>更新内核及配置文件</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sudo update-initramfs -u</span></span><br><span class="line"><span class="attribute">sudo reboot</span></span><br></pre></td></tr></table></figure></li><li><p>重新安装NVIDIA驱动</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo<span class="built_in"> service </span>lightdm stop</span><br><span class="line">sudo sh ./NVIDIA-Linux-x86_64-xxx.<span class="builtin-name">run</span> --no-opengl-files</span><br><span class="line">(sudo sh ./NVIDIA.<span class="builtin-name">run</span> -no-x-check -no-nouveau-check -no-opengl-files这句也可以不用)</span><br><span class="line">sudo<span class="built_in"> service </span>lightdm start</span><br></pre></td></tr></table></figure></li><li><p>重启</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sudo reboot</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="安装cuda"><a href="#安装cuda" class="headerlink" title="安装cuda"></a>安装cuda</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">bash</span> <span class="selector-tag">cuda_8</span><span class="selector-class">.0</span><span class="selector-class">.61_375</span><span class="selector-class">.26_linux</span><span class="selector-class">.run</span></span><br></pre></td></tr></table></figure><h2 id="安装TensorFlow-gpu版本"><a href="#安装TensorFlow-gpu版本" class="headerlink" title="安装TensorFlow-gpu版本"></a>安装TensorFlow-gpu版本</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> tensorflow-gpu</span><br></pre></td></tr></table></figure><p>我遇到了权限问题，可以用以下语句解决：<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chown -R <span class="string">[username]</span>:<span class="string">[username]</span> /home/<span class="string">[username]</span>/anaconda3</span><br></pre></td></tr></table></figure></p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><p>最后配置就完成了，以下为本人参考博客：</p><ol><li><a href="https://blog.csdn.net/gaowu959/article/details/79596724" target="_blank" rel="noopener">Ubuntu16.04+1080ti显卡驱动安装流程+循环登录问题【集锦】</a></li><li><a href="https://blog.csdn.net/u013000139/article/details/72991881" target="_blank" rel="noopener">NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the lat</a></li><li><a href="https://blog.csdn.net/jizhidexiaoming/article/details/79526123" target="_blank" rel="noopener">Ubuntu16.04安装tensorflow_gpu教程</a></li><li><a href="https://blog.csdn.net/Guangli_R/article/details/80859479" target="_blank" rel="noopener">ubuntu conda 更新、下载模块包权限问题 ‘Permission denied’</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> 显卡驱动配置 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>隐马尔可夫模型及分词上的实现</title>
      <link href="/2018/10/28/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%88%86%E8%AF%8D%E4%B8%8A%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
      <url>/2018/10/28/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%88%86%E8%AF%8D%E4%B8%8A%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近开始看自然语言处理的实战部分，看到了<strong>统计分词</strong>那里，统计分词用的模型是<strong>隐马尔可夫模型</strong>(HMM)，这里先介绍一下这个模型，它是可用于标注问题的统计学模型，描述由隐藏的马尔可夫链随机生成观测序列的过程，属于<strong>生成模型</strong>。</p><h2 id="隐马尔可夫模型的基本概念"><a href="#隐马尔可夫模型的基本概念" class="headerlink" title="隐马尔可夫模型的基本概念"></a>隐马尔可夫模型的基本概念</h2><h3 id="隐马尔可夫模型的定义"><a href="#隐马尔可夫模型的定义" class="headerlink" title="隐马尔可夫模型的定义"></a>隐马尔可夫模型的定义</h3><blockquote><p>定义10.1(隐马尔可夫模型)<br>隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马尔可夫链随机生成的状态的序列，称为<strong>状态序列</strong>；每个状态生成一个观测，而由此产生的观测的随机序列，称为<strong>观测序列</strong>。序列的每一个位置又可以看作是一个时刻。</p></blockquote><p>隐马尔可夫模型由初始概率分布、状态转移概率分布以及观测概率分布确定。隐马尔可夫模型的形式定义如下：<br>设$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合。$$Q=\{q_1,q_2,…,q_N\}，V=\{v_1,v_2,…,v_M\}$$其中，$N$是可能的状态数，$M$是可能的观测数.以下$I$是长度为$T$的状态序列，$O$是对应的观测序列。$$I=(i_1,i_2,…,i_T)，O=(o_1,o_2,…,o_T)$$$A$是状态转移概率矩阵.$$A=[a_{ij}]_{N\times N}\tag{10.1}$$</p><p>其中，$$a_{ij}=P(i_{t+1}=q_j|i_t=q_i)，i=1,2,…,N;j=1,2,…,N\tag{10.2}$$是在时刻t处于$q_i$的条件下在时刻$t+1$转移到状态$q_j$的概率。<br>$B$是观测概率矩阵：$$B=[b_j(k)]_{N\times M}\tag{10.3}$$其中，$$b_j(k)=P(o_t=v_k|i_t=q_j)，k=1,2,…,M;j=1,2,…,N\tag{10.4}$$是在时刻$t$处于状态$q_j$的条件下生成观测$v_k$的概率。<br>$\pi$是初始状态概率向量：$$\pi=(\pi_i)\tag{10.5}$$其中，$$\pi_i=P(i_1=q_i)，i=1,2,…,N\tag{10.6}$$是时刻$t=1$处于状态$q_i$的概率。<br>隐马尔可夫模型由初始状态概率向量$\pi$、状态转移概率矩阵$A$和观测概率矩阵$B$决定。$\pi$和$A$决定状态序列，$B$决定观测序列。因此，隐马尔可夫模型$\lambda$可以用三元符号表示，即$$\lambda=(A,B,\pi)\tag{10.7}$$<br>$A,B,\pi$称为隐马尔可夫模型的三要素。<br>状态转移概率矩阵$A$与初始状态概率向量$\pi$确定了隐藏的马尔可夫链，生成不可观测的状态序列。观测概率矩阵$B$确定了如何从状态生成观测，与状态序列综合确定了如何产生观测序列。<br>从定义上可知，隐马尔可夫模型作了两个基本假设：</p><ol><li>齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻$t$的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻$t$无关。$$P(i_t|i_{t-1},o_{t-1},…,i_1,o_1)=P(i_t|i_{t-1})，t=1,2,…,T\tag{10.8}$$</li><li>观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关。$$P(o_t|i_T,o_T,i_{T-1},o_{T-1},…,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},…,i_1,o_1)=P(o_t|i_t)\tag{10.9}$$</li></ol><p>&emsp;&emsp;隐马尔可夫模型可以用于标注，这时状态对应着标记。标注问题是给定观测的序列预测其对应的标记序列。可以假设标注问题的数据是由隐马尔可夫模型生成的。这样我们可以利用隐马尔可夫模型的学习与预测算法进行标注。</p><h3 id="观测序列的生成过程"><a href="#观测序列的生成过程" class="headerlink" title="观测序列的生成过程"></a>观测序列的生成过程</h3><p>&emsp;&emsp;根据隐马尔可夫模型定义，可以将一个长度为$T$的观测序列$O=(O_1,O_2,…,O_T)$的生成过程描述如下：</p><blockquote><p>算法10.1（观测序列的生成）<br>输入：隐马尔可夫模型$\lambda=(A,B,\pi)$，观测序列长度为$T$；<br>输出：观测序列$O=(o_1,o_2,…,o_T)$<br>(1)按照初始状态分布$\pi$产生状态$i_1$<br>(2)令$t=1$<br>(3)按照状态$i_t$的观测概率分布$b_{i_t}(k)$生成$o_t$<br>(4)按照状态$i_t$的状态转移概率分布$\{a_{i_ti_{t+1}}\}$产生状态$i_t+1，i_t+1=1,2,…,N$<br>(5)令$t=t+1$；如果$t&lt;T$，转步(3)；否则，终止。</p></blockquote><h3 id="隐马尔可夫模型的3个基本问题"><a href="#隐马尔可夫模型的3个基本问题" class="headerlink" title="隐马尔可夫模型的3个基本问题"></a>隐马尔可夫模型的3个基本问题</h3><p>隐马尔可夫模型有三个基本问题：</p><ol><li>概率计算问题。给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$.</li><li>学习问题。已知观测序列$O=(o_1,o_2,…,o_T)$，估计模型$\lambda=(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大。即用极大似然估计的方法估计参数。</li><li>预测问题。已知模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,…,i_T)$。即给定观测序列，求最有可能的对应的状态序列。</li></ol><h2 id="概率计算方法"><a href="#概率计算方法" class="headerlink" title="概率计算方法"></a>概率计算方法</h2><p>&emsp;&emsp;在这一节中，介绍一下计算观测序列概率$P(O|\lambda)$的前向(forward)和后向(backward)算法。先介绍概念上可行但<em>计算上不可行</em>的直接计算法。</p><h3 id="直接计算法"><a href="#直接计算法" class="headerlink" title="直接计算法"></a>直接计算法</h3><p>&emsp;&emsp;给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，计算观测序列$O$出现的概率$P(O|\lambda)$。最直接的方法是按概率公式直接计算。通过列举所有可能的长度为$T$的状态序列$I=(i_1,i_2,…,i_T)$，求各个状态序列I与观测序列$O=(o_1,o_2,…,o_T)$的联合概率$P(O,I|\lambda)$，然后对所有可能的状态序列求和，得到$P(O|\lambda)$.<br>&emsp;&emsp;状态序列$I=(i_1,i_2,…,i_T)$的概率是$$P(I|\lambda)=\pi_{i_1}a_{i_1i_2}a_{i_2i_3}…a_{i_{T-1}i_T}\tag{10.10}$$<br>&emsp;&emsp;对固定的状态序列I=（i_1,i_2,…,i_T)，观测序列$O=(o_1,o_2,…,o_T)$的概率是$P(O|I,\lambda)=b_{i_1}(o_1)b_{i_2}(o_2)…b_{i_T}(o_T)\tag{10.11}$<br>&emsp;&emsp;$O$和$I$同时出现的联合概率为$$P(O|I,\lambda)=P(I|\lambda)=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)…a_{i_{T-1}i_T}b_{i_T}(O_T)\tag{10.12}$$<br>&emsp;&emsp;然后，对所有可能的状态序列$I$求和，得到观测序列$O$的概率$P(O|\lambda)$，即$$<br>\begin{split}<br>P(O|\lambda)&amp;=\sum_IP(O|I,\lambda)P(I|\lambda)\\<br>&amp;=\sum_{i_1,i_2,…,i_T}{\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)…a_{i_{T-1}i_T}b_{i_T}(O_T)}<br>\end{split}\tag{10.13}<br>$$<br>&emsp;&emsp;但是，利用公式(10.13)计算量很大，是$O(TN^T)$阶的，这种算法不可行。</p><h3 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h3><p>&emsp;&emsp;首先定义前向概率</p><blockquote><p>定义10.2（前向概率）&emsp;给定隐马尔可夫模型$\lambda$，定义到时刻$t$部分观测序列为$o_1,o_2,…,o_t$且状态为$q_i$的概率为前向概率，记作$$\alpha _t(i)=P(o_1,o_2,…,o_t,i_t=q_i|\lambda)\tag{10.14}$$</p></blockquote><p>&emsp;&emsp;可以递推地求得前向概率$\alpha_t(i)$及观测序列概率$P(O|\lambda)$。</p><blockquote><p>算法10.2（观测序列概率的前向算法）<br>输入：隐马尔可夫模型$\lambda$，观测序列$O$;<br>输出：观测序列概率$P(O|\lambda)$.</p><ol><li>初值$$\alpha_1(i)=\pi_ib_i(o_1),i=1,2,…,N\tag{10.15}$$</li><li>递推&emsp;对$t=1,2,…,T-1$.$$\alpha_{t+1}(i)=[\sum_{j=1}^{N}\alpha_t(j)a_{ji}]b_i(o_t+1),i=1,2,…,N\tag{10.16}$$</li><li>终止$$P(O|\lambda)=\sum_{i=1}^{N}\alpha_T(i)\tag{10.17}$$</li></ol></blockquote><p><img src="/2018/10/28/隐马尔可夫模型及分词上的实现/QQ20181105-211743.png" alt="图 10.1 前向概率的递推公式"><br><img src="/2018/10/28/隐马尔可夫模型及分词上的实现/QQ20181105-213913.png" alt="图 10.2 观测序列路径结构"></p><blockquote><p>未完待续</p></blockquote><h2 id="HMM在统计分词中的实现"><a href="#HMM在统计分词中的实现" class="headerlink" title="HMM在统计分词中的实现"></a>HMM在统计分词中的实现</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br></pre></td><td class="code"><pre><span class="line">class HMM(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        import os</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 主要是用于存取算法中间结果，不用每次都训练模型</span></span><br><span class="line">        self.model_file = './data/hmm_model.pkl'</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 状态值集合</span></span><br><span class="line">        self.state_list = ['B', 'M', 'E', 'S']</span><br><span class="line">        <span class="comment"># 参数加载,用于判断是否需要重新加载model_file</span></span><br><span class="line">        self.load_para = False</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用于加载已计算的中间结果，当需要重新训练时，需初始化清空结果</span></span><br><span class="line">    def try_load_model(self, trained):</span><br><span class="line">        if trained:</span><br><span class="line">            import pickle</span><br><span class="line">            <span class="keyword">with</span> <span class="keyword">open</span>(self.model_file, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                self.A_dic = pickle.load(f)</span><br><span class="line">                self.B_dic = pickle.load(f)</span><br><span class="line">                self.Pi_dic = pickle.load(f)</span><br><span class="line">                self.load_para = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 状态转移概率（状态-&gt;状态的条件概率）</span></span><br><span class="line">            self.A_dic = &#123;&#125;</span><br><span class="line">            <span class="comment"># 发射概率（状态-&gt;词语的条件概率）</span></span><br><span class="line">            self.B_dic = &#123;&#125;</span><br><span class="line">            <span class="comment"># 状态的初始概率</span></span><br><span class="line">            self.Pi_dic = &#123;&#125;</span><br><span class="line">            self.load_para = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算转移概率、发射概率以及初始概率</span></span><br><span class="line">    <span class="keyword">def</span> train(<span class="keyword">self</span>, <span class="keyword">path</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重置几个概率矩阵</span></span><br><span class="line">        self.try_load_model(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 统计状态出现次数，求p(o)</span></span><br><span class="line">        Count_dic = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化参数</span></span><br><span class="line">        <span class="keyword">def</span> init_parameters():</span><br><span class="line">            <span class="keyword">for</span> state <span class="keyword">in</span> self.state_list:</span><br><span class="line">                self.A_dic[state] = &#123;s: <span class="number">0.0</span> <span class="keyword">for</span> s <span class="keyword">in</span> self.state_list&#125;</span><br><span class="line">                self.Pi_dic[state] = <span class="number">0.0</span></span><br><span class="line">                self.B_dic[state] = &#123;&#125;</span><br><span class="line"></span><br><span class="line">                Count_dic[state] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> makeLabel(<span class="built_in">text</span>):</span><br><span class="line">            out_text = []</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">len</span>(<span class="built_in">text</span>) == <span class="number">1</span>:</span><br><span class="line">                out_text.append(<span class="string">'S'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                out_text += [<span class="string">'B'</span>] + [<span class="string">'M'</span>] * (<span class="keyword">len</span>(<span class="built_in">text</span>) - <span class="number">2</span>) + [<span class="string">'E'</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> out_text</span><br><span class="line"></span><br><span class="line">        init_parameters()</span><br><span class="line">        line_num = <span class="number">-1</span></span><br><span class="line">        <span class="comment"># 观察者集合，主要是字以及标点等</span></span><br><span class="line">        words = <span class="keyword">set</span>()</span><br><span class="line">        <span class="keyword">with</span> <span class="keyword">open</span>(<span class="keyword">path</span>, <span class="keyword">encoding</span>=<span class="string">'utf8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                line_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                line = line.strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                    continue</span><br><span class="line"></span><br><span class="line">                word_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> line <span class="keyword">if</span> i != <span class="string">' '</span>]</span><br><span class="line">                words |= <span class="keyword">set</span>(word_list)  <span class="comment"># 更新字的集合</span></span><br><span class="line"></span><br><span class="line">                linelist = line.split()</span><br><span class="line"></span><br><span class="line">                line_state = []</span><br><span class="line">                <span class="keyword">for</span> w <span class="keyword">in</span> linelist:</span><br><span class="line">                    line_state.extend(makeLabel(w))</span><br><span class="line"></span><br><span class="line">                assert <span class="keyword">len</span>(word_list) == <span class="keyword">len</span>(line_state)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> k, v <span class="keyword">in</span> enumerate(line_state):</span><br><span class="line">                    Count_dic[v] += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">                        self.Pi_dic[v] += <span class="number">1</span>  <span class="comment"># 每个句子的第一个字的状态，用于计算初始状态概率</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self.A_dic[line_state[k - <span class="number">1</span>]][v] += <span class="number">1</span>  <span class="comment"># 计算转移概率</span></span><br><span class="line">                        self.B_dic[line_state[k]][word_list[k]] = \</span><br><span class="line">                            self.B_dic[line_state[k]].get(word_list[k], <span class="number">0</span>) + <span class="number">1.0</span>  <span class="comment"># 计算发射概率</span></span><br><span class="line"></span><br><span class="line">        self.Pi_dic = &#123;k: v * <span class="number">1.0</span> / line_num <span class="keyword">for</span> k, v <span class="keyword">in</span> self.Pi_dic.items()&#125;</span><br><span class="line">        self.A_dic = &#123;k: &#123;k1: v1 / Count_dic[k] <span class="keyword">for</span> k1, v1 <span class="keyword">in</span> v.items()&#125;</span><br><span class="line">                      <span class="keyword">for</span> k, v <span class="keyword">in</span> self.A_dic.items()&#125;</span><br><span class="line">        <span class="comment"># 加1平滑</span></span><br><span class="line">        self.B_dic = &#123;k: &#123;k1: (v1 + <span class="number">1</span>) / Count_dic[k] <span class="keyword">for</span> k1, v1 <span class="keyword">in</span> v.items()&#125;</span><br><span class="line">                      <span class="keyword">for</span> k, v <span class="keyword">in</span> self.B_dic.items()&#125;</span><br><span class="line">        <span class="comment"># 序列化</span></span><br><span class="line">        <span class="keyword">import</span> pickle</span><br><span class="line">        <span class="keyword">with</span> <span class="keyword">open</span>(self.model_file, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(self.A_dic, f)</span><br><span class="line">            pickle.dump(self.B_dic, f)</span><br><span class="line">            pickle.dump(self.Pi_dic, f)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">self</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> viterbi(<span class="keyword">self</span>, <span class="built_in">text</span>, states, start_p, trans_p, emit_p):</span><br><span class="line">        V = [&#123;&#125;]</span><br><span class="line">        <span class="keyword">path</span> = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">            V[<span class="number">0</span>][y] = start_p[y] * emit_p[y].get(<span class="built_in">text</span>[<span class="number">0</span>], <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">path</span>[y] = [y]</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="keyword">range</span>(<span class="number">1</span>, <span class="keyword">len</span>(<span class="built_in">text</span>)):</span><br><span class="line">            V.append(&#123;&#125;)</span><br><span class="line">            newpath = &#123;&#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 检验训练的发射概率矩阵中是否有该字</span></span><br><span class="line">            neverSeen = <span class="built_in">text</span>[t] <span class="keyword">not</span> <span class="keyword">in</span> emit_p[<span class="string">'S'</span>].keys() <span class="keyword">and</span> \</span><br><span class="line">                        <span class="built_in">text</span>[t] <span class="keyword">not</span> <span class="keyword">in</span> emit_p[<span class="string">'M'</span>].keys() <span class="keyword">and</span> \</span><br><span class="line">                        <span class="built_in">text</span>[t] <span class="keyword">not</span> <span class="keyword">in</span> emit_p[<span class="string">'E'</span>].keys() <span class="keyword">and</span> \</span><br><span class="line">                        <span class="built_in">text</span>[t] <span class="keyword">not</span> <span class="keyword">in</span> emit_p[<span class="string">'B'</span>].keys()</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">                emitP = emit_p[y].get(<span class="built_in">text</span>[t], <span class="number">0</span>) <span class="keyword">if</span> <span class="keyword">not</span> neverSeen <span class="keyword">else</span> <span class="number">1.0</span>  <span class="comment"># 设置未知字单独成词</span></span><br><span class="line">                (prob, state) = <span class="keyword">max</span>(</span><br><span class="line">                    [(V[t - <span class="number">1</span>][y0] * trans_p[y0].get(y, <span class="number">0</span>) *</span><br><span class="line">                      emitP, y0)</span><br><span class="line">                     <span class="keyword">for</span> y0 <span class="keyword">in</span> states <span class="keyword">if</span> V[t - <span class="number">1</span>][y0] &gt; <span class="number">0</span>])</span><br><span class="line">                V[t][y] = prob</span><br><span class="line">                newpath[y] = <span class="keyword">path</span>[state] + [y]</span><br><span class="line">            <span class="keyword">path</span> = newpath</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> emit_p[<span class="string">'M'</span>].get(<span class="built_in">text</span>[<span class="number">-1</span>], <span class="number">0</span>) &gt; emit_p[<span class="string">'S'</span>].get(<span class="built_in">text</span>[<span class="number">-1</span>], <span class="number">0</span>):</span><br><span class="line">            (prob, state) = <span class="keyword">max</span>([(V[<span class="keyword">len</span>(<span class="built_in">text</span>) - <span class="number">1</span>][y], y) <span class="keyword">for</span> y <span class="keyword">in</span> (<span class="string">'E'</span>, <span class="string">'M'</span>)])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            (prob, state) = <span class="keyword">max</span>([(V[<span class="keyword">len</span>(<span class="built_in">text</span>) - <span class="number">1</span>][y], y) <span class="keyword">for</span> y <span class="keyword">in</span> states])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (prob, <span class="keyword">path</span>[state])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> cut(<span class="keyword">self</span>, <span class="built_in">text</span>):</span><br><span class="line">        <span class="keyword">import</span> os</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.load_para:</span><br><span class="line">            self.try_load_model(os.path.exists(self.model_file))</span><br><span class="line">        prob, pos_list = self.viterbi(<span class="built_in">text</span>, self.state_list, self.Pi_dic, self.A_dic, self.B_dic)</span><br><span class="line">        <span class="keyword">begin</span>, <span class="keyword">next</span> = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, <span class="built_in">char</span> <span class="keyword">in</span> enumerate(<span class="built_in">text</span>):</span><br><span class="line">            pos = pos_list[i]</span><br><span class="line">            <span class="keyword">if</span> pos == <span class="string">'B'</span>:</span><br><span class="line">                <span class="keyword">begin</span> = i</span><br><span class="line">            elif pos == <span class="string">'E'</span>:</span><br><span class="line">                yield <span class="built_in">text</span>[<span class="keyword">begin</span>: i + <span class="number">1</span>]</span><br><span class="line">                <span class="keyword">next</span> = i + <span class="number">1</span></span><br><span class="line">            elif pos == <span class="string">'S'</span>:</span><br><span class="line">                yield <span class="built_in">char</span></span><br><span class="line">                <span class="keyword">next</span> = i + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">next</span> &lt; <span class="keyword">len</span>(<span class="built_in">text</span>):</span><br><span class="line">            yield <span class="built_in">text</span>[<span class="keyword">next</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    hmm = HMM()</span><br><span class="line">    hmm.train(<span class="string">'./data/trainCorpus.txt_utf8'</span>)</span><br><span class="line">    <span class="comment"># hmm.try_load_model(True)</span></span><br><span class="line">    <span class="built_in">text</span> = <span class="string">'这是一个非常棒的方案！'</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">text</span> !=<span class="string">'\n'</span>):</span><br><span class="line">        res = hmm.cut(<span class="built_in">text</span>)</span><br><span class="line">        print(<span class="built_in">text</span>)</span><br><span class="line">        print(<span class="keyword">str</span>(<span class="keyword">list</span>(res)))</span><br><span class="line">        <span class="built_in">text</span>=<span class="keyword">input</span>()</span><br></pre></td></tr></table></figure><p><img src="/2018/10/28/隐马尔可夫模型及分词上的实现/QQ20181028-214742.png" alt="分词结果"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 统计学习方法 </tag>
            
            <tag> python </tag>
            
            <tag> NLP </tag>
            
            <tag> HMM </tag>
            
            <tag> CWS </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>2019科大讯飞算法岗校招笔试</title>
      <link href="/2018/10/25/2019%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E%E7%AE%97%E6%B3%95%E5%B2%97%E6%A0%A1%E6%8B%9B%E7%AC%94%E8%AF%95/"/>
      <url>/2018/10/25/2019%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E%E7%AE%97%E6%B3%95%E5%B2%97%E6%A0%A1%E6%8B%9B%E7%AC%94%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前天收到了科大讯飞算法岗的笔试邀请，因为以后也想从事算法岗的工作，因此试了试水。讯飞在算法岗的笔试题中，有着20题每题三分的选择题，选择题包括了HMM、CRF、生成模型、判别模型等机器学习相关知识，令我以外的是他也包括了很多操作系统相关的算法，比如银行家算法。而后就是两道很简单的编程题，每题二十分。</p><h2 id="编程题"><a href="#编程题" class="headerlink" title="编程题"></a>编程题</h2><h3 id="修改成绩"><a href="#修改成绩" class="headerlink" title="修改成绩"></a>修改成绩</h3><h4 id="题面"><a href="#题面" class="headerlink" title="题面"></a>题面</h4><p><em>题目描述</em><br>华老师的n个学生参加了一次模拟测验，考出来的分数很糟糕，但是华老师可以将成绩修改为[0,100]中的任意值，所以他想知道，如果要使所有人的成绩的平均分不少于X分，至少要改动多少个人的分数？<br><em>输入</em><br>第一行一个数T，共T组数据（T≤10）<br>接下来对于每组数据：<br>第一行两个整数n和X。（1≤n≤1000, 0≤X≤100）<br>第二行n个整数，第i个数Ai表示第i个学生的成绩。（0≤Ai≤100）<br><em>输出</em><br>共T行，每行一个整数，代表最少的人数。<br><em>样例输入</em><br>2<br>5 60<br>59 20 30 90 100<br>5 60<br>59 20 10 10 100<br><em>样例输出</em><br>1<br>2<br><em>Hint</em><br>对于第一组数据，将59改成60即可</p><h4 id="AC代码"><a href="#AC代码" class="headerlink" title="AC代码"></a>AC代码</h4><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    n = <span class="keyword">int</span>(sys.stdin.readline().strip())</span><br><span class="line">    <span class="keyword">for</span> i in <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="built_in">line</span> = sys.stdin.readline().strip()</span><br><span class="line">        two=<span class="keyword">list</span>(<span class="keyword">map</span>(<span class="keyword">int</span>, <span class="built_in">line</span>.<span class="keyword">split</span>()))</span><br><span class="line">        <span class="built_in">line</span>=sys.stdin.readline().strip()</span><br><span class="line">        n_num=<span class="keyword">list</span>(<span class="keyword">map</span>(<span class="keyword">int</span>, <span class="built_in">line</span>.<span class="keyword">split</span>()))</span><br><span class="line">        mubiao=two[<span class="number">0</span>]*two[<span class="number">1</span>]</span><br><span class="line">        n_num=sorted(n_num)</span><br><span class="line">        flag=<span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> sum(n_num)&lt;mubiao <span class="built_in">and</span> flag&lt;=<span class="built_in">len</span>(n_num):</span><br><span class="line">            n_num[flag]=<span class="number">100</span></span><br><span class="line">            flag+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">print</span>(flag)</span><br></pre></td></tr></table></figure><h3 id="杀手"><a href="#杀手" class="headerlink" title="杀手"></a>杀手</h3><h4 id="题面-1"><a href="#题面-1" class="headerlink" title="题面"></a>题面</h4><p><em>题目描述</em><br>有n个杀手排成一行，每个杀手都有一个不同的编号(编号为1-n)，在每个夜晚，杀手都会行动，如果某个杀手编号大于他右边的杀手的编号，他就会杀死他右边的杀手，杀手是的行动是瞬间的，因此一个人可能某一个夜晚既杀死了别人又被别人杀死，例如3,2,1这个顺序，在第一个夜晚2会杀死1，同时3也会杀死2。<br>显而易见，一段时间之后，就不会有人杀或被杀，平安夜也就到来了，请问在平安夜之前有多少个夜晚。<br><em>输入</em><br>输入第一行是一个整数n（1≤n≤100000）,表示杀手的数量。<br>接下来一行有n个数，是一个1-n的全排列。<br><em>输出</em><br>输出包含一个整数，表示平安夜之前经历了多少个夜晚。<br><em>样例输入</em><br>10 10 9 7 8 6 5 3 4 2 1<br><em>样例输出</em><br>2<br><em>Hint</em><br>补充样例<br>输入样例2： 6 1 2 3 4 5 6<br>输出样例2 ：0<br>样例解释： 样例1中杀手的变化为[10 9 7 8 6 5 3 4 2 1]-&gt;[10 8 4]-&gt;[10]，故答案为2。</p><h4 id="AC代码-1"><a href="#AC代码-1" class="headerlink" title="AC代码"></a>AC代码</h4><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    n = int(sys.<span class="keyword">stdin</span>.readline().strip())</span><br><span class="line">    <span class="built_in">line</span> = sys.<span class="keyword">stdin</span>.readline().strip()</span><br><span class="line">    <span class="built_in">num</span>=list(map(int, <span class="built_in">line</span>.<span class="built_in">split</span>()))</span><br><span class="line">    flag=True</span><br><span class="line">    cishu=<span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span>(flag):</span><br><span class="line">        n=<span class="built_in">len</span>(<span class="built_in">num</span>)</span><br><span class="line">        next=[]</span><br><span class="line">        flag=False</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="built_in">len</span>(<span class="built_in">num</span>)+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 发生吃人</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">num</span>[n-i]&gt;<span class="built_in">num</span>[n-i+<span class="number">1</span>]):</span><br><span class="line">                flag=True</span><br><span class="line">            <span class="comment"># 吃不掉</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">num</span>[n-i]&lt;<span class="built_in">num</span>[n-i+<span class="number">1</span>]):</span><br><span class="line">                next.append(<span class="built_in">num</span>[n-i+<span class="number">1</span>])</span><br><span class="line">                <span class="comment"># flag=True</span></span><br><span class="line">            <span class="comment"># 加入第一个人</span></span><br><span class="line">            <span class="keyword">if</span>(i==<span class="built_in">len</span>(<span class="built_in">num</span>)):</span><br><span class="line">                next.append(<span class="built_in">num</span>[n-i])</span><br><span class="line">        <span class="built_in">num</span>=list(reversed(next))</span><br><span class="line">        <span class="keyword">if</span>(flag==True):</span><br><span class="line">            cishu+=<span class="number">1</span></span><br><span class="line">    print(cishu)</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>不得不说讯飞的速度是挺快的，第二天就发来了面试邀请。但是想问hr招不招实习的时候，hr不回复，因此也就没下文了。面试当然也就没去了，~难受~<br><img src="/2018/10/25/2019科大讯飞算法岗校招笔试/QQ20181026-152734.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 刷题记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 笔试 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>决策树的原理及实现</title>
      <link href="/2018/10/19/%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"/>
      <url>/2018/10/19/%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>不得不说，相对于前面几章，决策树的新鲜知识点明显要多了，引入了很多<em>信息论</em>中的内容，<del>看的很难受呀</del>，不过在信念的坚持下，还是开始总结了本章的知识点。</p><p>决策树是一种基本的分类与回归方法。本篇文章主要讨论分类的决策树，决策树呈现树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以被认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。主要优点是<em>模型具有可读型、分类速度快</em>，学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据利用决策树模型进行分类，决策树学习通常包括3个步骤：<em>特征选择、决策树的生成和决策树的修剪</em>。</p><h2 id="决策树模型与学习"><a href="#决策树模型与学习" class="headerlink" title="决策树模型与学习"></a>决策树模型与学习</h2><h3 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h3><blockquote><p>定义5.1（决策树） 分类决策树模型是一种描述对实例进行分类的树形结构，决策树由结点（node）和有向边（directed edge）组成。结点有两种类型，内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。<br>用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值，如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分到叶结点的类中。</p></blockquote><h3 id="决策树与if-then规则"><a href="#决策树与if-then规则" class="headerlink" title="决策树与if-then规则"></a>决策树与if-then规则</h3><p>可以将决策树看成一个if-then规则的集合。将决策树转换成if-then规则的过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树有着<em>互斥并且完备</em>这一特点。就是说，每一个实例都被一条路径或一条规则所覆盖，并且只被一条路径或一条规则所覆盖。</p><h3 id="决策树与条件概率分布"><a href="#决策树与条件概率分布" class="headerlink" title="决策树与条件概率分布"></a>决策树与条件概率分布</h3><p>决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义。这一条件概率分布定义在特征空间上的一个划分上。将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设X为表示特征的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为P(Y|X)。X取值于给定划分下单元的集合，Y取值于类的集合。各叶结点（单元）上的条件概率往往偏向某一个类，即属于某一类的概率较大。决策树分类时将该结点强行分到条件概率大的那一类去。</p><h3 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h3><p>决策树学习，假设给定训练数据集$$D=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$$其中，$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T$为输入实例（特征向量），$n$为特征个数，$y_i\in \{1,2,…,K\}$为类标记，$i=1,2,…,N$，$N$为样本容量，学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。<br>决策树学习本质上是从数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛华能力。<br>当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用<em>启发式</em>方法，近似求解这一最优化问题。这样得到的决策树是次最优的。<br>决策树学习的算法通常是一个递归地选择最优特征，并根据特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如果递归地进行下去，直至所有训练数据集被基本正确分类，或者没有合适的特征为止，最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一课决策树。<br>以上方法生成的决策树可能对训练数据集有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生<em>过拟合</em>现象。我们需要对已生成的树自下向上而进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。</p><h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><h3 id="特征选择问题"><a href="#特征选择问题" class="headerlink" title="特征选择问题"></a>特征选择问题</h3><p>特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是<em>信息增益</em>或<em>信息增益比</em>。</p><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为$$P(X=x_i)=p_i  ,i=1,2,…,n$$则随机变量$X$的熵定义为$$H(X)=-\sum_{i=1}^{n}{p_i log p_i}$$在式中，若$p_i=0$，则定义$0log0=0$。通常，该式中的对数以2为底或以e为底（自然对数），这时熵的单元分别称作比特或纳特。由定义可知，<em>熵只依赖于X的分布</em>，而与X的取值无关，所以也可将X的熵记作$H(p)$，即$$H(p)=-\sum_{i=1}^{n}p_i log p_i$$</p><p>熵越大，随机变量的不确定性就越大，当$p=0$或$p=1$时$H(p)=0$，随机变量完全没有不确定性。当$p=0.5$时，$H(p)=1$，熵取值就最大，随机变量不确定性最大。</p><p>设有随机变量$(X,Y)$，其联合概率分布为$$P(X=x_i,Y=y_j)=p_{ij}，i=1,2,…,n;j=1,2,…,m$$条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵$H(Y|X)$，定义为X给定条件下Y的条件概率分布的熵对X的数学期望$$H(Y|X)=\sum_{i=1}^{n}{p_iH(Y|X=x_i)}$$这里，$p_i=P(X=x_i),i=1,2,…,n.$</p><p>当熵和条件熵中的概率由数据估计得到时，所对应的熵和条件熵分别称为<em>经验熵</em>、<em>经验条件熵</em>。此时，如果有0概率，令$0log0=0$。<em>信息增益表示得知特征X的信息而使得类Y的信息的不确定性缺少的程度。</em></p><blockquote><p>定义5.2（信息增益）特征$A$对训练数据集D的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即$$g(D,A)=H(D)-H(D|A)$$一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p></blockquote><p>决策树学习应用信息增益准则选择特征。给定训练数据集D和特征A，经验熵$H(D)$表示对数据集$D$进行分类的不确定性。而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集D的不确定性。那么它们的差，即信息增益，就<strong>表示由于特征A而使得对数据集D的分类的不确定性减少的程度</strong>。显然，对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。</p><p>设训练数据为$D$，$|D|$表示其样本容量，即样本个数。设有$K$个类$C_k$，$k=1,2,…,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^{K}{|C_k|=|D|}$。设特征$A$有$n$个不同的取值${a_1,a_2,…,a_n}$，根据特征A的取值将D划分为n个子集$D_1,D_2,…,D_n$，$|D_i|$为$D_i$的样本数量，$\sum_{i=1}^{n}{|D_i|=|D|}$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik}=D_i\cap C_k$，$D_{ik}$为$D_{ik}$的样本个数。于是信息增益的算法如下：</p><blockquote><p>算法5.1（信息增益的算法）<br>输入：训练数据$D$和特征$A$；<br>输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$<br>(1)计算数据集D的经验熵$H(D)$$$H(D)=-\sum_{k=1}^{K}{\frac{|C_k|}{|D|} log_2{\frac{C_k}{D}}}$$<br>(2)计算特征A对数据集D的经验条件熵$H(D|A)$$$H(D|A)=\sum_{i=1}^{n}{\frac{|D_i|}{|D|}H(D)}=-\sum_{i=1}^{n}{\frac{|D_{ik}|}{|D_i|}log_2\frac{|C_k|}{|D|}}$$<br>(3)计算信息熵$$g(D,A)=H(D)-H(D|A)$$</p></blockquote><h4 id="信息增益算法的实现"><a href="#信息增益算法的实现" class="headerlink" title="信息增益算法的实现"></a>信息增益算法的实现</h4><p>代码：<br><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">import numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="meta"># 数据来源于P59页表5.1</span></span><br><span class="line"><span class="function">def <span class="title">get_data</span>(<span class="params"></span>):</span></span><br><span class="line"><span class="function">    dataSet </span>= [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],  <span class="meta"># 数据集</span></span><br><span class="line">               [<span class="meta">0, 0, 0, 1, 0</span>],</span><br><span class="line">               [<span class="meta">0, 1, 0, 1, 1</span>],</span><br><span class="line">               [<span class="meta">0, 1, 1, 0, 1</span>],</span><br><span class="line">               [<span class="meta">0, 0, 0, 0, 0</span>],</span><br><span class="line">               [<span class="meta">1, 0, 0, 0, 0</span>],</span><br><span class="line">               [<span class="meta">1, 0, 0, 1, 0</span>],</span><br><span class="line">               [<span class="meta">1, 1, 1, 1, 1</span>],</span><br><span class="line">               [<span class="meta">1, 0, 1, 2, 1</span>],</span><br><span class="line">               [<span class="meta">1, 0, 1, 2, 1</span>],</span><br><span class="line">               [<span class="meta">2, 0, 1, 2, 1</span>],</span><br><span class="line">               [<span class="meta">2, 0, 1, 1, 1</span>],</span><br><span class="line">               [<span class="meta">2, 1, 0, 1, 1</span>],</span><br><span class="line">               [<span class="meta">2, 1, 0, 2, 1</span>],</span><br><span class="line">               [<span class="meta">2, 0, 0, 0, 0</span>]]</span><br><span class="line">    X=np.zeros([len(dataSet),<span class="number">4</span>])</span><br><span class="line">    Y=np.zeros(len(dataSet))</span><br><span class="line">    <span class="function"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="title">range</span>(<span class="params">len(dataSet</span>)):</span></span><br><span class="line"><span class="function">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="title">range</span>(<span class="params"><span class="number">4</span></span>):</span></span><br><span class="line"><span class="function">            X[i][t]</span>=dataSet[i][t]</span><br><span class="line">        Y[i]=dataSet[i][<span class="number">4</span>]</span><br><span class="line">    <span class="keyword">return</span> X,Y</span><br><span class="line"></span><br><span class="line"><span class="meta"># 求解经验熵H(x)</span></span><br><span class="line"><span class="function">def <span class="title">empirical_entropy</span>(<span class="params">x</span>):</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    x_value_list </span>= <span class="keyword">set</span>([x[i] <span class="function"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="title">range</span>(<span class="params">x.shape[<span class="number">0</span>]</span>)])</span></span><br><span class="line"><span class="function">    ent </span>= <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> x_value <span class="keyword">in</span> x_value_list:</span><br><span class="line">        p = <span class="keyword">float</span>(x[x == x_value].shape[<span class="number">0</span>]) / x.shape[<span class="number">0</span>]</span><br><span class="line">        logp = np.log2(p)</span><br><span class="line">        ent -= p * logp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ent</span><br><span class="line"></span><br><span class="line"><span class="meta"># 求解经验条件熵H(y|x)</span></span><br><span class="line"><span class="function">def <span class="title">empirical_conditional_entropy</span>(<span class="params">x, y</span>):</span></span><br><span class="line"><span class="function">    x_value_list </span>= <span class="keyword">set</span>([x[i] <span class="function"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="title">range</span>(<span class="params">x.shape[<span class="number">0</span>]</span>)])</span></span><br><span class="line"><span class="function">    # <span class="title">print</span>(<span class="params">x_value_list</span>)</span></span><br><span class="line"><span class="function">    ent </span>= <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> x_value <span class="keyword">in</span> x_value_list:</span><br><span class="line">        sub_y = y[x == x_value]</span><br><span class="line">        temp_ent = empirical_entropy(sub_y)</span><br><span class="line">        ent += (<span class="keyword">float</span>(sub_y.shape[<span class="number">0</span>]) / y.shape[<span class="number">0</span>]) * temp_ent</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ent</span><br><span class="line"></span><br><span class="line"><span class="meta"># 求解信息增益</span></span><br><span class="line"><span class="function">def <span class="title">information_gain</span>(<span class="params">x,y</span>):</span></span><br><span class="line"><span class="function">    base_ent </span>= empirical_entropy(y)</span><br><span class="line">    condition_ent = empirical_conditional_entropy(x, y)</span><br><span class="line">    ent_grap = base_ent - condition_ent</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ent_grap</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    X,Y=get_data()</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"X"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    print(X)</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"Y"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    print(Y)</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"信息增益"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="title">range</span>(<span class="params">X[<span class="number">0</span>].shape[<span class="number">0</span>]</span>):</span></span><br><span class="line"><span class="function">        <span class="title">print</span>(<span class="params"><span class="string">"g(D,A_"</span>+str(i</span>)+"): %.3f " % (<span class="params">information_gain(X[:,i],Y</span>)))</span></span><br></pre></td></tr></table></figure></p><p>结果：<br><img src="/2018/10/19/决策树的原理及实现/QQ20181024-133943.png" alt="信息增益算法结果"></p><h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h3><p>信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比可以怼这一问题进行校正。这是特征选择的另一准则。</p><blockquote><p>定义5.3（信息增益比）特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D的经验熵$H(D)$之比：$$g_R(D,A)=\frac{g(D,A)}{H(D)}$$其中，$H_A(D)=-\sum_{i=1}^{n}log_2\frac{D_i}{D}$，n是特征A取值的个数。</p></blockquote><h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h3><p>ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归地构建决策树。具体方法是：<strong>从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一棵决策树。</strong>ID3相当于用<a href="https://baike.baidu.com/item/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/3350286?fr=aladdin" target="_blank" rel="noopener">极大似然法</a>进行概率模型的选择。</p><blockquote><p>算法5.2（ID3算法）<br>输入：训练数据集D，特征集$A$，阈值$\varepsilon$<br>输出：决策树$T$<br>(1)若D中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$。<br>(2)若$A\ne\emptyset$，则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；<br>(3)否则，按<a href="#信息增益"><strong>算法5.1</strong></a>计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$；<br>(4)<strong>如果$A_g$的信息增益小于阈值$\varepsilon$</strong>，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；<br>(5)否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；<br>(6)对$i$个子结点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归地调用步(1)~(5)，得到子树$T_i$，返回$T_i$。</p></blockquote><h3 id="C4-5的生成算法"><a href="#C4-5的生成算法" class="headerlink" title="C4.5的生成算法"></a>C4.5的生成算法</h3><p>C4.5算法与ID3算法相似，是对其的一种改进，C4.5在生成的过程中，<strong>用信息增益比来选择特征</strong>。</p><blockquote><p>算法5.3（C4.5的生成算法）<br>输入：训练数据集$D$，特征集$A$，阈值$\varepsilon$；<br>输出：决策树$T$；<br>(1)如果$D$中所有实例属于同一类$C_k$，则置$T$为单结点树，并将$C_k$作为该结点的类，返回$T$；<br>(2)若$A\ne\emptyset$，则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；<br>(3)否则，按<a href="#信息增益比"><strong>算法5.10</strong></a>计算$A$中各特征对$D$的信息增益比，选择信息增益比最大的特征$A_g$；<br>(4)<strong>如果$A_g$的信息增益小于阈值$\varepsilon$</strong>，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；<br>(5)否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；<br>(6)对$i$个子结点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归地调用步(1)~(5)，得到子树$T_i$，返回$T_i$。</p></blockquote><h2 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h2><p>决策树通过递归来产生决策树，这样产生的树往往对训练数据分了很精确，但是在未知数据中不准确，即出现过拟合的现象。在决策树学习中将已生成的树进行简化的过程称为<strong>剪枝</strong>，具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为叶结点，从而简化决策树模型。</p><h3 id="剪枝所用的损失函数"><a href="#剪枝所用的损失函数" class="headerlink" title="剪枝所用的损失函数"></a>剪枝所用的损失函数</h3><p>决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。设树$T$的叶结点个数为$|T|$，t是树$T$的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$k=1,2,…,K$，$H_t(T)$为叶结点$t$上的经验熵，$\alpha\ge0$为参数，则决策树学习的损失函数可以定义为$$C_{\alpha}(T)=\sum_{t=1}^{|T|}{N_tH_t(T)+\alpha{|T|}}\tag{5.11}$$其中经验熵为$$H_t(T)=-\sum_{k}\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}\tag{5.12}$$在损失函数中，将式(5.11)右端的第1项记作$$C(T)=\sum_{t=1}^{|T|}=N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}log\frac{N_{tk}}{N_t}\tag{5.13}$$这时有$$C_{\alpha}(T)=C(T)+\alpha{|T|}\tag{5.14}$$式(5.14)中，$C(T)$表示<strong>模型对训练数据的预测误差</strong>，即模型与训练数据的拟合度。$|T|$表示模型复杂度，参数$\alpha\ge0$控制两者之间的影响。较大的$\alpha$促使选择简单的模型，较大的$\alpha$促使选择较复杂的模型（树）。$\alpha=0$意味着只考虑模型与训练数据的拟合程度，<strong>不考虑模型的复杂度</strong>。</p><p>式(5.11)或式(5.14)定义的损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。</p><p>图5.6就是决策树剪枝过程的示意图。<br><img src="/2018/10/19/决策树的原理及实现/WX20181027-180850.png" alt=""></p><blockquote><p>算法5.4（树的剪枝算法）<br>输入：生成算法产生的整个树$T$，参数$\alpha$；<br>输出：修剪后的子树$T_\alpha$。<br>(1)计算每个结点的经验熵。<br>(2)递归地从树的叶结点向上回缩。<br>设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数值分别是$C_\alpha(T_B)$与$C_\alpha(T_A)$，如果$$C_\alpha(T_A)\le C_\alpha(T_B)\tag{5.15}$$则进行剪枝，即将父结点变成新的叶结点。<br>(3)返回(2)，直至不能继续为止，得到损失函数最小的子树$T_\alpha$。</p></blockquote><p>注意，式(5.15)只需考虑两个树的损失函数的差，其计算可以在局部进行。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> python </tag>
            
            <tag> 决策树 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>朴素贝叶斯法原理及实现</title>
      <link href="/2018/10/17/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"/>
      <url>/2018/10/17/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对于给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。</p><h2 id="朴素贝叶斯法的学习与分类"><a href="#朴素贝叶斯法的学习与分类" class="headerlink" title="朴素贝叶斯法的学习与分类"></a>朴素贝叶斯法的学习与分类</h2><h3 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h3><p>设输入空间$\mathcal{X}\subseteq R_n$为n维向量的集合，输出空间为类标记集合$\mathcal{Y}=\{c_1,c_2,…,c_k\}$.输入为特征向量$x\in \mathcal{X}$，输出为类标记$y\in \mathcal{Y}$.X是定义在输入空间$\mathcal{X}$上的随机向量，Y是定义在输出空间$\mathcal{Y}$上的随机变量。$P(X,Y)$是X和Y的联合概率分布。训练数据集$$T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$$由<em>$P(X,Y)$独立同分布</em>产生。</p><p>朴素贝叶斯法通过训练数据集学习联合概率分布$P(X,Y)$。具体地，学习以下<em>先验概率分布</em>及<em>条件概率分布</em>。先验概率分布$$P(Y=c_k)，k=1,2,…,K$$条件概率分布（后验概率分布）$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)}|Y=c_k)$$于是学习到联合概率分布$P(X,Y)$.</p><p>条件概率分布$P(X=x|Y=c_k)$有指数级数量的参数，其估计实际是不可行的。事实上，假设$x^{(j)}$可取值有$S_j$个，$j=1,2,…,n$，Y可取值有K个，那么参数个数为$$K\prod_{j=1}^{n}S_j$$</p><p>朴素贝叶斯法对条件概率分布作了<em>条件独立性</em>的假设。由于这是一个较强的假设，朴素贝叶斯法也由此得名。朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型。条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。</p><p>朴素贝叶斯法分类的基本公式：$$P(Y=c_k|X=x)=\frac{P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_k P(Y=c_k) \prod_j (X^{(j)}=x^{(j)}|Y=c_k)}$$</p><p>朴素贝叶斯法分类器可表示为$$y=f(x)=arg \max_{c_k} P(Y=c_k) \prod_j P(X^{(j)}=x^{(j)}|Y=c_k)$$</p><h2 id="朴素贝叶斯法的参数估计"><a href="#朴素贝叶斯法的参数估计" class="headerlink" title="朴素贝叶斯法的参数估计"></a>朴素贝叶斯法的参数估计</h2><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>在朴素贝叶斯法中，学习意味着估计$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$.可以应用极大似然估计法估计相应的概率。<em>先验概率</em>$P(Y=c_k)$的极大似然估计是$$P(Y=c_k)=\frac {\sum_{i=1}^{N} I(y_i=c_k)} {N}，k=1,2,…,K$$设第$j$个特征$x^{(j)}$可能取值的集合为$\{a_{j1},a_{j2},…,a_{jS_j}\}$，条件概率$P(X^{j}=a_{jl}||Y=c_k)$的极大似然估计是$$P(x^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N} I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)}\\j=1,2,…,n;l=1,2,…,S_j;k=1,2,…,K$$式中，$x_i^{(j)}$是第$i$个样本的第$j$个特征；$a_{jl}$是第$j$个特征可能取的第$l$个值：$I$为指示函数。</p><h3 id="学习与分类算法"><a href="#学习与分类算法" class="headerlink" title="学习与分类算法"></a>学习与分类算法</h3><blockquote><p>(朴素贝叶斯算法)<br>输入：训练数据$T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$，其中$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})$，$x_i^{(j)}$是第$i$个样本的第$j$个特征，$x_i^{(j)}\in \{a_{j1},a_{j2},…,a_{jS_j}\}$，$a_{jl}$是第$j$个特征可能取的第$l$个值，$j=1,2,…,n$，$l=1,2,…,S_j$，$y_i\in \{c1,c2,…,c_k\}$；实例$x$；<br>输出：实例$x$的分类</p><ol><li>计算先验概率及条件概率$$P(Y=c_k)=\frac {\sum_{i=1}^{N} I(y_i=c_k)}{N},k=1,2,…,K\\P(X^{(j)}=a_{jl}|Y=c_k)=\frac {\sum_{i=1}^{(j)}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)}\\j=1,2,…,n；l=1,2,…,S_j；k=1,2,…,K$$</li><li>对于给定的实例$x=(x^{(1)},x^{(2)},…,x^{(n)})^T$，计算$$P(Y=c_k)\prod_{j=1}^{n}P(X^{(j)}=X^{(j)}|Y=c_k),k=1,2,…,K$$</li><li>确定实例$x$的类$$y=arg \max_{c_k} P(Y=c_k)\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$</li></ol></blockquote><h2 id="朴素贝叶斯算法在digits数据集上的实现"><a href="#朴素贝叶斯算法在digits数据集上的实现" class="headerlink" title="朴素贝叶斯算法在digits数据集上的实现"></a>朴素贝叶斯算法在digits数据集上的实现</h2><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_digits</span><span class="params">()</span></span><span class="symbol">:</span></span><br><span class="line">    digits = load_digits()</span><br><span class="line">    data = np.zeros([len(digits.data), len(digits.data[<span class="number">0</span>])])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(digits.data))<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(len(digits.data[<span class="number">0</span>]))<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">if</span> digits.data[i][t] &gt; <span class="number">0</span><span class="symbol">:</span></span><br><span class="line">                data[i][t] = <span class="number">1</span></span><br><span class="line">            <span class="symbol">else:</span></span><br><span class="line">                data[i][t] = <span class="number">0</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(data,np.array(digits.target) , test_size=<span class="number">0</span>.<span class="number">25</span>,random_state=<span class="number">33</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Naive_Bayes</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(<span class="keyword">self</span>,X,Y)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.P_Yc=np.zeros(<span class="number">10</span>)<span class="comment">#保存P(Y=c_k)</span></span><br><span class="line">        <span class="keyword">self</span>.I_Yc=np.zeros(<span class="number">10</span>)<span class="comment">#保存I(Y_i=c_k)</span></span><br><span class="line">        <span class="keyword">self</span>.I_XaYc=np.zeros((len(X[<span class="number">0</span>]),len([<span class="number">0</span>,<span class="number">1</span>]),<span class="number">10</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Y))<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>.I_Yc[Y[i]]+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(len(X[<span class="number">0</span>]))<span class="symbol">:</span></span><br><span class="line">                <span class="keyword">self</span>.I_XaYc[t][ int(X[i][t]) ][Y[i]]+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">self</span>.P_Yc=[i/sum(<span class="keyword">self</span>.I_Yc) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">self</span>.I_Yc]</span><br><span class="line">        <span class="comment"># 第一项特征的数量，特征值的种类数量，分类的种类</span></span><br><span class="line">        <span class="keyword">self</span>.P_XaYc=np.zeros((len(X[<span class="number">0</span>]),len([<span class="number">0</span>,<span class="number">1</span>]),<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X[<span class="number">0</span>]))<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(len([<span class="number">0</span>,<span class="number">1</span>]))<span class="symbol">:</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>)<span class="symbol">:</span></span><br><span class="line">                    <span class="keyword">self</span>.P_XaYc[i][t][j]=<span class="keyword">self</span>.I_XaYc[i][t][j]/<span class="keyword">self</span>.I_Yc[j]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(<span class="keyword">self</span>,x)</span></span><span class="symbol">:</span></span><br><span class="line">        score=np.zeros(<span class="number">10</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)<span class="symbol">:</span></span><br><span class="line">            a=<span class="keyword">self</span>.P_Yc[i]</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(len(x))<span class="symbol">:</span></span><br><span class="line">                a*=<span class="keyword">self</span>.P_XaYc[t][int(x[t])][i]</span><br><span class="line">            score[i]=a</span><br><span class="line">        <span class="keyword">return</span> score.argmax()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name_<span class="number">_</span> == <span class="string">'__main__'</span><span class="symbol">:</span></span><br><span class="line">    <span class="comment"># print(get_digits())</span></span><br><span class="line">    X_train, X_test, Y_train, y_test= get_digits()</span><br><span class="line">    nb=Naive_Bayes()</span><br><span class="line">    nb.train(X_train,Y_train)</span><br><span class="line">    t_num=<span class="number">0</span></span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"预测过程"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y_test))<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> nb.predict(X_test[i])==y_test[i]<span class="symbol">:</span></span><br><span class="line">            t_num+=<span class="number">1</span></span><br><span class="line">        print(<span class="string">"case"</span>,i,<span class="string">"，预测值："</span>,nb.predict(X_test[i]),<span class="string">"，真实值："</span>,y_test[i],<span class="string">"预测结果:"</span>,nb.predict(X_test[i])==y_test[i])</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"预测结果"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"准确率："</span>,t_num/len(y_test))</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/2018/10/17/朴素贝叶斯法原理及实现/QQ20181018-080645.png" alt="朴素贝叶斯法预测结果"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> python </tag>
            
            <tag> 朴素贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>K近邻算法的原理及实现</title>
      <link href="/2018/10/16/K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"/>
      <url>/2018/10/16/K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在学习并实现完了之前的感知机算法，感觉它是比较简单的，并且只能进行处理线性分类问题。<br><br>本章节学习了K近邻算法（k-nearest neighbor,k-NN），它是一种基本分类与回归方法，在这一章节中，不介绍它在回归问题中的应用，仅仅介绍其在分类问题中的应用。K近邻算法的输入为实例的特征向量，对应于特征空间的点，输出为实例的类别。</p><h2 id="k近邻算法"><a href="#k近邻算法" class="headerlink" title="k近邻算法"></a>k近邻算法</h2><p>通俗的说：给定一个数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类（多数表决投票）。</p><blockquote><p>输入：训练数据集$T=\{(x_1,y_1),(x_1,y_2),…,(x_N,y_N)\}$其中，$x_i\in \mathcal{X}\subseteq R^n$为实例的特征向量，$y_i\in \mathcal{Y}={c_1,c_2,…,c_k}$为实例的类别，$i=1,2,…,N$；实例特征向量$x$；<br>输出：在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$:<br><br>$$y=\mathop{\arg\max}_{c_j} \sum_{x_i\in N_k (x)} I(y_i=c_j),i=1,2,…,N;j=1,2,…,K$$<br>I为指示函数，即当$y_i=c_i$时I为1，否则I为0.</p></blockquote><p>当k近邻法的k=1时，就是特殊情形，也就是最近邻算法，对于输入的实例点（特征向量）$x$，最近邻法将训练数据集中与$x$最邻近点的类作为$x$的类。<em>k近邻算法没有显式的学习过程。</em></p><h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>如《统计学习方法》概论总结中所说，距离的度量方法有很多，特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间一般是$n$维实数向量空间$R^n$。使用的距离是欧氏距离，但也可以是其他距离，如更一般的$L_p$距离或Minkowski距离。这里就不再赘述了。</p><h3 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h3><p>k值的选择会对k近邻法的结果产生重大影响。</p><p>如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有于输入实例相近的训练实例才会对预测结果起作用。但是缺点是“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k值的减小就意味着整体模型变得复杂，容易发生过拟合。</p><p>如果选择较大的k值，就相当于用较大领域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的训练实例也会对预测起作用，使预测发生错误。k值的增大就意味着整体的模型变得简单。</p><p>在应用中，k值一般选择一个比较小的数值。通常采用交叉验证法来选取最优的k值。</p><h3 id="分类决策规则"><a href="#分类决策规则" class="headerlink" title="分类决策规则"></a>分类决策规则</h3><p>k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。<br>多数表决规则有如下解释：如果分类的损失函数为0-1损失函数，分类函数为$$f:R^n \to \{c_1,c_2,…,c_k\}$$<br>那么误分类的概率是<br>$$P(Y\ne f(X))=1-P(Y=f(X))$$<br>对给定的实例$x\in X$，其最近邻的k个训练实例点构成集合$N_k(x)$。如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是<br>$$\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \ne c_j)=1- \frac{1}{k} \sum_{x_i\in N_(x)} I(y_i=c_j)$$<br>要使误分类绿最小即经验风险最小，就要使$\sum_{x_i\in N_k(x)} I(y_i=c_j)$最大，所以多数表决规则等于经验风险最小化。</p><h2 id="KNN的实现"><a href="#KNN的实现" class="headerlink" title="KNN的实现"></a>KNN的实现</h2><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 仅使用sklearn中的iris数据集，以及归一化、数据拆分函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getIrisData</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    data=load_iris()</span><br><span class="line">    X=np.array(data.data)</span><br><span class="line">    Y=np.array(data.target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># X数据归一化处理</span></span><br><span class="line">    min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class="line">    X = min_max_scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据拆分，测试数据占比0.3</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = <span class="number">0.2</span>, random_state = <span class="number">33</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">cell</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        self.x=x</span><br><span class="line">        self.y=y</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># k值设定，可以根据需要更改</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,k=<span class="number">11</span>)</span>:</span></span><br><span class="line">        self.k=k</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 向当前knn模型中注入原始数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,X,Y)</span>:</span></span><br><span class="line">        self.size=len(X[<span class="number">0</span>])</span><br><span class="line">        self.cells=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Y)):</span><br><span class="line">            self.cells.append(cell(X[i],Y[i]))</span><br><span class="line">        <span class="comment"># print(self.cells)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用模型进行预测</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="comment"># 创建投票列表</span></span><br><span class="line">        Nk=sorted(self.cells,key=<span class="keyword">lambda</span> cell:np.linalg.norm(cell.x-x))[:self.k] <span class="comment">#使用的np.linalg.norm函数在默认情况下求解的为范数为2的范式</span></span><br><span class="line">        <span class="comment"># 多数表决投票</span></span><br><span class="line">        ans=np.zeros(<span class="number">4</span>,float)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> Nk:</span><br><span class="line">            ans[i.y]+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print("*" * 10, "置信度", "*" * 10)</span></span><br><span class="line">        ans=ans/self.k</span><br><span class="line">        <span class="comment"># print(ans)</span></span><br><span class="line">        <span class="keyword">return</span> ans.argmax(),ans</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    X_train, X_test, y_train, y_test=getIrisData()</span><br><span class="line">    knn=KNN()</span><br><span class="line">    knn.train(X_train,y_train)</span><br><span class="line">    print(<span class="string">"*"</span> * <span class="number">10</span>, <span class="string">"预测"</span>, <span class="string">"*"</span> * <span class="number">10</span>)</span><br><span class="line">    t_num=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y_test)):</span><br><span class="line">        p_y,ans=knn.predict(X_test[i])</span><br><span class="line">        <span class="keyword">if</span> p_y==y_test[i]:</span><br><span class="line">            t_num+=<span class="number">1</span></span><br><span class="line">        print(<span class="string">"各参数:"</span>,X_test[i],<span class="string">"真实类:"</span>,y_test[i],<span class="string">"预测类:"</span>,p_y,<span class="string">"是否正确:"</span>,p_y==y_test[i],<span class="string">"置信度:"</span>,ans)</span><br><span class="line">    print(<span class="string">"*"</span> * <span class="number">10</span>, <span class="string">"结果"</span>, <span class="string">"*"</span> * <span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"iris数据集中，交叉验证正确率：&#123;:2f&#125;"</span>.format(t_num/len(y_test)))</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/2018/10/16/K近邻算法的原理及实现/QQ20181017-133128.png" alt="KNN在iris数据集中的效果"></p><h2 id="kd树寻找最邻近算法"><a href="#kd树寻找最邻近算法" class="headerlink" title="kd树寻找最邻近算法"></a>kd树寻找最邻近算法</h2><h3 id="构造算法"><a href="#构造算法" class="headerlink" title="构造算法"></a>构造算法</h3><blockquote><p>构造平衡kd树算法：<br>输入：k维空间数据集$T=\{x_1,x_2,…,x_N\}$，<br>其中$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(k)})^T$,$i=1,2,…,N;$<br>输出：kd树<br>（1）开始：构造根节点，根节点对应于包括T的k维空间的超矩形区域。<br>选择$x^{(1)}$为坐标轴，以T中所有实例$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。<br>由根结点生成深度为1的左、右子结点：左子结点对应坐标$x^{(1)}$小与切分点的子区域，右子结点对于坐标$x^{(1)}$大于切分点的子区域。<br>将落在切分超平面上的实例点保存在根结点。<br>（2）重复：对深度为j的结点，选择$x^{(l)}$为切分的坐标轴，$l=j(mod k)+1$，以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(l)}$垂直的超平面实现。<br>由该结点生成深度为$j+1$的左、右子结点：左子结点对应坐标$x^{(l)}$小与切分点的子区域，右子结点对应坐标$x^{(l)}$大于切分点的子区域。<br>将落在切分超平面上的实例点保存在该结点。<br>（3）直到两个子区域没有实例存在时停止，从而形成kd树的区域划分。</p></blockquote><h3 id="搜索算法"><a href="#搜索算法" class="headerlink" title="搜索算法"></a>搜索算法</h3><blockquote><p>用kd树的<em>最邻近</em>搜索：<br><br>输入：已够早的kd树：目标点x<br>输出：x的最近邻<br>(1)在kd树种找出包含目标点x的叶结点：从根结点出发，递归地向下访问kd树，若目标点x当前维的坐标小与切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。<br>(2)以此叶结点为“当前最近点”<br>(3)递归地向上回退，在每个结点进行以下操作：</p><blockquote><p>(a)如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”<br>(b)当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与目标点为球心，以目标点与“当前最近点”间的距离为半径的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点。移动到另一个子结点，接着，递归地进行最邻近搜索。如果不相交，向上回退。</p></blockquote></blockquote><blockquote><p>(4)当回退到根结点时，搜索结束，最后的“当前最近点”即为x的最邻近点。</p></blockquote><h2 id="kd树查找最近邻的实现"><a href="#kd树查找最近邻的实现" class="headerlink" title="kd树查找最近邻的实现"></a>kd树查找最近邻的实现</h2><h3 id="code-1"><a href="#code-1" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造kd树，P41</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KDnode</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dome_point,split_num,left,right)</span>:</span></span><br><span class="line">        self.dome_point=dome_point  <span class="comment">#此次样本点</span></span><br><span class="line">        self.split_num=split_num    <span class="comment">#切割的维度号</span></span><br><span class="line">        self.left=left              <span class="comment">#左子树</span></span><br><span class="line">        self.right=right            <span class="comment">#右子树</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KDtree</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,data)</span>:</span></span><br><span class="line">        k=len(data[<span class="number">0</span>])              <span class="comment">#k维度</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">CreateNode</span><span class="params">(split_num,data_set)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> data_set :       <span class="comment">#data_set为空返回空</span></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">            data_set.sort(key=<span class="keyword">lambda</span> x : x[split_num])  <span class="comment">#按照我们想要切割的维度号进行排序</span></span><br><span class="line">            split_pos=len(data_set)//<span class="number">2</span>                  <span class="comment">#从中间将其分割开，获得中位点标号</span></span><br><span class="line">            middle_pos=data_set[split_pos]              <span class="comment">#找到中位点</span></span><br><span class="line">            split_next=(split_num+<span class="number">1</span>)%k                  <span class="comment">#下一个需要分割的维度</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> KDnode(middle_pos,split_num,</span><br><span class="line">                          CreateNode(split_next,data_set[:split_pos]),</span><br><span class="line">                          CreateNode(split_next,data_set[split_pos+<span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line">        self.root=CreateNode(<span class="number">0</span>,data)    <span class="comment">#创造树</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 寻找最近点，参考学习了https://www.cnblogs.com/21207-iHome/p/6084670.html</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(tree, point)</span>:</span></span><br><span class="line">    k = len(point)  <span class="comment"># 数据维度</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">travel</span><span class="params">(kd_node, target, max_dist)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> kd_node <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="number">0</span>] * k, float(<span class="string">"inf"</span>), <span class="number">0</span>  <span class="comment"># python中用float("inf")和float("-inf")表示正负无穷</span></span><br><span class="line">                                            <span class="comment"># return的参数分别是最近坐标点、最近距离和访问过的节点数</span></span><br><span class="line"></span><br><span class="line">        nodes_visited = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        s = kd_node.split_num  <span class="comment"># 进行分割的维度</span></span><br><span class="line">        pivot = kd_node.dome_point  <span class="comment"># 进行分割的“轴”</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> target[s] &lt;= pivot[s]:  <span class="comment"># 如果目标点第s维小于分割轴的对应值(目标离左子树更近)</span></span><br><span class="line">            nearer_node = kd_node.left  <span class="comment"># 下一个访问节点为左子树根节点</span></span><br><span class="line">            further_node = kd_node.right  <span class="comment"># 同时记录下右子树</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 目标离右子树更近</span></span><br><span class="line">            nearer_node = kd_node.right  <span class="comment"># 下一个访问节点为右子树根节点</span></span><br><span class="line">            further_node = kd_node.left</span><br><span class="line"></span><br><span class="line">        temp1 = travel(nearer_node, target, max_dist)  <span class="comment"># 进行遍历找到包含目标点的区域</span></span><br><span class="line"></span><br><span class="line">        nearest = temp1[<span class="number">0</span>]  <span class="comment"># 以此叶结点作为“当前最近点”</span></span><br><span class="line">        dist = temp1[<span class="number">1</span>]  <span class="comment"># 更新最近距离</span></span><br><span class="line"></span><br><span class="line">        nodes_visited += temp1[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dist &lt; max_dist:</span><br><span class="line">            max_dist = dist  <span class="comment"># 最近点将在以目标点为球心，max_dist为半径的超球体内</span></span><br><span class="line"></span><br><span class="line">        temp_dist = abs(pivot[s] - target[s])  <span class="comment"># 第s维上目标点与分割超平面的距离</span></span><br><span class="line">        <span class="keyword">if</span> max_dist &lt; temp_dist:  <span class="comment"># 判断超球体是否与超平面相交</span></span><br><span class="line">            <span class="keyword">return</span> nearest, dist, nodes_visited  <span class="comment"># 不相交则可以直接返回，不用继续判断</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># 计算目标点与分割点的欧氏距离</span></span><br><span class="line">        temp_dist=np.linalg.norm((np.array(pivot)-np.array(target)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> temp_dist &lt; dist:  <span class="comment"># 如果“更近”</span></span><br><span class="line">            nearest = pivot  <span class="comment"># 更新最近点</span></span><br><span class="line">            dist = temp_dist  <span class="comment"># 更新最近距离</span></span><br><span class="line">            max_dist = dist  <span class="comment"># 更新超球体半径</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查另一个子结点对应的区域是否有更近的点</span></span><br><span class="line">        temp2 = travel(further_node, target, max_dist)</span><br><span class="line"></span><br><span class="line">        nodes_visited += temp2[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> temp2[<span class="number">1</span>] &lt; dist:  <span class="comment"># 如果另一个子结点内存在更近距离</span></span><br><span class="line">            nearest = temp2[<span class="number">0</span>]  <span class="comment"># 更新最近点</span></span><br><span class="line">            dist = temp2[<span class="number">1</span>]  <span class="comment"># 更新最近距离</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nearest, dist, nodes_visited</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> travel(tree.root, point, float(<span class="string">"inf"</span>))  <span class="comment"># 从根节点开始递归</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#先left后right，前序遍历</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preorder</span><span class="params">(root)</span>:</span></span><br><span class="line">    print(root.dome_point)</span><br><span class="line">    <span class="keyword">if</span> root.left:</span><br><span class="line">        preorder(root.left)</span><br><span class="line">    <span class="keyword">if</span> root.right:</span><br><span class="line">        preorder(root.right)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    data=[[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">5</span>,<span class="number">4</span>],[<span class="number">9</span>,<span class="number">6</span>],[<span class="number">4</span>,<span class="number">7</span>],[<span class="number">8</span>,<span class="number">1</span>],[<span class="number">7</span>,<span class="number">2</span>]]     <span class="comment">#书上P42例子</span></span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"data"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    print(data)</span><br><span class="line">    kd=KDtree(data)</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"先序遍历"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    preorder(kd.root)</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"search"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    print(search(kd,[<span class="number">5</span>,<span class="number">6</span>]))</span><br></pre></td></tr></table></figure><h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><p><img src="/2018/10/16/K近邻算法的原理及实现/QQ20181017-133411.png" alt="kd树寻找最邻近点的结果"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> python </tag>
            
            <tag> K近邻 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hexo中文章的两种创建方法及加密</title>
      <link href="/2018/10/15/Hexo%E4%B8%AD%E6%96%87%E7%AB%A0%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%88%9B%E5%BB%BA%E6%96%B9%E6%B3%95%E5%8F%8A%E5%8A%A0%E5%AF%86/"/>
      <url>/2018/10/15/Hexo%E4%B8%AD%E6%96%87%E7%AB%A0%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%88%9B%E5%BB%BA%E6%96%B9%E6%B3%95%E5%8F%8A%E5%8A%A0%E5%AF%86/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在每次写笔记的时候，一天肯定是写不完的，但是又不希望直接<code>clean g d</code>了，因此想使用草稿的功能，鸣谢<a href="https://blog.csdn.net/wizardforcel/article/details/40684575" target="_blank" rel="noopener">https://blog.csdn.net/wizardforcel/article/details/40684575</a>找到了使用草稿功能的方法。</p><h2 id="写文章的步骤"><a href="#写文章的步骤" class="headerlink" title="写文章的步骤"></a>写文章的步骤</h2><p>回顾一下正常写文章的步骤：<br>首先创建一个名叫<code>name</code>的文章<br><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="keyword">new</span> <span class="string">"name"</span></span><br></pre></td></tr></table></figure></p><p>接着系统会提示文章保存在<code>/sources/_post</code>里面，然后书写文章，直至完毕。<br>然后就是同步预览文章和提交github pages的时间了。<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo clean</span></span><br><span class="line"><span class="attribute">hexo g</span></span><br><span class="line"><span class="attribute">hexo d</span></span><br></pre></td></tr></table></figure></p><p>此刻就完成了写文章的标准步骤。</p><h2 id="写草稿的步骤"><a href="#写草稿的步骤" class="headerlink" title="写草稿的步骤"></a>写草稿的步骤</h2><p>接下来讲写草稿的步骤：<br>首先创建一个草稿<br><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="keyword">new</span> <span class="type">draft</span> <span class="string">"new draft"</span></span><br></pre></td></tr></table></figure></p><p>会在<code>source/_drafts</code>目录下生成一个<code>new-draft.md</code>文件。但是这个文件不被显示在页面上，链接也访问不到。也就是说如果你想把某一篇文章移除显示，又不舍得删除，可以把它移动到<code>_drafts</code>目录之中。<br>如果你希望强行预览草稿，更改配置文件：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">render_drafts:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>或者，如下方式启动server：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo<span class="built_in"> server </span>--drafts</span><br></pre></td></tr></table></figure></p><p>下面这条命令可以把草稿变成文章，或者页面：<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo publish <span class="string">[layout]</span> &lt;filename&gt;</span><br></pre></td></tr></table></figure></p><h2 id="加密文章的方法"><a href="#加密文章的方法" class="headerlink" title="加密文章的方法"></a>加密文章的方法</h2><p>我们所书写的一些文章，总会因为一些原因不想让大家看到，比如：文章未完成、涉及个人隐私等等。因此想要将它加密一下。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>在此，我们使用的是<code>hexo-blog-encrypt</code>，并且得在<code>2.0</code>版本以上,然后执行<code>npm install</code>命令。然后该插件会自动安装。</p><h3 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h3><h4 id="1、首先在-config-yml中启用该插件"><a href="#1、首先在-config-yml中启用该插件" class="headerlink" title="1、首先在_config.yml中启用该插件:"></a>1、首先在<code>_config.yml</code>中启用该插件:</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Security</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="attr">encrypt:</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h4 id="2、在你的文章的头部添加上对应的字段，如password-message"><a href="#2、在你的文章的头部添加上对应的字段，如password-message" class="headerlink" title="2、在你的文章的头部添加上对应的字段，如password,message"></a>2、在你的文章的头部添加上对应的字段，如<code>password</code>,<code>message</code></h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">加密测试</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="bullet">-10</span><span class="bullet">-15</span> <span class="number">18</span><span class="string">:48:35</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">加密测试</span></span><br><span class="line"><span class="attr">password:</span> <span class="string">passwd</span></span><br><span class="line"><span class="attr">message:</span> <span class="string">欢迎来到我的博客，由于一些原因，请输入密码查看。</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure><p>如果要取消加密，在<code>password</code>之前用<code>#</code>注释掉即可。</p><h4 id="3、加密效果"><a href="#3、加密效果" class="headerlink" title="3、加密效果"></a>3、加密效果</h4><p><img src="/2018/10/15/Hexo中文章的两种创建方法及加密/QQ20181015-191701.png" alt="加密效果展示"></p>]]></content>
      
      
      <categories>
          
          <category> 技术技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>更改hexo中的Mathjax引擎</title>
      <link href="/2018/10/14/%E6%9B%B4%E6%94%B9hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E7%9A%84Mathjax%E5%BC%95%E6%93%8E/"/>
      <url>/2018/10/14/%E6%9B%B4%E6%94%B9hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E7%9A%84Mathjax%E5%BC%95%E6%93%8E/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>搭建的博客在使用Math引擎构建数学公式的时候遇到了几个问题。</p><ul><li>显示公式出错，显示的是语法错误</li><li>不能使用<code>$...$</code>行内公式</li><li>显示的公式十分丑</li></ul><p>在网上也看到了很多的答案，但是都不是很靠谱啊。真的是个大坑啊啊啊，看<a href="https://docs.mathjax.org/en/latest/mathjax.html" target="_blank" rel="noopener">官方文档</a>看了好久，然后又去找的Hexo的实现方法。emmm，索性解决了。</p><h3 id="找到自己模板配置"><a href="#找到自己模板配置" class="headerlink" title="找到自己模板配置"></a>找到自己模板配置</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd themes<span class="regexp">/maupassant/</span></span><br><span class="line"><span class="comment"># 找到head的配置文件，我的模板中head等的配置文件是在这里，其他的可以自己找找</span></span><br><span class="line">cd layout<span class="regexp">/_partial/</span></span><br></pre></td></tr></table></figure><p>layout/_partial/下就是该模板的配置信息了，然后看一下是否有mathjax.pug（部分模板是html格式或其他格式，无所谓），如果有的话，选择方案一，如果没有，选择方案二。</p><p>优先推荐方案一，因为这样可以只在使用mathjax的页面进行渲染，速度更加快。</p><h2 id="解决方案一"><a href="#解决方案一" class="headerlink" title="解决方案一"></a>解决方案一</h2><h3 id="只需要修改mathjax-pug"><a href="#只需要修改mathjax-pug" class="headerlink" title="只需要修改mathjax.pug"></a>只需要修改mathjax.pug</h3><p>在某些模板中，有着自己的mathjax.pug，那么只需要将js相应部分改为以下的pug代码即可。</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">script(type=<span class="string">"text/x-mathjax-config"</span>).</span><br><span class="line">    <span class="symbol">MathJax</span>.<span class="symbol">Hub</span>.<span class="symbol">Config</span>(&#123;</span><br><span class="line">    extensions: [<span class="string">"tex2jax.js"</span>],</span><br><span class="line">    jax: [<span class="string">"input/TeX"</span>, <span class="string">"output/HTML-CSS"</span>],</span><br><span class="line">    tex2jax: &#123;</span><br><span class="line">        inlineMath: [ [<span class="string">'$'</span>,<span class="string">'$'</span>], [<span class="string">"\\("</span>,<span class="string">"\\)"</span>] ],</span><br><span class="line">        displayMath: [ [<span class="string">'$$'</span>,<span class="string">'$$'</span>], [<span class="string">"\\["</span>,<span class="string">"\\]"</span>] ],</span><br><span class="line">        processEscapes: true</span><br><span class="line">    &#125;,</span><br><span class="line">        <span class="string">"HTML-CSS"</span>: &#123; fonts: [<span class="string">"TeX"</span>] &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">script(type=<span class="string">"text/javascript"</span> src=<span class="string">"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"</span>)</span><br></pre></td></tr></table></figure><h2 id="解决方案二"><a href="#解决方案二" class="headerlink" title="解决方案二"></a>解决方案二</h2><p>若mathjax不是单独配置的话，按照以下步骤进行。</p><h3 id="关闭hexo本身的mathjax"><a href="#关闭hexo本身的mathjax" class="headerlink" title="关闭hexo本身的mathjax"></a>关闭hexo本身的mathjax</h3><p>关闭hexo本身的mathjax，也就是在HEXO目录下的_config.yml，改为<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mathjax:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></p><p>然后关闭相应文章的<code>front-matter</code>中的<code>mathjax: true</code></p><h3 id="修改主题模板"><a href="#修改主题模板" class="headerlink" title="修改主题模板"></a>修改主题模板</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 修改<span class="selector-tag">head</span><span class="selector-class">.pug</span>，为了让全局页面中加入<span class="selector-tag">mathjax</span></span><br><span class="line"><span class="selector-tag">vim</span> <span class="selector-tag">head</span><span class="selector-class">.pug</span></span><br></pre></td></tr></table></figure><p>然后就在其中加入我们需要的mathjax<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">script(type=<span class="string">"text/x-mathjax-config"</span>).</span><br><span class="line">    <span class="symbol">MathJax</span>.<span class="symbol">Hub</span>.<span class="symbol">Config</span>(&#123;</span><br><span class="line">    extensions: [<span class="string">"tex2jax.js"</span>],</span><br><span class="line">    jax: [<span class="string">"input/TeX"</span>, <span class="string">"output/HTML-CSS"</span>],</span><br><span class="line">    tex2jax: &#123;</span><br><span class="line">        inlineMath: [ [<span class="string">'$'</span>,<span class="string">'$'</span>], [<span class="string">"\\("</span>,<span class="string">"\\)"</span>] ],</span><br><span class="line">        displayMath: [ [<span class="string">'$$'</span>,<span class="string">'$$'</span>], [<span class="string">"\\["</span>,<span class="string">"\\]"</span>] ],</span><br><span class="line">        processEscapes: true</span><br><span class="line">    &#125;,</span><br><span class="line">        <span class="string">"HTML-CSS"</span>: &#123; fonts: [<span class="string">"TeX"</span>] &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">script(type=<span class="string">"text/javascript"</span> src=<span class="string">"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"</span>)</span><br></pre></td></tr></table></figure></p><p>因为pug格式和html相似，因此如果格式是html的话，可以自己更改一下。</p><h2 id="修改前后对比"><a href="#修改前后对比" class="headerlink" title="修改前后对比"></a>修改前后对比</h2><h3 id="修改前"><a href="#修改前" class="headerlink" title="修改前"></a>修改前</h3><p><img src="/2018/10/14/更改hexo博客中的Mathjax引擎/E7217D6653FB277C412019A200AE3D19.png" alt="修改前的0-1损失函数" title="修改前的0-1损失函数"></p><h3 id="修改后"><a href="#修改后" class="headerlink" title="修改后"></a>修改后</h3><p>$$L(Y,f(x))=\begin{cases}<br>    1,&amp; Y \ne f(X)\\<br>    0,&amp; Y = f(X)<br>\end{cases}$$</p><p>$$<br>\begin{matrix}<br>    1 &amp; x &amp; x^2 \\<br>    1 &amp; y &amp; y^2 \\<br>    1 &amp; z &amp; z^2 \\<br>\end{matrix}<br>$$</p><blockquote><p>PS:在该版本中，换行符是<code>\\\</code>，而不是两个<code>\</code>，请在测试的时候注意一下。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mac配置ssh免密登录腾讯云服务器</title>
      <link href="/2018/10/13/mac%E9%85%8D%E7%BD%AEssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
      <url>/2018/10/13/mac%E9%85%8D%E7%BD%AEssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我在前一段时间，斥巨资购买了一年的云服务器，但是一直落着吃灰，今天忽然想起来折腾一下。</p><p><br></p><h2 id="ssh直连"><a href="#ssh直连" class="headerlink" title="ssh直连"></a>ssh直连</h2><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -i ~/.ssh/<span class="string">[密钥名称]</span> <span class="string">[账户]</span>@<span class="string">[ip地址]</span></span><br></pre></td></tr></table></figure><p><img src="/2018/10/13/mac配置ssh免密登录腾讯云服务器/QQ20181013-165116.png" alt="连接上的效果" title="连接上的效果"></p><p>可以看到我们已经连接上了腾讯云服务器，但是是不是每次都需要输入很长的ip地址，这就很烦了，因此接下来我们设置一下hosts，用代号替换ip地址。</p><h2 id="设置hosts"><a href="#设置hosts" class="headerlink" title="设置hosts"></a>设置hosts</h2><p>以下以mac osx系统为例<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim <span class="regexp">/etc/</span>hosts</span><br></pre></td></tr></table></figure></p><p>然后直接在最后加上你的云服务器ip地址和想要表示的名字。<br>或者用下面的语句直接添加：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"[ip地址]  [替代名]"</span></span></span><br></pre></td></tr></table></figure></p><p><img src="/2018/10/13/mac配置ssh免密登录腾讯云服务器/QQ20181013-165910.png" alt="修改后的样子" title="修改后的样子"></p><h3 id="设置免密登录"><a href="#设置免密登录" class="headerlink" title="设置免密登录"></a>设置免密登录</h3><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>cd .ssh</span><br><span class="line"><span class="variable">$ </span>ssh-keygen</span><br><span class="line"><span class="variable">$ </span>ssh-copy-id ubuntu<span class="variable">@t_cloud</span></span><br></pre></td></tr></table></figure><p>首先进入.ssh目录，生成密钥，然后将公钥传给云服务器。注意：在腾讯云服务器中，默认的用户是ubuntu，而不是root。</p><p><img src="/2018/10/13/mac配置ssh免密登录腾讯云服务器/QQ20181013-170408.png" alt="密钥传送完毕" title="密钥传送完毕"></p><p>最后尝试免密登录吧</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ssh</span> ubuntu<span class="variable">@t_cloud</span></span><br></pre></td></tr></table></figure><p><img src="/2018/10/13/mac配置ssh免密登录腾讯云服务器/QQ20181013-170625.png" alt="免密登录成功" title="免密登录成功"></p><p>这样就免密登录成功了，再也不需要繁琐的输入密码了！</p>]]></content>
      
      
      <categories>
          
          <category> 技术技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssh配置 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>感知机的原理及实现</title>
      <link href="/2018/10/10/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"/>
      <url>/2018/10/10/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>经过第一章漫长的学习，终于进入到了算法的学习，也很激动地开始了对感知机的学习。</p><h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><h3 id="感知机的定义"><a href="#感知机的定义" class="headerlink" title="感知机的定义"></a>感知机的定义</h3><blockquote><p>假设输入空间（特征空间）是$\mathcal{X} \subseteq R^n$，输出空间是$\mathcal{Y}=\{+1,-1\}$，输入$x\in\mathcal{X}$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y\in\mathcal{Y}$表示实例的类别，由输入空间到输出空间的如下函数：<br>$$f(x)=sign(w\bullet x+b)$$<br>称为感知机，其中，w和b为感知机模型参数，$w\in R^n$叫作权值（weight）或权值向量（weight vector），$b\in R$叫作偏置（biss），$w\bullet x$表示w和x的内积，sign表示符号函数，即：<br>$$sign(x)=\begin{cases}<br> +1,x\ge 0 \\<br>-1,x&lt;0\end{cases}$$<br>感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型或线性分类器，即函数集合$ \{ f|f(x)=w\bullet x+b=0 \} $</p></blockquote><p>对于特征空间$R^n$中的一个超平面$S$，其中w是超平面的法向量，$b$是超平面的截距。这个超平面将特征空间划分为两个部分，位于两部分的点（特征向量）分别被分为正、负两类，因此超平面S被称为分离超平面。如下图所示:</p><p><img src="/2018/10/10/感知机的原理及实现/感知机模型.png" alt="感知机模型"></p><h2 id="感知机学习策略"><a href="#感知机学习策略" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h2><h3 id="数据集的线性可分性"><a href="#数据集的线性可分性" class="headerlink" title="数据集的线性可分性"></a>数据集的线性可分性</h3><blockquote><p>定义(数据集的线性可分性)：给定一个数据集$$T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$$<br>其中$x_i\in \mathcal{X}=R^n$,$y_i\in \mathcal{Y}=\{+1,-1\}$，$i=1,2,…,N$，如果存在某个超平面$S$$$w\bullet x_i +b &lt;0$$能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i=+1$的实例$i$，有$w\bullet x_i+b&gt;0$，对所有$y_i=1$的实例$i$，有$w\bullet x_i+b&lt;0$，则称数据集T为线性可分数据集，否则称数据集T线性不可分。</p></blockquote><p>在我的理解中，问题的关键在于存不存在一个超平面将所有的数据集按照正例负例分割开来，若存在，则线性可分；不存在，则线性不可分。</p><h3 id="感知机学习策略-1"><a href="#感知机学习策略-1" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h3><p>如果训练数据集是线性可分的，感知机学习的目标是找到能够将正、负实例点完全正确分开的<code>分离超平面</code>，为了找到这样的超平面，我们也就要找到感知机的参数，因此需要确定一个学习策略，即定义（经验）损失函数并将损失函数极小化。</p><p>损失函数的选择有很多，最简单的一个就是<code>误分点的个数</code>，但是这样的损失函数并不是参数w、b的连续可导函数，不容易优化。另一个选择则是<code>误分类点到超平面S的总距离</code>，这个是感知机所采用的。<br>输入空间$R^n$中任一点$x_0$到超平面S的距离：<br>$$\frac {1}{||w||} |w\bullet x_0+b|，其中||w||是w的L_2范数$$<br>其次，对于误分类的数据$(x_i,y_i)$，来说$$-y_i(w\bullet x_i+b)&gt;0$$成立,也意味着预测得到值和真实值符号相反。<br>这样，假设超平面S的误分类点集合为M，那么所有误分类点到超平面S的总距离为：$$-\frac{1}{||w||} \sum_{x_i\in M}{y_i(w\bullet x_i+b)}$$不考虑$\frac{1}{||w||}$，就得到感知机学习的损失函数.<br>$$L(w,b)=-\sum_{x_i\in M}{y_i(w\bullet x_i+b)}$$<br>其中，$M$是误分类点的集合，这个损失函数就是感知机学习的经验风险函数。<br>可以看出，损失函数$L(w,b)$是非负的，如果没有误分类点，损失函数值是0.而误分类点越少，误分类点距离超平面也就越近，损失函数值就越小。一个特定的样本点的损失函数：在误分类时是参数$w,b$的线性函数，在正确分类时为0，因此在给定训练数据T，损失函数$L(w,b)$是$w，b$的连续可导函数。</p><h2 id="感知机学习算法"><a href="#感知机学习算法" class="headerlink" title="感知机学习算法"></a>感知机学习算法</h2><h3 id="感知机学习算法的原始形式"><a href="#感知机学习算法的原始形式" class="headerlink" title="感知机学习算法的原始形式"></a>感知机学习算法的原始形式</h3><p>感知机算法其实就是求解损失函数极小化问题的解$$\min_{w,b} L(w,b)=-\sum_{x_i\in M} y_i(w\bullet x_i +b)$$<br>感知机学习算法是由误分类驱动的，具体采用随机梯度下降法，过程是首先，任意选取一个超平面$w_0,b_0$，然后用梯度下降法不断地极小化目标函数，一次随机选取一个误分类点使其梯度下降。<br>假设误分类点集合M是固定的，那么损失函数$L(w,b)$的梯度如下：</p><p>$$\nabla_w L(w,b)=-\sum_{x_i\in M} y_ix_i\\<br>\nabla_b L(w,b)=-\sum_{x_i\in M} y_i$$</p><p>随机选取一个误分类点$(x_i,y_i)$，对$w,b$进行更新：</p><p>$$w \gets w+\eta y_ix_i \\<br>b \gets b+\eta y_i$$</p><p>其中，式子中的$\eta$是步长，也就是我们熟知的学习率(learning rate)。</p><h3 id="感知机学习算法的原始形式算法"><a href="#感知机学习算法的原始形式算法" class="headerlink" title="感知机学习算法的原始形式算法"></a>感知机学习算法的原始形式算法</h3><blockquote><p>输入：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i \in \mathcal{X} = R^n$，$y_i \in \mathcal{Y}=\{-1,+1\}$，$i=1,2,…,N$，学习率$\eta(0&lt;\eta\le 1);$<br>输出：$w,b$，感知机模型$f(x)=sign(w\bullet x+b)$.</p><ol><li>选取初值$w_0,b_0$</li><li>在训练集中选取数据$(x_i,y_i)$</li><li>如果$y_i(w\bullet x_i+b)\le 0$ $$w \gets w+\eta y_ix_i$$$$b \gets b+\eta y_i$$</li><li>转至2步骤，直至训练集中没有误分类点。</li></ol></blockquote><p>这种学习方法在直观上有如下解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$w,b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该分类点使其被正确分类。</p><h3 id="感知机学习算法的原始形式的实现"><a href="#感知机学习算法的原始形式的实现" class="headerlink" title="感知机学习算法的原始形式的实现"></a>感知机学习算法的原始形式的实现</h3><h4 id="code"><a href="#code" class="headerlink" title="code"></a>code</h4><p>根据上述的原始形式算法，直接构建算法模型，数据是通过公式创造的，然后用于分类，实现如下：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">import <span class="built_in">random</span></span><br><span class="line">import numpy as <span class="built_in">np</span></span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"># 创建数据集</span><br><span class="line">def createData(t_w,t_b,num_size):</span><br><span class="line">    <span class="built_in">random</span>.seed(<span class="number">78</span>)</span><br><span class="line">    x,y=[],[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_size):</span><br><span class="line">        x.<span class="built_in">append</span>([<span class="built_in">random</span>.uniform(<span class="number">0</span>,<span class="number">10</span>),<span class="built_in">random</span>.uniform(<span class="number">0</span>,<span class="number">10</span>)])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">        sy=i[<span class="number">0</span>]*t_w[<span class="number">0</span>]+i[<span class="number">1</span>]*t_w[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> sy&gt;t_b:</span><br><span class="line">            y.<span class="built_in">append</span>(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y.<span class="built_in">append</span>(-<span class="number">1</span>)</span><br><span class="line">    x=<span class="built_in">np</span>.<span class="built_in">array</span>(x)</span><br><span class="line">    y=<span class="built_in">np</span>.<span class="built_in">array</span>(y)</span><br><span class="line">    <span class="built_in">return</span> x,y</span><br><span class="line"></span><br><span class="line">#训练过程</span><br><span class="line">def train(x,y,learning_rate):</span><br><span class="line">    w=<span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">    b=<span class="number">0</span></span><br><span class="line">    finish_flag=False</span><br><span class="line">    <span class="built_in">num</span>=<span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="keyword">not</span> finish_flag):</span><br><span class="line">        <span class="built_in">num</span>+=<span class="number">1</span></span><br><span class="line">        finish_flag=True</span><br><span class="line">        f_num=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len(y)):</span><br><span class="line">            <span class="keyword">if</span> y[i]*(<span class="built_in">np</span>.dot(w,x[i])+b)&lt;=<span class="number">0</span>:</span><br><span class="line">                w=w+learning_rate*y[i]*x[i]</span><br><span class="line">                b=b+learning_rate*y[i]</span><br><span class="line">                finish_flag=False</span><br><span class="line">                f_num+=<span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Traversing the &#123;&#125; times ,w= [&#123;:.2f&#125;,&#123;:.2f&#125;] b= &#123;:.2f&#125; accuracy= &#123;:.2f&#125;."</span>.format(<span class="built_in">num</span>,w[<span class="number">0</span>],w[<span class="number">1</span>],b,(<span class="number">10</span>-f_num)/<span class="number">10</span>))</span><br><span class="line">    <span class="built_in">return</span> w,b</span><br><span class="line"></span><br><span class="line">#预测出来的model</span><br><span class="line">def f(w,b):</span><br><span class="line">    x1 = <span class="built_in">np</span>.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">0.1</span>)</span><br><span class="line">    <span class="built_in">return</span> x1,[-(w[<span class="number">0</span>]*i+b)/w[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> x1]</span><br><span class="line"></span><br><span class="line">#图像展示</span><br><span class="line">def <span class="built_in">show</span>(x,y,w,b):</span><br><span class="line">    x1=<span class="built_in">np</span>.dot(x,[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">    x2=<span class="built_in">np</span>.dot(x,[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="built_in">color</span>=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">np</span>.<span class="built_in">array</span>([i==-<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> y]):</span><br><span class="line">        <span class="keyword">if</span> i:</span><br><span class="line">            <span class="built_in">color</span>.<span class="built_in">append</span>(<span class="number">255</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">color</span>.<span class="built_in">append</span>(<span class="number">0</span>)</span><br><span class="line">    plt.scatter(x1,x2,c=<span class="built_in">color</span>)</span><br><span class="line">    xx,yy=f(w,b)</span><br><span class="line">    plt.plot(xx,yy)</span><br><span class="line">    plt.<span class="built_in">xlabel</span>(<span class="string">"x0"</span>)</span><br><span class="line">    plt.<span class="built_in">ylabel</span>(<span class="string">"x1"</span>)</span><br><span class="line">    plt.xlim(<span class="number">0</span>,<span class="number">10</span>)</span><br><span class="line">    plt.ylim(<span class="number">0</span>,<span class="number">10</span>)</span><br><span class="line">    plt.<span class="built_in">title</span>(<span class="string">"Realization of perceptron"</span>)</span><br><span class="line">    plt.<span class="built_in">show</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == '__main__':</span><br><span class="line">    t_w=<span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">3</span>,-<span class="number">2</span>])    #学习方便，仅设计两个参数，两个特征值</span><br><span class="line">    t_b=<span class="number">4.3</span>                 #生成值的b</span><br><span class="line">    num_size=<span class="number">80</span>             #创造的训练数据量</span><br><span class="line">    learning_rate=<span class="number">0.1</span></span><br><span class="line">    x,y=createData(t_w,t_b,num_size)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"train data"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"&#123;&#125;*w0+&#123;&#125;*w1+b &gt;= 0? &#123;&#125;"</span>.format(x[i][<span class="number">0</span>],x[i][<span class="number">1</span>],y[i]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"Answer"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    w,b=train(x,y,learning_rate)</span><br><span class="line">    <span class="built_in">show</span>(x,y,w,b)</span><br></pre></td></tr></table></figure></p><h4 id="实现结果"><a href="#实现结果" class="headerlink" title="实现结果"></a>实现结果</h4><p><img src="/2018/10/10/感知机的原理及实现/Figure_1.png" alt="感知机学习算法的原始形式的结果图"><br><img src="/2018/10/10/感知机的原理及实现/QQ20181016-090136.png" alt="感知机学习算法的原始形式的预测性能与结果"></p><h3 id="感知机学习算法的对偶形式"><a href="#感知机学习算法的对偶形式" class="headerlink" title="感知机学习算法的对偶形式"></a>感知机学习算法的对偶形式</h3><blockquote><p>输入：线性可分的数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i \in R^n$，$x_i \in R^n,y_i \in \{-1,+1\},i=1,2,…,N$，$学习率\eta(0&lt;\eta\le 1)$<br>输出：$\alpha,b$；感知机模型$f(x)=sign(\sum_{j=1}^{N}\alpha_j y_j x_j+b)$，其中$\alpha = (\alpha_1,\alpha_2,…,\alpha_N)^T.$</p><ol><li>$\alpha\gets 0,b\gets 0$</li><li>在训练集中选取数据$(x_i,y_i)$</li><li>如果$y_i\lgroup\sum_{j=1} ^N{\alpha_j y_j x_i +b}\rgroup\le 0$ $$\alpha_i \gets \alpha_i+\eta$$ $$b\gets b+\eta y_i$$</li><li>转至2直到没有误分类数据</li></ol><p>对偶形式中训练实例仅为内积的形式出现，为了方便，可以预先将训练集中实例的内积计算出来并用矩阵的形式存储，这个矩阵就是所谓的Gram矩阵$$G=[x_i\bullet x_j]_{N\times N}$$</p></blockquote><h3 id="感知机学习算法的对偶形式的实现"><a href="#感知机学习算法的对偶形式的实现" class="headerlink" title="感知机学习算法的对偶形式的实现"></a>感知机学习算法的对偶形式的实现</h3><h4 id="code-1"><a href="#code-1" class="headerlink" title="code"></a>code</h4><p>与原始形式不同的地方在于实例中的内积是通过求解Gram矩阵获得，详细代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Author   : yechenchen</span></span><br><span class="line"><span class="comment"># @Time     : 2018/10/9 下午5:19</span></span><br><span class="line"><span class="comment"># @File     : 感知机学习算法的对偶形式.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm Community Edition</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#感知机学习算法的对偶形式，P33</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">np.random.seed(<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CreateData</span><span class="params">(h_size,w_size)</span>:</span></span><br><span class="line">    x=np.random.rand(h_size,w_size)</span><br><span class="line">    <span class="keyword">return</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#给予对应的真实值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getY</span><span class="params">(X,feature_num)</span>:</span></span><br><span class="line">    w=np.random.rand(feature_num)</span><br><span class="line">    b=<span class="number">-0.2</span></span><br><span class="line">    print(w,b)</span><br><span class="line">    Y=np.array([])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> X:</span><br><span class="line">        <span class="keyword">if</span> np.dot(w,i)+b&gt;<span class="number">0</span>:</span><br><span class="line">            Y=np.append(Y,[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y=np.append(Y,[<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建Gram矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getG</span><span class="params">(X)</span>:</span></span><br><span class="line">    G=np.zeros((len(X),len(X)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(len(X)):</span><br><span class="line">            G[i][t]=np.dot(X[i],X[t])</span><br><span class="line">    <span class="keyword">return</span> G</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练过程，默认学习率为0.1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X,Y,G,feature_num,learning_rate=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    a=np.zeros(len(Y))</span><br><span class="line">    b=<span class="number">0</span></span><br><span class="line">    finish_flag=<span class="keyword">False</span></span><br><span class="line">    num=<span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> finish_flag:</span><br><span class="line">        finish_flag=<span class="keyword">True</span></span><br><span class="line">        num+=<span class="number">1</span></span><br><span class="line">        f_num=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(G)):</span><br><span class="line">            tmp=<span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(Y)):</span><br><span class="line">                tmp+=(a[j]*Y[j]*G[i][j])</span><br><span class="line">            ans=Y[i]*(tmp+b)</span><br><span class="line">            <span class="keyword">if</span> ans&lt;=<span class="number">0</span>:</span><br><span class="line">                a[i]+=<span class="number">1</span>*learning_rate   <span class="comment">#更新a值</span></span><br><span class="line">                b+=Y[i]*learning_rate   <span class="comment">#更新b值</span></span><br><span class="line">                finish_flag=<span class="keyword">False</span></span><br><span class="line">                f_num+=<span class="number">1</span></span><br><span class="line">        print(<span class="string">"Traversing the &#123;&#125; times , b= &#123;:.2f&#125; accuracy= &#123;:.2f&#125;."</span>.format(num,b,(len(Y)-f_num)/len(Y)))</span><br><span class="line">    w=np.zeros(feature_num)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        w+=a[i]*X[i]*Y[i]</span><br><span class="line">    <span class="keyword">return</span> w,b</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建预测到的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(w,b)</span>:</span></span><br><span class="line">    x0 = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> x0,[-(w[<span class="number">0</span>]*i+b)/w[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> x0]</span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化显示得到的函数以及散点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(x,y,w,b)</span>:</span></span><br><span class="line">    x1=np.dot(x,[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">    x2=np.dot(x,[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    color=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.array([i==<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> y]):</span><br><span class="line">        <span class="keyword">if</span> i:</span><br><span class="line">            color.append(<span class="number">255</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            color.append(<span class="number">0</span>)</span><br><span class="line">    plt.scatter(x1,x2,c=color)</span><br><span class="line">    xx,yy=f(w,b)</span><br><span class="line">    plt.plot(xx,yy)</span><br><span class="line">    plt.xlabel(<span class="string">"x0"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"x1"</span>)</span><br><span class="line">    plt.xlim(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    plt.ylim(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">"Realization of perceptron"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#主函数</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    feature_num=<span class="number">2</span>   <span class="comment">#每个样本具备的特征值的数量，设为2时可以可视化输出，否则需要用PCA降维</span></span><br><span class="line">    data_size=<span class="number">80</span>    <span class="comment">#训练数据量</span></span><br><span class="line">    X=CreateData(data_size,feature_num)    <span class="comment">#每组数据五个特征值，共80组数据</span></span><br><span class="line">    Y=getY(X,feature_num)</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"train data"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(data_size):</span><br><span class="line">        print(X[i],Y[i])</span><br><span class="line">    print(<span class="string">"*"</span> * <span class="number">10</span>, <span class="string">"train"</span>, <span class="string">"*"</span> * <span class="number">10</span>)</span><br><span class="line">    G=getG(X)</span><br><span class="line">    w,b=train(X,Y,G,feature_num)</span><br><span class="line">    print(<span class="string">"*"</span> * <span class="number">10</span>, <span class="string">"ans"</span>, <span class="string">"*"</span> * <span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"w=&#123;&#125;,b=&#123;&#125;"</span>.format(w,b))</span><br><span class="line">    show(X,Y,w,b)</span><br></pre></td></tr></table></figure></p><h4 id="实现结果-1"><a href="#实现结果-1" class="headerlink" title="实现结果"></a>实现结果</h4><p><img src="/2018/10/10/感知机的原理及实现/Figure_2.png" alt="感知机学习算法的对偶形式的结果图"><br><img src="/2018/10/10/感知机的原理及实现/QQ20181016-092327.png" alt="感知机学习算法的原始形式的预测性能与结果"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> 感知机 </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>《统计学习方法》概论整理</title>
      <link href="/2018/10/04/2018-10-04-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%A6%82%E8%AE%BA%E6%95%B4%E7%90%86/"/>
      <url>/2018/10/04/2018-10-04-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%A6%82%E8%AE%BA%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="前记"><a href="#前记" class="headerlink" title="前记"></a>前记</h2><p>在导师的推荐下，本人开始阅读李航博士的<code>《统计学习方法》</code>，该书是以内容短小精悍著称，因此我也从此书开始正式进入了机器学习的学习阶段。当然在这一节中，本人只是对该章节知识点进行梳理，由于多是基本知识点，所以量很大，内容也很多。</p><h2 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h2><h3 id="统计学习的概念"><a href="#统计学习的概念" class="headerlink" title="统计学习的概念"></a>统计学习的概念</h3><p>统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行<code>预测与分析</code>的一门学科，统计学习也称为统计机器学习。<br></p><h3 id="统计学习的对象"><a href="#统计学习的对象" class="headerlink" title="统计学习的对象"></a>统计学习的对象</h3><p>统计学习的对象是<code>数据</code>，它从数据出发，提取数据的特征，抽象出数据的模型，从数据中发现知识，又回到对数据的分析与预测中去。</p><h3 id="统计学习的方法"><a href="#统计学习的方法" class="headerlink" title="统计学习的方法"></a>统计学习的方法</h3><p>统计学习由监督学习(supervised learning)、非监督学习(unsupervised learning)、半监督学习(semi-supervised learning)、强化学习(reinforcement learning)等组成。</p><h3 id="统计学习中的常用概念"><a href="#统计学习中的常用概念" class="headerlink" title="统计学习中的常用概念"></a>统计学习中的常用概念</h3><ul><li>我们认为要学习的模型属于某个函数的集合，该集合被称为<code>假设空间</code>。<br></li><li>训练数据、测试数据：意如其名。<br></li><li>统计学习方法的三要素为<code>模型(model)</code>、<code>策略(strategy)</code>、<code>算法(algorithm)</code>。</li></ul><h3 id="实现统计学习方法的步骤"><a href="#实现统计学习方法的步骤" class="headerlink" title="实现统计学习方法的步骤"></a>实现统计学习方法的步骤</h3><blockquote><p>(1) 得到一个有限的训练数据集合；<br><br>(2) 确定包含所有可能的模型的假设空间，即学习的模型的集合；<br><br>(3) 确定模型选择的准则，即学习的策略；<br><br>(4) 实现求解最优模型的算法，即学习的算法；<br><br>(5) 通过学习方法选择最优模型；<br><br>(6) 利用学习的最优模型对新数据进行预测或分析；</p></blockquote><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><h3 id="监督学习的概念"><a href="#监督学习的概念" class="headerlink" title="监督学习的概念"></a>监督学习的概念</h3><p>监督学习的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测（注意，这里的输入、输出是指某个系统的输入与输出，与学习的输入与输出不同）<br></p><p>本人认为，”<code>这里的输入、输出</code>“指的不是特征值，而仅仅只是指代系统需要输入输出样本。</p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li><strong>输入空间、特征空间与输出空间</strong><br><br>首先，输入空间与输出空间指的是输入输出所有的可能取值的集合。输入与输出空间可以是有限元素的集合，也可以是整个欧式空间，两者可以是指同一个空间，也可以是不同的空间，通常情况下，输出空间远远小于输入空间。<br><br>每个具体的输入是一个实例，通常由特征向量来表示，此时所有的特征向量存在的空间称为特征空间。特征空间的每一维对应于一个特征。</li><li><strong>联合概率分布</strong><br><br>监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)，P(X,Y)表示分布函数，或者称分布密度函数。</li><li><strong>假设空间</strong><br><br>模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。假设空间的确定意味着学习范围的确定。</li></ul><h2 id="统计学习三要素"><a href="#统计学习三要素" class="headerlink" title="统计学习三要素"></a>统计学习三要素</h2><p>$$方法=模型+策略+算法$$</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>在监督学习中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。<br><br>假设空间用$\mathcal{F}$表示，假设空间可以定义为决策函数的集合：<br></p><p>$$ \mathcal{F}=\{f|Y=f(X)\} $$</p><p>其中，X和Y是定义在输入空间$\mathcal{X}$和输出空间$\mathcal{Y}$上的变量，这时$\mathcal{F}$通常是由一个参数向量决定的函数族：<br></p><p>$$\mathcal{F}=\{ f|Y=f_\theta(X),\theta\in R^n \}$$</p><p>参数向量$\theta$取值与n维欧式空间$R^n$，称为参数空间。<br></p><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>统计学习的<code>目标</code>在于从假设空间中选取<strong>最优模型</strong>。首先需要引入损失函数与风险函数的概念，损失函数度量模型<code>一次预测</code>的好坏，风险函数度量<code>平均意义下</code>的模型预测的好坏。两者的区别就在于一次预测和平均意义。</p><h4 id="损失函数和风险函数（重要）"><a href="#损失函数和风险函数（重要）" class="headerlink" title="损失函数和风险函数（重要）"></a><strong>损失函数和风险函数（重要）</strong></h4><p>监督学习问题是在假设空间$\mathcal{F}$中选取模型$f$作为决策函数，对于给定的输入$X$,由$f(X)$给出相应的输出$Y$，这个输出的预测值$f(X)$与真实值$Y$可能一致也可能不一致，因此需要一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度，损失函数是$f(X)$和$Y$的非负实值函数，记作为$L(Y,f(X))$.<br>统计学习常用的损失函数有以下几种：</p><ul><li><p>0-1损失函数（0-1 loss function）<br></p><p>  $$L(Y,f(x))=\begin{cases}<br>  1,&amp; Y \ne f(X)\\<br>  0,&amp; Y = f(X)<br>  \end{cases}$$</p></li><li><p>平方损失函数（quadratic loss function）</p><p>$$L(Y,f(X))=(Y-f(X))^2$$</p></li><li><p>绝对损失函数（absolute loss function）</p><p>$$L(Y,f(X))=|Y-f(X)|$$</p></li><li><p>对数损失函数（logarithmic loss function）或对数似然损失函数（log-likelihood loss function）</p><p>$$L(Y,P(Y|X))=-log(P(Y|X))$$</p></li></ul><p>损失函数值越小，模型就越好，由于模型的输入、输出是随机变量，遵循联合分布$P(X,Y)$所以损失函数的期望是:<br></p><p>$$R_{exp}(f)=E_p[L(Y,f(X))]= \lmoustache_{x \times y}L(y,f(x))P(x,y)dxdy$$</p><p>这就是理论上模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数或期望损失。<br>由于联合分布是未知的，因此$R_exp(f)$不能直接计算，如果知道了联合分布那也就不需要学习了，一方面根据期望风险最小来学习模型要用到联合分布，而联合分布却是未知的，因此我们需要引入一个<code>经验风险</code>的概念，其实模型f(x)关于训练数据集的平均损失，记作$R_{emp}$</p><p>$$R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))   \qquad$$</p><p>注：上式子中，N代表着训练集中的样本数量</p><h4 id="经验风险最小化与结构风险最小化"><a href="#经验风险最小化与结构风险最小化" class="headerlink" title="经验风险最小化与结构风险最小化"></a><strong>经验风险最小化与结构风险最小化</strong></h4><p>在经验风险最小化的策略中，我们需要的是求解上述经验风险的最小化，就是求解最优化问题：<br></p><p>$$min_{f\in \mathcal{F}}  \frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\qquad$$</p><p>当样本容量足够大的时候，经验风险最小化有着很好的效果，因此被广泛采用。<br>当样本容量很小的时候，经验风险最小化学习的效果就未必很好，可能还会有过拟合的情况发生。结构风险最小化就是为了防止过拟合而提出来的策略，结构风险最小化等价于<strong>正则化</strong>，结构风险是在经验风险上加了表示模型复杂度的正则化项（或者称为罚项），在假设空间、损失函数、训练数据集确定的时候，<strong>结构风险</strong>的定义是</p><p>$$R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)$$</p><ul><li>$J(f)$为模型的复杂度，是定义在假设空间$\mathcal{F}$上的反函数，模型$f$越复杂，复杂度$J(f)$就越大。</li><li>$\lambda$是系数，泳衣权衡经验风险和模型复杂度。</li></ul><p>结构风险最小化策略认为结构风险最小的模型是最优的模型，所以最优模型，就是求解最优化的问题:<br></p><p>$$min_{f\in \mathcal{F}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)$$</p><p>在这时，监督学习问题就变成了经验风险或结构风险函数的最优化问题。</p><h4 id="公式整理"><a href="#公式整理" class="headerlink" title="公式整理"></a><strong>公式整理</strong></h4><table><thead><tr><th>风险名称</th><th>损失函数</th><th>最优化</th></tr></thead><tbody><tr><td>期望风险</td><td>$R_{exp}(f)=E_p[L(Y,f(X))]= \lmoustache_{x \times y}L(y,f(x))P(x,y)dxdy$</td><td>不可求</td></tr><tr><td>经验风险</td><td>$ R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))   \qquad $</td><td>$min_{f\in \mathcal{F}}R_{emp}(f)  $</td></tr><tr><td>结构风险</td><td>$R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)$</td><td>$min_{f\in \mathcal{F}}R_{srm}(f)  $</td></tr></tbody></table><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>算法是指学习模型的具体计算方法。按照以上所讲，统计学习问题归结为最优化问题，统计学习的算法称为求解最优化问题的算法，通过数值计算的方法求解，来保证找到全局最优解。</p><p>统计学习方法之间的不同，主要来自其模型、策略、算法的不同，若三者确定，统计学习的方法也就随之确定，因此称其为统计学习的三要素。</p><h2 id="模型评估与模型选择"><a href="#模型评估与模型选择" class="headerlink" title="模型评估与模型选择"></a>模型评估与模型选择</h2><h3 id="训练误差与测试误差"><a href="#训练误差与测试误差" class="headerlink" title="训练误差与测试误差"></a>训练误差与测试误差</h3><p>当损失函数给定时，基于损失函数的模型的训练误差和模型的测试误差就自然成为学习方法评估的标准，注意，统计学习方法具体采用的损失函数未必是评估时使用的损失函数，当然，让两者一致是比较理想的。</p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>如果一味地追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高，这种情况呗称为过拟合。过拟合是指学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但是对未知数据预测得很差的现象。</p><h2 id="正则化与交叉验证"><a href="#正则化与交叉验证" class="headerlink" title="正则化与交叉验证"></a>正则化与交叉验证</h2><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>正则化是结构风险最小化策略的实现，是在经验风险上加上正则化项和罚项。正则化项一般是模型复杂度的<code>单调递增函数</code>，也就是模型越复杂，正则化值越大，LOSS值也就越大。</p><p>正则化的一般形式：</p><p>$$min_{f\in \mathcal{F}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)$$</p><p>正则化项有着不同的形式，一般是与参数向量有关，经常使用范数来表示。</p><h3 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h3><p>这里就需要引入一个范数的概念了。</p><p>L0范数：实际上表示的为向量中非零元素的个数</p><p>$$L0(x)= ^0 \sqrt{\sum_{i=1}^n|x_i|^0}$$</p><ul><li>L1范数：又被称为曼哈顿距离、最小绝对误差等</li></ul><p>$$L1(X)=\sum_{i=1}^n|x_i|$$</p><ul><li>L2范数：又被称为欧式距离，是用的最多的距离度量</li></ul><p>$$L2(X)=\sqrt{\sum_{i=1}^n|x_i|^2}$$</p><p>范数在numpy中有具体的实现方法：<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下列语句可以实现求解a向量与b向量之间的距离</span></span><br><span class="line">np.linalg.norm(<span class="keyword">a</span>-b)</span><br></pre></td></tr></table></figure></p><h2 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h2><h3 id="生成方法"><a href="#生成方法" class="headerlink" title="生成方法"></a>生成方法</h3><p>生成方法是由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(X|Y)$作为预测的模型。这就是生成模型：</p><p>$$P(X|Y)=\frac{P(X,Y)}{P(X)}$$</p><p>该方法之所以称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。</p><h3 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h3><p>判别方法是数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。判别模型关心的是对给定的输入，应该预测什么样的输出。</p><h3 id="生成方法和判别方法的特点比较"><a href="#生成方法和判别方法的特点比较" class="headerlink" title="生成方法和判别方法的特点比较"></a>生成方法和判别方法的特点比较</h3><p>生成方法的特点：</p><ul><li>生成方法可以还原出联合概率分布$P(X,Y)$，而判别方法不能；</li><li>生成方法的学习收敛速度更快，模型可以更快地收敛于真实模型</li><li>当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。</li></ul><p>判别方法的特点：</p><ul><li>判别方法直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$</li><li>直接面对预测，往往学习的准确率更高。</li><li>由于直接学习$P(Y|X)或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</li></ul><h2 id="针对分类问题的指标（重要）"><a href="#针对分类问题的指标（重要）" class="headerlink" title="针对分类问题的指标（重要）"></a>针对分类问题的指标（重要）</h2><p>对二分类问题常用的评价指标是精准率和召回率。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4中情况出现的总数分别记作：</p><ul><li>TP——将正类预测为正类的数量</li><li>FN——将正类预测为负类的数量</li><li>FP——将负类预测为正类的数量</li><li>TN——将负类预测为负类的数量</li></ul><p>精准率的定义为$P=\frac{TP}{TP+FP}$，可以记为判断为正类的样本中的真实正类的比重。</p><p>召回率的定义为$R=\frac{TP}{TP+FN}$，可以记为真实的正类被判定为正类的比重。</p><p>$F_1$值，是精确率和召回率的调和均值，即$\frac{2}{F_1}=\frac{1}{P}\frac{1}{R}$，$F_1=\frac{2TP}{2TP+FP+FN}$，因此可以看出，当精准率和召回率都高的时候，$F1$值才会高。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本章节主要讲的都是统计学习中的一些基本概念，这份整理也花了不少的时间，应该理解深刻。从下一章开始，进入了十个算法的学习，并且每章都会在理论的基础上配上代码实现，尽请期待了。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>网络实践课程设计——五子棋</title>
      <link href="/2018/09/18/2018-09-18-%E7%BD%91%E7%BB%9C%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/"/>
      <url>/2018/09/18/2018-09-18-%E7%BD%91%E7%BB%9C%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="游戏形式"><a href="#游戏形式" class="headerlink" title="游戏形式"></a>游戏形式</h2><p>本游戏是通过socket来进行五子棋游戏，我们的形式是<code>一台server</code>开启之后可以支持尽可能多的<code>客户端</code>运行。</p><h2 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h2><ul><li><p><code>server</code></p><ul><li><code>net</code>：socket通信相关功能<ul><li>Action:解析socket传来的指令后进行的执行操作</li><li>EndDeal:游戏结束后的相关处理</li><li>Resolve：对收到的指令（字符串）进行解析处理</li><li>ServerThread：针对多个客户端，开启多个线程支持</li></ul></li><li><code>tool</code><ul><li>FightManager：下棋对战时的相关信息</li><li>HashMapManager：存储对战时配对上的map组合</li><li>MessageManager：用于进行消息管理，包括信息的发送等等</li><li>Player：存储玩家的个人信息</li><li><code>check</code>:进行游戏状态的检测<ul><li>check:检测胜负</li><li>checkX&amp;Y:横纵检测状态</li><li>checkM&amp;N:斜着检测状态</li></ul></li></ul></li><li><code>ui</code>：ui界面<ul><li>ClientPanel:客户端列表版面</li><li>MatchsPanel：配对连接版面</li><li>MessagePanel：消息显示版面</li><li>ServerFrame：服务器主窗口</li></ul></li><li>Server.java:主程序</li></ul></li><li><p><code>client</code></p><ul><li><code>data</code><ul><li>Data:存储玩家自己的相关信息，包括ID、昵称、配对对象等等</li></ul></li><li><code>image</code>：相关的图片，用于表示棋子等</li><li><code>listener</code>：用于监听相关的操作<ul><li>BackListener：悔棋操作</li><li>ChallengeListener：挑战操作</li><li>ConnectListener：登录操作</li><li>ListListener：列表双击操作</li><li>MapListener：监听棋盘</li><li>MessageListener：监听消息发送</li><li>NameListener：重命名操作</li><li>QuitListener：退出操作</li><li>RestartListener：重新开始操作</li><li>StartListener：游戏开始</li></ul></li><li><code>manager</code><ul><li>IOManager:输入输出流</li><li>ListManager:管理玩家列表</li><li>MessageManager：管理消息</li></ul></li><li><code>net</code><ul><li>Connect：登录服务器</li><li>PlayChess：游戏落字传输</li><li>Receive：接收数据线程</li><li>Resolve：解析数据</li></ul></li><li><code>ui</code><ul><li>ChessBoardCanvas：棋盘画板</li><li>FunctionPanel：功能区</li><li>GameFrame：游戏主界面</li><li>GamePanel：左边游戏区</li><li>MessagePanel：消息面板</li><li>OperationPanel：操作面板</li><li>PlayerPanel：玩家面板</li></ul></li><li>QuinterGame：客户端启动</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 课设记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Jekyll框架搭建</title>
      <link href="/2018/09/16/2018-09-16-MAC_OSX%E4%B8%8A%E5%AE%89%E8%A3%85jekyll/"/>
      <url>/2018/09/16/2018-09-16-MAC_OSX%E4%B8%8A%E5%AE%89%E8%A3%85jekyll/</url>
      
        <content type="html"><![CDATA[<p>1.安装ruby(mac上自带，此步可以跳过)</p><p>2.安装jekyll</p><ul><li><p>安装jekyll</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gem <span class="keyword">install</span> jekyll</span><br></pre></td></tr></table></figure></li><li><p>顺利地安装完成之后，可以生成自己的一个博客</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jekyll <span class="keyword">new</span> myBlog</span><br></pre></td></tr></table></figure></li><li><p>接下来尝试进入博客运行一下</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd myBlog</span><br><span class="line">jekyll <span class="keyword">server</span></span><br></pre></td></tr></table></figure></li><li><p>hin难受，果然报了个错！！！错误输出如下：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/System/</span>Library<span class="regexp">/Frameworks/</span>Ruby.framework<span class="regexp">/Versions/</span><span class="number">2.3</span><span class="regexp">/usr/</span>lib<span class="regexp">/ruby/</span><span class="number">2.3</span><span class="number">.0</span><span class="regexp">/rubygems/</span>core_ext/kernel_require.<span class="string">rb:</span><span class="number">55</span>:<span class="keyword">in</span> <span class="string">'require'</span>: cannot load such file -- bundler (LoadError)</span><br><span class="line">from <span class="regexp">/System/</span>Library<span class="regexp">/Frameworks/</span>Ruby.framework<span class="regexp">/Versions/</span><span class="number">2.3</span><span class="regexp">/usr/</span>lib<span class="regexp">/ruby/</span><span class="number">2.3</span><span class="number">.0</span><span class="regexp">/rubygems/</span>core_ext/kernel_require.<span class="string">rb:</span><span class="number">55</span>:<span class="keyword">in</span> <span class="string">'require'</span></span><br><span class="line">from <span class="regexp">/Library/</span>Ruby<span class="regexp">/Gems/</span><span class="number">2.3</span><span class="number">.0</span><span class="regexp">/gems/</span>jekyll<span class="number">-3.8</span><span class="number">.3</span><span class="regexp">/lib/</span>jekyll/plugin_manager.<span class="string">rb:</span><span class="number">48</span>:<span class="keyword">in</span> <span class="string">'require_from_bundler'</span></span><br><span class="line">from <span class="regexp">/Library/</span>Ruby<span class="regexp">/Gems/</span><span class="number">2.3</span><span class="number">.0</span><span class="regexp">/gems/</span>jekyll<span class="number">-3.8</span><span class="number">.3</span><span class="regexp">/exe/</span><span class="string">jekyll:</span><span class="number">11</span>:<span class="keyword">in</span> <span class="string">'&lt;top (required)&gt;'</span></span><br><span class="line">from <span class="regexp">/usr/</span>local<span class="regexp">/bin/</span><span class="string">jekyll:</span><span class="number">22</span>:<span class="keyword">in</span> <span class="string">'load'</span></span><br><span class="line">from <span class="regexp">/usr/</span>local<span class="regexp">/bin/</span><span class="string">jekyll:</span><span class="number">22</span>:<span class="keyword">in</span> <span class="string">'&lt;main&gt;'</span></span><br></pre></td></tr></table></figure></li><li><p>接下来就是上网找了找如何解决该问题，因为笔者也是小白，第一次搭博客，所幸找到了解决方案，感激！解决办法如下：</p></li><li><p>安装bundle</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gem <span class="keyword">install </span><span class="keyword">bundle</span></span><br><span class="line"><span class="keyword">gem </span><span class="keyword">install </span>minima</span><br><span class="line">gem <span class="keyword">install </span><span class="keyword">jekyll-feed</span></span><br></pre></td></tr></table></figure></li><li><p>本以为完成了，就开心的开启了一下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bundle </span>exec <span class="keyword">jekyll </span>serve</span><br></pre></td></tr></table></figure></li><li><p>但是！但是！但是！它又报了一个错！</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Could <span class="keyword">not</span> <span class="builtin-name">find</span> public_suffix-3.0.0 <span class="keyword">in</span> any of the sources</span><br><span class="line"><span class="builtin-name">Run</span> `bundle install` <span class="keyword">to</span> install missing gems.</span><br></pre></td></tr></table></figure></li><li><p>好的是，只要我们认真听话的执行一下提示代码就可以了！</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bundle </span>install</span><br></pre></td></tr></table></figure></li><li><p>然后等待全部安装完成</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">Bundle complete! <span class="number">4</span> Gemfile dependencies, <span class="number">23</span> gems now installed.</span><br><span class="line">Use `bundle info [gemname]` to see <span class="keyword">where</span> a bundled gem is installed.</span><br></pre></td></tr></table></figure></li><li><p>最后启动！</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bundle </span>exec <span class="keyword">jekyll </span>serve</span><br></pre></td></tr></table></figure></li><li><p>然后打开<a href="http://127.0.0.1:4000" target="_blank" rel="noopener">http://127.0.0.1:4000</a> ，就能完美的看到自己的博客啦，jekyll就已经安装完成咯。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Jekyll </tag>
            
            <tag> blog </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>2018华为秋招笔试</title>
      <link href="/2018/08/30/2018%E5%8D%8E%E4%B8%BA%E7%A7%8B%E6%8B%9B%E7%AC%94%E8%AF%95/"/>
      <url>/2018/08/30/2018%E5%8D%8E%E4%B8%BA%E7%A7%8B%E6%8B%9B%E7%AC%94%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>徘徊在保研与不保研之间，看了考研书感觉自己根本看不下去，看实验室同学都去了华为，因此就像报名了华为的秋招，报名了大数据开发岗，<del>由于之前参加比赛学过hadoop等大数据框架</del>，所以报了试试看，前几天通知了笔试，于是就去水了一波。</p><h2 id="笔试介绍"><a href="#笔试介绍" class="headerlink" title="笔试介绍"></a>笔试介绍</h2><p>华为今年的笔试和去年形式差不多，三道编程题，分值分别是100、200、300，语言基本上都可以使用，题目自我感觉很简单，因为都是一遍AC了，一个小时就解决了，hhh，就不吹了，下面记录记录题目和我的代码。</p><h3 id="字符串处理题"><a href="#字符串处理题" class="headerlink" title="字符串处理题"></a>字符串处理题</h3><p>题目是从一个字符串中找到所有存在的整数，包括负号，且负号可累计，然后将寻找到的整数相加即可，下面是代码：<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">import sys</span><br><span class="line">numlist= [str(i) <span class="keyword">for</span> i <span class="keyword">in</span> (range(<span class="number">10</span>))]</span><br><span class="line"><span class="comment"># print(numlist)</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">line</span> <span class="keyword">in</span> sys.<span class="keyword">stdin</span>:</span><br><span class="line">    <span class="keyword">a</span> = <span class="built_in">line</span>.<span class="built_in">split</span>()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">a</span>:</span><br><span class="line">        flag=True<span class="comment">#当前数字为正</span></span><br><span class="line">        <span class="built_in">sum</span>=<span class="number">0</span><span class="comment">#当前和</span></span><br><span class="line">        <span class="built_in">num</span>=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> <span class="keyword">char</span> <span class="keyword">in</span> i:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">char</span> == <span class="string">'-'</span>:</span><br><span class="line">                flag=<span class="keyword">not</span> flag</span><br><span class="line">            elif <span class="keyword">char</span> <span class="keyword">in</span> numlist:</span><br><span class="line">                <span class="comment">#数字处理</span></span><br><span class="line">                    <span class="built_in">num</span>=<span class="built_in">num</span>*<span class="number">10</span>+int(<span class="keyword">char</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> flag:</span><br><span class="line">                    <span class="built_in">sum</span>+=<span class="built_in">num</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">sum</span>-=<span class="built_in">num</span></span><br><span class="line">                <span class="comment"># sum+=num</span></span><br><span class="line">                <span class="comment"># print(sum)</span></span><br><span class="line">                <span class="built_in">num</span>=<span class="number">0</span></span><br><span class="line">                flag=True</span><br><span class="line">        <span class="keyword">if</span> flag:</span><br><span class="line">            <span class="built_in">sum</span>+=<span class="built_in">num</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">sum</span>-=<span class="built_in">num</span></span><br><span class="line">        print(<span class="built_in">sum</span>)</span><br></pre></td></tr></table></figure></p><h3 id="卷积计算"><a href="#卷积计算" class="headerlink" title="卷积计算"></a>卷积计算</h3><p>题目是需要按照给定的规则，来计算带有复数的卷积，卷积定义网上也有，我就不赘述了，直接开模拟，代码如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line">import sys</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">fuNum</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>,x,y)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.r=x</span><br><span class="line">        <span class="keyword">self</span>.im=y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(<span class="keyword">self</span>,b)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> fuNum(<span class="keyword">self</span>.r + b.r, <span class="keyword">self</span>.im + b.im)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sub</span><span class="params">(<span class="keyword">self</span>,b)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> fuNum(<span class="keyword">self</span>.r - b.r, <span class="keyword">self</span>.im - b.im)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mult</span><span class="params">(<span class="keyword">self</span>,b)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> fuNum(<span class="keyword">self</span>.r * b.r - <span class="keyword">self</span>.im * b.im, b.r * <span class="keyword">self</span>.im + <span class="keyword">self</span>.r * b.im)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name_<span class="number">_</span> == <span class="string">"__main__"</span><span class="symbol">:</span></span><br><span class="line">    duoa=[]</span><br><span class="line">    duob=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>)<span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 读取每一行</span></span><br><span class="line">        r = eval(sys.stdin.readline().strip())</span><br><span class="line">        im= eval(sys.stdin.readline().strip())</span><br><span class="line">        duoa.append(fuNum(r,im))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>)<span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 读取每一行</span></span><br><span class="line">        r = eval(sys.stdin.readline().strip())</span><br><span class="line">        im= eval(sys.stdin.readline().strip())</span><br><span class="line">        duob.append(fuNum(r,im))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)<span class="symbol">:</span></span><br><span class="line">        duoa.append(fuNum(<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">        duob.append(fuNum(<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    <span class="comment"># n=4</span></span><br><span class="line">    duoc=[]</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">9</span>)<span class="symbol">:</span></span><br><span class="line">        temp=fuNum(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(n+<span class="number">1</span>)<span class="symbol">:</span></span><br><span class="line">            <span class="comment"># print(k,n)</span></span><br><span class="line">            temp=temp.add(duoa[k].mult(duob[n-k]))</span><br><span class="line">        <span class="comment"># print(temp.r,temp.im)</span></span><br><span class="line">        duoc.append(temp)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(duoc)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="symbol">duoc:</span></span><br><span class="line">        print(i.r)</span><br><span class="line">        print(i.im)</span><br></pre></td></tr></table></figure></p><h3 id="牛生小牛的问题"><a href="#牛生小牛的问题" class="headerlink" title="牛生小牛的问题"></a>牛生小牛的问题</h3><p>传说中300分的题目，题目具体数据记不清了，是牛生小牛的问题，不过好像和经典例题的递归形式不同，这题我在思考的时候加上了dp的思想，<del>因为用递归模拟，发现过不了样例</del>，代码如下：<br><figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 读取第一行的n</span></span><br><span class="line">    n = <span class="built_in">int</span>(sys.<span class="literal">stdin</span>.readline().strip())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="comment"># 每一个例子</span></span><br><span class="line">        M = eval(sys.<span class="literal">stdin</span>.readline().strip())</span><br><span class="line">        N = eval(sys.<span class="literal">stdin</span>.readline().strip())</span><br><span class="line">        cheng=[M,M,M,M,M]</span><br><span class="line">        wei=[<span class="number">0</span>,M,<span class="number">2</span>*M,<span class="number">3</span>*M,<span class="number">4</span>*M]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>,N+<span class="number">1</span>):</span><br><span class="line">            cheng.append(cheng[i-<span class="number">1</span>]+(wei[i-<span class="number">3</span>]-wei[i-<span class="number">4</span>]))</span><br><span class="line">            wei.append(wei[i-<span class="number">1</span>]+cheng[i]-(wei[i-<span class="number">3</span>]-wei[i-<span class="number">4</span>]))</span><br><span class="line">            <span class="comment"># print(i,cheng[i],wei[i])</span></span><br><span class="line">        print(cheng[N]+wei[N])</span><br></pre></td></tr></table></figure></p><p>就这样三道题目就全部AC了，难度不是太大，感觉打过ACM的全AC是很正常的事情，所以对ACMer菊厂也是相当欢迎的。</p><hr><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>九月初拿到学校保研资格的那天，放弃了菊厂的面试，菊厂打了好几个电话来问，给人的感觉是相当负责的，当时和他们说等我研究生毕业再去菊厂了，哈哈哈，又装了13，溜了溜了看书了。</p>]]></content>
      
      
      <categories>
          
          <category> 刷题记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 笔试 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python实践——塔防游戏</title>
      <link href="/2018/07/20/python%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%A1%94%E9%98%B2%E6%B8%B8%E6%88%8F/"/>
      <url>/2018/07/20/python%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%A1%94%E9%98%B2%E6%B8%B8%E6%88%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这仅仅只是一个入门级游戏，一个小toy，使用的也就是贴图形式的pygame，因为需要过多的访存次数，图片的精度也较高，所以对内存以及硬盘的速度有着较高的要求。<del>当时答辩的辣鸡主机上，我们跑的就相当卡。</del></p><h2 id="环境要求"><a href="#环境要求" class="headerlink" title="环境要求"></a>环境要求</h2><p>语言版本：python 3.6.2<br><br>依赖：pygame<br><br>系统：皆可<br></p><h2 id="游戏按键"><a href="#游戏按键" class="headerlink" title="游戏按键"></a>游戏按键</h2><p>暂停/开始：Space<br><br>退出：Esc<br></p><h2 id="截图"><a href="#截图" class="headerlink" title="截图"></a>截图</h2><p><img src="/2018/07/20/python实践——塔防游戏/开始界面.png" alt="游戏开成动画" title="游戏开成动画"><br><img src="/2018/07/20/python实践——塔防游戏/游戏截图.png" alt="游戏中截图" title="游戏截图"></p><h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><p><a href="https://github.com/netycc/Tower-defense-game" title="塔防游戏" target="_blank" rel="noopener">https://github.com/netycc/Tower-defense-game</a></p>]]></content>
      
      
      <categories>
          
          <category> 课设记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
