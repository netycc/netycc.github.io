<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>2019科大讯飞算法岗校招笔试</title>
      <link href="/2018/10/25/2019%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E%E7%AE%97%E6%B3%95%E5%B2%97%E6%A0%A1%E6%8B%9B%E7%AC%94%E8%AF%95/"/>
      <url>/2018/10/25/2019%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E%E7%AE%97%E6%B3%95%E5%B2%97%E6%A0%A1%E6%8B%9B%E7%AC%94%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前天收到了科大讯飞算法岗的笔试邀请，因为以后也想从事算法岗的工作，因此试了试水。讯飞在算法岗的笔试题中，有着20题每题三分的选择题，选择题包括了HMM、CRF、生成模型、判别模型等机器学习相关知识，令我以外的是他也包括了很多操作系统相关的算法，比如银行家算法。而后就是两道很简单的编程题，每题二十分。</p><h2 id="编程题"><a href="#编程题" class="headerlink" title="编程题"></a>编程题</h2><h3 id="修改成绩"><a href="#修改成绩" class="headerlink" title="修改成绩"></a>修改成绩</h3><h4 id="题面"><a href="#题面" class="headerlink" title="题面"></a>题面</h4><p><em>题目描述</em><br>华老师的n个学生参加了一次模拟测验，考出来的分数很糟糕，但是华老师可以将成绩修改为[0,100]中的任意值，所以他想知道，如果要使所有人的成绩的平均分不少于X分，至少要改动多少个人的分数？<br><em>输入</em><br>第一行一个数T，共T组数据（T≤10）<br>接下来对于每组数据：<br>第一行两个整数n和X。（1≤n≤1000, 0≤X≤100）<br>第二行n个整数，第i个数Ai表示第i个学生的成绩。（0≤Ai≤100）<br><em>输出</em><br>共T行，每行一个整数，代表最少的人数。<br><em>样例输入</em><br>2<br>5 60<br>59 20 30 90 100<br>5 60<br>59 20 10 10 100<br><em>样例输出</em><br>1<br>2<br><em>Hint</em><br>对于第一组数据，将59改成60即可</p><h4 id="AC代码"><a href="#AC代码" class="headerlink" title="AC代码"></a>AC代码</h4><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    n = <span class="keyword">int</span>(sys.stdin.readline().strip())</span><br><span class="line">    <span class="keyword">for</span> i in <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="built_in">line</span> = sys.stdin.readline().strip()</span><br><span class="line">        two=<span class="keyword">list</span>(<span class="keyword">map</span>(<span class="keyword">int</span>, <span class="built_in">line</span>.<span class="keyword">split</span>()))</span><br><span class="line">        <span class="built_in">line</span>=sys.stdin.readline().strip()</span><br><span class="line">        n_num=<span class="keyword">list</span>(<span class="keyword">map</span>(<span class="keyword">int</span>, <span class="built_in">line</span>.<span class="keyword">split</span>()))</span><br><span class="line">        mubiao=two[<span class="number">0</span>]*two[<span class="number">1</span>]</span><br><span class="line">        n_num=sorted(n_num)</span><br><span class="line">        flag=<span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> sum(n_num)&lt;mubiao <span class="built_in">and</span> flag&lt;=<span class="built_in">len</span>(n_num):</span><br><span class="line">            n_num[flag]=<span class="number">100</span></span><br><span class="line">            flag+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">print</span>(flag)</span><br></pre></td></tr></table></figure><h3 id="杀手"><a href="#杀手" class="headerlink" title="杀手"></a>杀手</h3><h4 id="题面-1"><a href="#题面-1" class="headerlink" title="题面"></a>题面</h4><p><em>题目描述</em><br>有n个杀手排成一行，每个杀手都有一个不同的编号(编号为1-n)，在每个夜晚，杀手都会行动，如果某个杀手编号大于他右边的杀手的编号，他就会杀死他右边的杀手，杀手是的行动是瞬间的，因此一个人可能某一个夜晚既杀死了别人又被别人杀死，例如3,2,1这个顺序，在第一个夜晚2会杀死1，同时3也会杀死2。<br>显而易见，一段时间之后，就不会有人杀或被杀，平安夜也就到来了，请问在平安夜之前有多少个夜晚。<br><em>输入</em><br>输入第一行是一个整数n（1≤n≤100000）,表示杀手的数量。<br>接下来一行有n个数，是一个1-n的全排列。<br><em>输出</em><br>输出包含一个整数，表示平安夜之前经历了多少个夜晚。<br><em>样例输入</em><br>10 10 9 7 8 6 5 3 4 2 1<br><em>样例输出</em><br>2<br><em>Hint</em><br>补充样例<br>输入样例2： 6 1 2 3 4 5 6<br>输出样例2 ：0<br>样例解释： 样例1中杀手的变化为[10 9 7 8 6 5 3 4 2 1]-&gt;[10 8 4]-&gt;[10]，故答案为2。</p><h4 id="AC代码-1"><a href="#AC代码-1" class="headerlink" title="AC代码"></a>AC代码</h4><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    n = int(sys.<span class="keyword">stdin</span>.readline().strip())</span><br><span class="line">    <span class="built_in">line</span> = sys.<span class="keyword">stdin</span>.readline().strip()</span><br><span class="line">    <span class="built_in">num</span>=list(map(int, <span class="built_in">line</span>.<span class="built_in">split</span>()))</span><br><span class="line">    flag=True</span><br><span class="line">    cishu=<span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span>(flag):</span><br><span class="line">        n=<span class="built_in">len</span>(<span class="built_in">num</span>)</span><br><span class="line">        next=[]</span><br><span class="line">        flag=False</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="built_in">len</span>(<span class="built_in">num</span>)+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 发生吃人</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">num</span>[n-i]&gt;<span class="built_in">num</span>[n-i+<span class="number">1</span>]):</span><br><span class="line">                flag=True</span><br><span class="line">            <span class="comment"># 吃不掉</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">num</span>[n-i]&lt;<span class="built_in">num</span>[n-i+<span class="number">1</span>]):</span><br><span class="line">                next.append(<span class="built_in">num</span>[n-i+<span class="number">1</span>])</span><br><span class="line">                <span class="comment"># flag=True</span></span><br><span class="line">            <span class="comment"># 加入第一个人</span></span><br><span class="line">            <span class="keyword">if</span>(i==<span class="built_in">len</span>(<span class="built_in">num</span>)):</span><br><span class="line">                next.append(<span class="built_in">num</span>[n-i])</span><br><span class="line">        <span class="built_in">num</span>=list(reversed(next))</span><br><span class="line">        <span class="keyword">if</span>(flag==True):</span><br><span class="line">            cishu+=<span class="number">1</span></span><br><span class="line">    print(cishu)</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>不得不说讯飞的速度是挺快的，第二天就发来了面试邀请。但是想问hr招不招实习的时候，hr不回复，因此也就没下文了。面试当然也就没去了，~难受~<br><img src="/2018/10/25/2019科大讯飞算法岗校招笔试/QQ20181026-152734.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 刷题记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 笔试 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>决策树的原理及实现</title>
      <link href="/2018/10/19/%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"/>
      <url>/2018/10/19/%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>不得不说，相对于前面几章，决策树的新鲜知识点明显要多了，引入了很多<em>信息论</em>中的内容，<del>看的很难受呀</del>，不过在信念的坚持下，还是开始总结了本章的知识点。</p><p>决策树是一种基本的分类与回归方法。本篇文章主要讨论分类的决策树，决策树呈现树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以被认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。主要优点是<em>模型具有可读型、分类速度快</em>，学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据利用决策树模型进行分类，决策树学习通常包括3个步骤：<em>特征选择、决策树的生成和决策树的修剪</em>。</p><h2 id="决策树模型与学习"><a href="#决策树模型与学习" class="headerlink" title="决策树模型与学习"></a>决策树模型与学习</h2><h3 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h3><blockquote><p>定义5.1（决策树） 分类决策树模型是一种描述对实例进行分类的树形结构，决策树由结点（node）和有向边（directed edge）组成。结点有两种类型，内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。<br>用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值，如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分到叶结点的类中。</p></blockquote><h3 id="决策树与if-then规则"><a href="#决策树与if-then规则" class="headerlink" title="决策树与if-then规则"></a>决策树与if-then规则</h3><p>可以将决策树看成一个if-then规则的集合。将决策树转换成if-then规则的过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树有着<em>互斥并且完备</em>这一特点。就是说，每一个实例都被一条路径或一条规则所覆盖，并且只被一条路径或一条规则所覆盖。</p><h3 id="决策树与条件概率分布"><a href="#决策树与条件概率分布" class="headerlink" title="决策树与条件概率分布"></a>决策树与条件概率分布</h3><p>决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义。这一条件概率分布定义在特征空间上的一个划分上。将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设X为表示特征的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为P(Y|X)。X取值于给定划分下单元的集合，Y取值于类的集合。各叶结点（单元）上的条件概率往往偏向某一个类，即属于某一类的概率较大。决策树分类时将该结点强行分到条件概率大的那一类去。</p><h3 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h3><p>决策树学习，假设给定训练数据集$$D=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$$其中，$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T$为输入实例（特征向量），$n$为特征个数，$y_i\in \{1,2,…,K\}$为类标记，$i=1,2,…,N$，$N$为样本容量，学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。<br>决策树学习本质上是从数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛华能力。<br>当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用<em>启发式</em>方法，近似求解这一最优化问题。这样得到的决策树是次最优的。<br>决策树学习的算法通常是一个递归地选择最优特征，并根据特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如果递归地进行下去，直至所有训练数据集被基本正确分类，或者没有合适的特征为止，最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一课决策树。<br>以上方法生成的决策树可能对训练数据集有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生<em>过拟合</em>现象。我们需要对已生成的树自下向上而进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。</p><h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><h3 id="特征选择问题"><a href="#特征选择问题" class="headerlink" title="特征选择问题"></a>特征选择问题</h3><p>特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是<em>信息增益</em>或<em>信息增益比</em>。</p><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为$$P(X=x_i)=p_i  ,i=1,2,…,n$$则随机变量$X$的熵定义为$$H(X)=-\sum_{i=1}^{n}{p_i log p_i}$$在式中，若$p_i=0$，则定义$0log0=0$。通常，该式中的对数以2为底或以e为底（自然对数），这时熵的单元分别称作比特或纳特。由定义可知，<em>熵只依赖于X的分布</em>，而与X的取值无关，所以也可将X的熵记作$H(p)$，即$$H(p)=-\sum_{i=1}^{n}p_i log p_i$$</p><p>熵越大，随机变量的不确定性就越大，当$p=0$或$p=1$时$H(p)=0$，随机变量完全没有不确定性。当$p=0.5$时，$H(p)=1$，熵取值就最大，随机变量不确定性最大。</p><p>设有随机变量$(X,Y)$，其联合概率分布为$$P(X=x_i,Y=y_j)=p_{ij}，i=1,2,…,n;j=1,2,…,m$$条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵$H(Y|X)$，定义为X给定条件下Y的条件概率分布的熵对X的数学期望$$H(Y|X)=\sum_{i=1}^{n}{p_iH(Y|X=x_i)}$$这里，$p_i=P(X=x_i),i=1,2,…,n.$</p><p>当熵和条件熵中的概率由数据估计得到时，所对应的熵和条件熵分别称为<em>经验熵</em>、<em>经验条件熵</em>。此时，如果有0概率，令$0log0=0$。<em>信息增益表示得知特征X的信息而使得类Y的信息的不确定性缺少的程度。</em></p><blockquote><p>定义5.2（信息增益）特征$A$对训练数据集D的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即$$g(D,A)=H(D)-H(D|A)$$一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p></blockquote><p>决策树学习应用信息增益准则选择特征。给定训练数据集D和特征A，经验熵$H(D)$表示对数据集$D$进行分类的不确定性。而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集D的不确定性。那么它们的差，即信息增益，就<strong>表示由于特征A而使得对数据集D的分类的不确定性减少的程度</strong>。显然，对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。</p><p>设训练数据为$D$，$|D|$表示其样本容量，即样本个数。设有$K$个类$C_k$，$k=1,2,…,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^{K}{|C_k|=|D|}$。设特征$A$有$n$个不同的取值${a_1,a_2,…,a_n}$，根据特征A的取值将D划分为n个子集$D_1,D_2,…,D_n$，$|D_i|$为$D_i$的样本数量，$\sum_{i=1}^{n}{|D_i|=|D|}$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik}=D_i\cap C_k$，$D_{ik}$为$D_{ik}$的样本个数。于是信息增益的算法如下：</p><blockquote><p>算法5.1（信息增益的算法）<br>输入：训练数据$D$和特征$A$；<br>输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$<br>(1)计算数据集D的经验熵$H(D)$$$H(D)=-\sum_{k=1}^{K}{\frac{|C_k|}{|D|} log_2{\frac{C_k}{D}}}$$<br>(2)计算特征A对数据集D的经验条件熵$H(D|A)$$$H(D|A)=\sum_{i=1}^{n}{\frac{|D_i|}{|D|}H(D)}=-\sum_{i=1}^{n}{\frac{|D_{ik}|}{|D_i|}log_2\frac{|C_k|}{|D|}}$$<br>(3)计算信息熵$$g(D,A)=H(D)-H(D|A)$$</p></blockquote><h4 id="信息增益算法的实现"><a href="#信息增益算法的实现" class="headerlink" title="信息增益算法的实现"></a>信息增益算法的实现</h4><p>代码：<br><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">import numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="meta"># 数据来源于P59页表5.1</span></span><br><span class="line"><span class="function">def <span class="title">get_data</span>(<span class="params"></span>):</span></span><br><span class="line"><span class="function">    dataSet </span>= [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],  <span class="meta"># 数据集</span></span><br><span class="line">               [<span class="meta">0, 0, 0, 1, 0</span>],</span><br><span class="line">               [<span class="meta">0, 1, 0, 1, 1</span>],</span><br><span class="line">               [<span class="meta">0, 1, 1, 0, 1</span>],</span><br><span class="line">               [<span class="meta">0, 0, 0, 0, 0</span>],</span><br><span class="line">               [<span class="meta">1, 0, 0, 0, 0</span>],</span><br><span class="line">               [<span class="meta">1, 0, 0, 1, 0</span>],</span><br><span class="line">               [<span class="meta">1, 1, 1, 1, 1</span>],</span><br><span class="line">               [<span class="meta">1, 0, 1, 2, 1</span>],</span><br><span class="line">               [<span class="meta">1, 0, 1, 2, 1</span>],</span><br><span class="line">               [<span class="meta">2, 0, 1, 2, 1</span>],</span><br><span class="line">               [<span class="meta">2, 0, 1, 1, 1</span>],</span><br><span class="line">               [<span class="meta">2, 1, 0, 1, 1</span>],</span><br><span class="line">               [<span class="meta">2, 1, 0, 2, 1</span>],</span><br><span class="line">               [<span class="meta">2, 0, 0, 0, 0</span>]]</span><br><span class="line">    X=np.zeros([len(dataSet),<span class="number">4</span>])</span><br><span class="line">    Y=np.zeros(len(dataSet))</span><br><span class="line">    <span class="function"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="title">range</span>(<span class="params">len(dataSet</span>)):</span></span><br><span class="line"><span class="function">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="title">range</span>(<span class="params"><span class="number">4</span></span>):</span></span><br><span class="line"><span class="function">            X[i][t]</span>=dataSet[i][t]</span><br><span class="line">        Y[i]=dataSet[i][<span class="number">4</span>]</span><br><span class="line">    <span class="keyword">return</span> X,Y</span><br><span class="line"></span><br><span class="line"><span class="meta"># 求解经验熵H(x)</span></span><br><span class="line"><span class="function">def <span class="title">empirical_entropy</span>(<span class="params">x</span>):</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    x_value_list </span>= <span class="keyword">set</span>([x[i] <span class="function"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="title">range</span>(<span class="params">x.shape[<span class="number">0</span>]</span>)])</span></span><br><span class="line"><span class="function">    ent </span>= <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> x_value <span class="keyword">in</span> x_value_list:</span><br><span class="line">        p = <span class="keyword">float</span>(x[x == x_value].shape[<span class="number">0</span>]) / x.shape[<span class="number">0</span>]</span><br><span class="line">        logp = np.log2(p)</span><br><span class="line">        ent -= p * logp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ent</span><br><span class="line"></span><br><span class="line"><span class="meta"># 求解经验条件熵H(y|x)</span></span><br><span class="line"><span class="function">def <span class="title">empirical_conditional_entropy</span>(<span class="params">x, y</span>):</span></span><br><span class="line"><span class="function">    x_value_list </span>= <span class="keyword">set</span>([x[i] <span class="function"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="title">range</span>(<span class="params">x.shape[<span class="number">0</span>]</span>)])</span></span><br><span class="line"><span class="function">    # <span class="title">print</span>(<span class="params">x_value_list</span>)</span></span><br><span class="line"><span class="function">    ent </span>= <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> x_value <span class="keyword">in</span> x_value_list:</span><br><span class="line">        sub_y = y[x == x_value]</span><br><span class="line">        temp_ent = empirical_entropy(sub_y)</span><br><span class="line">        ent += (<span class="keyword">float</span>(sub_y.shape[<span class="number">0</span>]) / y.shape[<span class="number">0</span>]) * temp_ent</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ent</span><br><span class="line"></span><br><span class="line"><span class="meta"># 求解信息增益</span></span><br><span class="line"><span class="function">def <span class="title">information_gain</span>(<span class="params">x,y</span>):</span></span><br><span class="line"><span class="function">    base_ent </span>= empirical_entropy(y)</span><br><span class="line">    condition_ent = empirical_conditional_entropy(x, y)</span><br><span class="line">    ent_grap = base_ent - condition_ent</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ent_grap</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    X,Y=get_data()</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"X"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    print(X)</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"Y"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    print(Y)</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"信息增益"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="title">range</span>(<span class="params">X[<span class="number">0</span>].shape[<span class="number">0</span>]</span>):</span></span><br><span class="line"><span class="function">        <span class="title">print</span>(<span class="params"><span class="string">"g(D,A_"</span>+str(i</span>)+"): %.3f " % (<span class="params">information_gain(X[:,i],Y</span>)))</span></span><br></pre></td></tr></table></figure></p><p>结果：<br><img src="/2018/10/19/决策树的原理及实现/QQ20181024-133943.png" alt="信息增益算法结果"></p><h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h3><p>信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比可以怼这一问题进行校正。这是特征选择的另一准则。</p><blockquote><p>定义5.3（信息增益比）特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D的经验熵$H(D)$之比：$$g_R(D,A)=\frac{g(D,A)}{H(D)}$$</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> python </tag>
            
            <tag> 决策树 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>朴素贝叶斯法原理及实现</title>
      <link href="/2018/10/17/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"/>
      <url>/2018/10/17/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对于给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。</p><h2 id="朴素贝叶斯法的学习与分类"><a href="#朴素贝叶斯法的学习与分类" class="headerlink" title="朴素贝叶斯法的学习与分类"></a>朴素贝叶斯法的学习与分类</h2><h3 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h3><p>设输入空间$\mathcal{X}\subseteq R_n$为n维向量的集合，输出空间为类标记集合$\mathcal{Y}=\{c_1,c_2,…,c_k\}$.输入为特征向量$x\in \mathcal{X}$，输出为类标记$y\in \mathcal{Y}$.X是定义在输入空间$\mathcal{X}$上的随机向量，Y是定义在输出空间$\mathcal{Y}$上的随机变量。$P(X,Y)$是X和Y的联合概率分布。训练数据集$$T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$$由<em>$P(X,Y)$独立同分布</em>产生。</p><p>朴素贝叶斯法通过训练数据集学习联合概率分布$P(X,Y)$。具体地，学习以下<em>先验概率分布</em>及<em>条件概率分布</em>。先验概率分布$$P(Y=c_k)，k=1,2,…,K$$条件概率分布（后验概率分布）$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)}|Y=c_k)$$于是学习到联合概率分布$P(X,Y)$.</p><p>条件概率分布$P(X=x|Y=c_k)$有指数级数量的参数，其估计实际是不可行的。事实上，假设$x^{(j)}$可取值有$S_j$个，$j=1,2,…,n$，Y可取值有K个，那么参数个数为$$K\prod_{j=1}^{n}S_j$$</p><p>朴素贝叶斯法对条件概率分布作了<em>条件独立性</em>的假设。由于这是一个较强的假设，朴素贝叶斯法也由此得名。朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型。条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。</p><p>朴素贝叶斯法分类的基本公式：$$P(Y=c_k|X=x)=\frac{P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_k P(Y=c_k) \prod_j (X^{(j)}=x^{(j)}|Y=c_k)}$$</p><p>朴素贝叶斯法分类器可表示为$$y=f(x)=arg \max_{c_k} P(Y=c_k) \prod_j P(X^{(j)}=x^{(j)}|Y=c_k)$$</p><h2 id="朴素贝叶斯法的参数估计"><a href="#朴素贝叶斯法的参数估计" class="headerlink" title="朴素贝叶斯法的参数估计"></a>朴素贝叶斯法的参数估计</h2><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>在朴素贝叶斯法中，学习意味着估计$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$.可以应用极大似然估计法估计相应的概率。<em>先验概率</em>$P(Y=c_k)$的极大似然估计是$$P(Y=c_k)=\frac {\sum_{i=1}^{N} I(y_i=c_k)} {N}，k=1,2,…,K$$设第$j$个特征$x^{(j)}$可能取值的集合为$\{a_{j1},a_{j2},…,a_{jS_j}\}$，条件概率$P(X^{j}=a_{jl}||Y=c_k)$的极大似然估计是$$P(x^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N} I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)}\\j=1,2,…,n;l=1,2,…,S_j;k=1,2,…,K$$式中，$x_i^{(j)}$是第$i$个样本的第$j$个特征；$a_{jl}$是第$j$个特征可能取的第$l$个值：$I$为指示函数。</p><h3 id="学习与分类算法"><a href="#学习与分类算法" class="headerlink" title="学习与分类算法"></a>学习与分类算法</h3><blockquote><p>(朴素贝叶斯算法)<br>输入：训练数据$T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$，其中$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})$，$x_i^{(j)}$是第$i$个样本的第$j$个特征，$x_i^{(j)}\in \{a_{j1},a_{j2},…,a_{jS_j}\}$，$a_{jl}$是第$j$个特征可能取的第$l$个值，$j=1,2,…,n$，$l=1,2,…,S_j$，$y_i\in \{c1,c2,…,c_k\}$；实例$x$；<br>输出：实例$x$的分类</p><ol><li>计算先验概率及条件概率$$P(Y=c_k)=\frac {\sum_{i=1}^{N} I(y_i=c_k)}{N},k=1,2,…,K\\P(X^{(j)}=a_{jl}|Y=c_k)=\frac {\sum_{i=1}^{(j)}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)}\\j=1,2,…,n；l=1,2,…,S_j；k=1,2,…,K$$</li><li>对于给定的实例$x=(x^{(1)},x^{(2)},…,x^{(n)})^T$，计算$$P(Y=c_k)\prod_{j=1}^{n}P(X^{(j)}=X^{(j)}|Y=c_k),k=1,2,…,K$$</li><li>确定实例$x$的类$$y=arg \max_{c_k} P(Y=c_k)\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$</li></ol></blockquote><h2 id="朴素贝叶斯算法在digits数据集上的实现"><a href="#朴素贝叶斯算法在digits数据集上的实现" class="headerlink" title="朴素贝叶斯算法在digits数据集上的实现"></a>朴素贝叶斯算法在digits数据集上的实现</h2><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_digits</span><span class="params">()</span></span><span class="symbol">:</span></span><br><span class="line">    digits = load_digits()</span><br><span class="line">    data = np.zeros([len(digits.data), len(digits.data[<span class="number">0</span>])])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(digits.data))<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(len(digits.data[<span class="number">0</span>]))<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">if</span> digits.data[i][t] &gt; <span class="number">0</span><span class="symbol">:</span></span><br><span class="line">                data[i][t] = <span class="number">1</span></span><br><span class="line">            <span class="symbol">else:</span></span><br><span class="line">                data[i][t] = <span class="number">0</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(data,np.array(digits.target) , test_size=<span class="number">0</span>.<span class="number">25</span>,random_state=<span class="number">33</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Naive_Bayes</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(<span class="keyword">self</span>,X,Y)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.P_Yc=np.zeros(<span class="number">10</span>)<span class="comment">#保存P(Y=c_k)</span></span><br><span class="line">        <span class="keyword">self</span>.I_Yc=np.zeros(<span class="number">10</span>)<span class="comment">#保存I(Y_i=c_k)</span></span><br><span class="line">        <span class="keyword">self</span>.I_XaYc=np.zeros((len(X[<span class="number">0</span>]),len([<span class="number">0</span>,<span class="number">1</span>]),<span class="number">10</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Y))<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>.I_Yc[Y[i]]+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(len(X[<span class="number">0</span>]))<span class="symbol">:</span></span><br><span class="line">                <span class="keyword">self</span>.I_XaYc[t][ int(X[i][t]) ][Y[i]]+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">self</span>.P_Yc=[i/sum(<span class="keyword">self</span>.I_Yc) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">self</span>.I_Yc]</span><br><span class="line">        <span class="comment"># 第一项特征的数量，特征值的种类数量，分类的种类</span></span><br><span class="line">        <span class="keyword">self</span>.P_XaYc=np.zeros((len(X[<span class="number">0</span>]),len([<span class="number">0</span>,<span class="number">1</span>]),<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X[<span class="number">0</span>]))<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(len([<span class="number">0</span>,<span class="number">1</span>]))<span class="symbol">:</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>)<span class="symbol">:</span></span><br><span class="line">                    <span class="keyword">self</span>.P_XaYc[i][t][j]=<span class="keyword">self</span>.I_XaYc[i][t][j]/<span class="keyword">self</span>.I_Yc[j]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(<span class="keyword">self</span>,x)</span></span><span class="symbol">:</span></span><br><span class="line">        score=np.zeros(<span class="number">10</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)<span class="symbol">:</span></span><br><span class="line">            a=<span class="keyword">self</span>.P_Yc[i]</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(len(x))<span class="symbol">:</span></span><br><span class="line">                a*=<span class="keyword">self</span>.P_XaYc[t][int(x[t])][i]</span><br><span class="line">            score[i]=a</span><br><span class="line">        <span class="keyword">return</span> score.argmax()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name_<span class="number">_</span> == <span class="string">'__main__'</span><span class="symbol">:</span></span><br><span class="line">    <span class="comment"># print(get_digits())</span></span><br><span class="line">    X_train, X_test, Y_train, y_test= get_digits()</span><br><span class="line">    nb=Naive_Bayes()</span><br><span class="line">    nb.train(X_train,Y_train)</span><br><span class="line">    t_num=<span class="number">0</span></span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"预测过程"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y_test))<span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> nb.predict(X_test[i])==y_test[i]<span class="symbol">:</span></span><br><span class="line">            t_num+=<span class="number">1</span></span><br><span class="line">        print(<span class="string">"case"</span>,i,<span class="string">"，预测值："</span>,nb.predict(X_test[i]),<span class="string">"，真实值："</span>,y_test[i],<span class="string">"预测结果:"</span>,nb.predict(X_test[i])==y_test[i])</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"预测结果"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"准确率："</span>,t_num/len(y_test))</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/2018/10/17/朴素贝叶斯法原理及实现/QQ20181018-080645.png" alt="朴素贝叶斯法预测结果"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> python </tag>
            
            <tag> 朴素贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>K近邻算法的原理及实现</title>
      <link href="/2018/10/16/K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"/>
      <url>/2018/10/16/K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在学习并实现完了之前的感知机算法，感觉它是比较简单的，并且只能进行处理线性分类问题。<br><br>本章节学习了K近邻算法（k-nearest neighbor,k-NN），它是一种基本分类与回归方法，在这一章节中，不介绍它在回归问题中的应用，仅仅介绍其在分类问题中的应用。K近邻算法的输入为实例的特征向量，对应于特征空间的点，输出为实例的类别。</p><h2 id="k近邻算法"><a href="#k近邻算法" class="headerlink" title="k近邻算法"></a>k近邻算法</h2><p>通俗的说：给定一个数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类（多数表决投票）。</p><blockquote><p>输入：训练数据集$T=\{(x_1,y_1),(x_1,y_2),…,(x_N,y_N)\}$其中，$x_i\in \mathcal{X}\subseteq R^n$为实例的特征向量，$y_i\in \mathcal{Y}={c_1,c_2,…,c_k}$为实例的类别，$i=1,2,…,N$；实例特征向量$x$；<br>输出：在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$:<br><br>$$y=\mathop{\arg\max}_{c_j} \sum_{x_i\in N_k (x)} I(y_i=c_j),i=1,2,…,N;j=1,2,…,K$$<br>I为指示函数，即当$y_i=c_i$时I为1，否则I为0.</p></blockquote><p>当k近邻法的k=1时，就是特殊情形，也就是最近邻算法，对于输入的实例点（特征向量）$x$，最近邻法将训练数据集中与$x$最邻近点的类作为$x$的类。<em>k近邻算法没有显式的学习过程。</em></p><h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>如《统计学习方法》概论总结中所说，距离的度量方法有很多，特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间一般是$n$维实数向量空间$R^n$。使用的距离是欧氏距离，但也可以是其他距离，如更一般的$L_p$距离或Minkowski距离。这里就不再赘述了。</p><h3 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h3><p>k值的选择会对k近邻法的结果产生重大影响。</p><p>如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有于输入实例相近的训练实例才会对预测结果起作用。但是缺点是“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k值的减小就意味着整体模型变得复杂，容易发生过拟合。</p><p>如果选择较大的k值，就相当于用较大领域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的训练实例也会对预测起作用，使预测发生错误。k值的增大就意味着整体的模型变得简单。</p><p>在应用中，k值一般选择一个比较小的数值。通常采用交叉验证法来选取最优的k值。</p><h3 id="分类决策规则"><a href="#分类决策规则" class="headerlink" title="分类决策规则"></a>分类决策规则</h3><p>k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。<br>多数表决规则有如下解释：如果分类的损失函数为0-1损失函数，分类函数为$$f:R^n \to \{c_1,c_2,…,c_k\}$$<br>那么误分类的概率是<br>$$P(Y\ne f(X))=1-P(Y=f(X))$$<br>对给定的实例$x\in X$，其最近邻的k个训练实例点构成集合$N_k(x)$。如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是<br>$$\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \ne c_j)=1- \frac{1}{k} \sum_{x_i\in N_(x)} I(y_i=c_j)$$<br>要使误分类绿最小即经验风险最小，就要使$\sum_{x_i\in N_k(x)} I(y_i=c_j)$最大，所以多数表决规则等于经验风险最小化。</p><h2 id="KNN的实现"><a href="#KNN的实现" class="headerlink" title="KNN的实现"></a>KNN的实现</h2><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 仅使用sklearn中的iris数据集，以及归一化、数据拆分函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getIrisData</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    data=load_iris()</span><br><span class="line">    X=np.array(data.data)</span><br><span class="line">    Y=np.array(data.target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># X数据归一化处理</span></span><br><span class="line">    min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class="line">    X = min_max_scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据拆分，测试数据占比0.3</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = <span class="number">0.2</span>, random_state = <span class="number">33</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">cell</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        self.x=x</span><br><span class="line">        self.y=y</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># k值设定，可以根据需要更改</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,k=<span class="number">11</span>)</span>:</span></span><br><span class="line">        self.k=k</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 向当前knn模型中注入原始数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,X,Y)</span>:</span></span><br><span class="line">        self.size=len(X[<span class="number">0</span>])</span><br><span class="line">        self.cells=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Y)):</span><br><span class="line">            self.cells.append(cell(X[i],Y[i]))</span><br><span class="line">        <span class="comment"># print(self.cells)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用模型进行预测</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="comment"># 创建投票列表</span></span><br><span class="line">        Nk=sorted(self.cells,key=<span class="keyword">lambda</span> cell:np.linalg.norm(cell.x-x))[:self.k] <span class="comment">#使用的np.linalg.norm函数在默认情况下求解的为范数为2的范式</span></span><br><span class="line">        <span class="comment"># 多数表决投票</span></span><br><span class="line">        ans=np.zeros(<span class="number">4</span>,float)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> Nk:</span><br><span class="line">            ans[i.y]+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print("*" * 10, "置信度", "*" * 10)</span></span><br><span class="line">        ans=ans/self.k</span><br><span class="line">        <span class="comment"># print(ans)</span></span><br><span class="line">        <span class="keyword">return</span> ans.argmax(),ans</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    X_train, X_test, y_train, y_test=getIrisData()</span><br><span class="line">    knn=KNN()</span><br><span class="line">    knn.train(X_train,y_train)</span><br><span class="line">    print(<span class="string">"*"</span> * <span class="number">10</span>, <span class="string">"预测"</span>, <span class="string">"*"</span> * <span class="number">10</span>)</span><br><span class="line">    t_num=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y_test)):</span><br><span class="line">        p_y,ans=knn.predict(X_test[i])</span><br><span class="line">        <span class="keyword">if</span> p_y==y_test[i]:</span><br><span class="line">            t_num+=<span class="number">1</span></span><br><span class="line">        print(<span class="string">"各参数:"</span>,X_test[i],<span class="string">"真实类:"</span>,y_test[i],<span class="string">"预测类:"</span>,p_y,<span class="string">"是否正确:"</span>,p_y==y_test[i],<span class="string">"置信度:"</span>,ans)</span><br><span class="line">    print(<span class="string">"*"</span> * <span class="number">10</span>, <span class="string">"结果"</span>, <span class="string">"*"</span> * <span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"iris数据集中，交叉验证正确率：&#123;:2f&#125;"</span>.format(t_num/len(y_test)))</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/2018/10/16/K近邻算法的原理及实现/QQ20181017-133128.png" alt="KNN在iris数据集中的效果"></p><h2 id="kd树寻找最邻近算法"><a href="#kd树寻找最邻近算法" class="headerlink" title="kd树寻找最邻近算法"></a>kd树寻找最邻近算法</h2><h3 id="构造算法"><a href="#构造算法" class="headerlink" title="构造算法"></a>构造算法</h3><blockquote><p>构造平衡kd树算法：<br>输入：k维空间数据集$T=\{x_1,x_2,…,x_N\}$，<br>其中$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(k)})^T$,$i=1,2,…,N;$<br>输出：kd树<br>（1）开始：构造根节点，根节点对应于包括T的k维空间的超矩形区域。<br>选择$x^{(1)}$为坐标轴，以T中所有实例$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。<br>由根结点生成深度为1的左、右子结点：左子结点对应坐标$x^{(1)}$小与切分点的子区域，右子结点对于坐标$x^{(1)}$大于切分点的子区域。<br>将落在切分超平面上的实例点保存在根结点。<br>（2）重复：对深度为j的结点，选择$x^{(l)}$为切分的坐标轴，$l=j(mod k)+1$，以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(l)}$垂直的超平面实现。<br>由该结点生成深度为$j+1$的左、右子结点：左子结点对应坐标$x^{(l)}$小与切分点的子区域，右子结点对应坐标$x^{(l)}$大于切分点的子区域。<br>将落在切分超平面上的实例点保存在该结点。<br>（3）直到两个子区域没有实例存在时停止，从而形成kd树的区域划分。</p></blockquote><h3 id="搜索算法"><a href="#搜索算法" class="headerlink" title="搜索算法"></a>搜索算法</h3><blockquote><p>用kd树的<em>最邻近</em>搜索：<br><br>输入：已够早的kd树：目标点x<br>输出：x的最近邻<br>(1)在kd树种找出包含目标点x的叶结点：从根结点出发，递归地向下访问kd树，若目标点x当前维的坐标小与切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。<br>(2)以此叶结点为“当前最近点”<br>(3)递归地向上回退，在每个结点进行以下操作：</p><blockquote><p>(a)如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”<br>(b)当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与目标点为球心，以目标点与“当前最近点”间的距离为半径的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点。移动到另一个子结点，接着，递归地进行最邻近搜索。如果不相交，向上回退。</p></blockquote></blockquote><blockquote><p>(4)当回退到根结点时，搜索结束，最后的“当前最近点”即为x的最邻近点。</p></blockquote><h2 id="kd树查找最近邻的实现"><a href="#kd树查找最近邻的实现" class="headerlink" title="kd树查找最近邻的实现"></a>kd树查找最近邻的实现</h2><h3 id="code-1"><a href="#code-1" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造kd树，P41</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KDnode</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dome_point,split_num,left,right)</span>:</span></span><br><span class="line">        self.dome_point=dome_point  <span class="comment">#此次样本点</span></span><br><span class="line">        self.split_num=split_num    <span class="comment">#切割的维度号</span></span><br><span class="line">        self.left=left              <span class="comment">#左子树</span></span><br><span class="line">        self.right=right            <span class="comment">#右子树</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KDtree</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,data)</span>:</span></span><br><span class="line">        k=len(data[<span class="number">0</span>])              <span class="comment">#k维度</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">CreateNode</span><span class="params">(split_num,data_set)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> data_set :       <span class="comment">#data_set为空返回空</span></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">            data_set.sort(key=<span class="keyword">lambda</span> x : x[split_num])  <span class="comment">#按照我们想要切割的维度号进行排序</span></span><br><span class="line">            split_pos=len(data_set)//<span class="number">2</span>                  <span class="comment">#从中间将其分割开，获得中位点标号</span></span><br><span class="line">            middle_pos=data_set[split_pos]              <span class="comment">#找到中位点</span></span><br><span class="line">            split_next=(split_num+<span class="number">1</span>)%k                  <span class="comment">#下一个需要分割的维度</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> KDnode(middle_pos,split_num,</span><br><span class="line">                          CreateNode(split_next,data_set[:split_pos]),</span><br><span class="line">                          CreateNode(split_next,data_set[split_pos+<span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line">        self.root=CreateNode(<span class="number">0</span>,data)    <span class="comment">#创造树</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 寻找最近点，参考学习了https://www.cnblogs.com/21207-iHome/p/6084670.html</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(tree, point)</span>:</span></span><br><span class="line">    k = len(point)  <span class="comment"># 数据维度</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">travel</span><span class="params">(kd_node, target, max_dist)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> kd_node <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="number">0</span>] * k, float(<span class="string">"inf"</span>), <span class="number">0</span>  <span class="comment"># python中用float("inf")和float("-inf")表示正负无穷</span></span><br><span class="line">                                            <span class="comment"># return的参数分别是最近坐标点、最近距离和访问过的节点数</span></span><br><span class="line"></span><br><span class="line">        nodes_visited = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        s = kd_node.split_num  <span class="comment"># 进行分割的维度</span></span><br><span class="line">        pivot = kd_node.dome_point  <span class="comment"># 进行分割的“轴”</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> target[s] &lt;= pivot[s]:  <span class="comment"># 如果目标点第s维小于分割轴的对应值(目标离左子树更近)</span></span><br><span class="line">            nearer_node = kd_node.left  <span class="comment"># 下一个访问节点为左子树根节点</span></span><br><span class="line">            further_node = kd_node.right  <span class="comment"># 同时记录下右子树</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 目标离右子树更近</span></span><br><span class="line">            nearer_node = kd_node.right  <span class="comment"># 下一个访问节点为右子树根节点</span></span><br><span class="line">            further_node = kd_node.left</span><br><span class="line"></span><br><span class="line">        temp1 = travel(nearer_node, target, max_dist)  <span class="comment"># 进行遍历找到包含目标点的区域</span></span><br><span class="line"></span><br><span class="line">        nearest = temp1[<span class="number">0</span>]  <span class="comment"># 以此叶结点作为“当前最近点”</span></span><br><span class="line">        dist = temp1[<span class="number">1</span>]  <span class="comment"># 更新最近距离</span></span><br><span class="line"></span><br><span class="line">        nodes_visited += temp1[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dist &lt; max_dist:</span><br><span class="line">            max_dist = dist  <span class="comment"># 最近点将在以目标点为球心，max_dist为半径的超球体内</span></span><br><span class="line"></span><br><span class="line">        temp_dist = abs(pivot[s] - target[s])  <span class="comment"># 第s维上目标点与分割超平面的距离</span></span><br><span class="line">        <span class="keyword">if</span> max_dist &lt; temp_dist:  <span class="comment"># 判断超球体是否与超平面相交</span></span><br><span class="line">            <span class="keyword">return</span> nearest, dist, nodes_visited  <span class="comment"># 不相交则可以直接返回，不用继续判断</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># 计算目标点与分割点的欧氏距离</span></span><br><span class="line">        temp_dist=np.linalg.norm((np.array(pivot)-np.array(target)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> temp_dist &lt; dist:  <span class="comment"># 如果“更近”</span></span><br><span class="line">            nearest = pivot  <span class="comment"># 更新最近点</span></span><br><span class="line">            dist = temp_dist  <span class="comment"># 更新最近距离</span></span><br><span class="line">            max_dist = dist  <span class="comment"># 更新超球体半径</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查另一个子结点对应的区域是否有更近的点</span></span><br><span class="line">        temp2 = travel(further_node, target, max_dist)</span><br><span class="line"></span><br><span class="line">        nodes_visited += temp2[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> temp2[<span class="number">1</span>] &lt; dist:  <span class="comment"># 如果另一个子结点内存在更近距离</span></span><br><span class="line">            nearest = temp2[<span class="number">0</span>]  <span class="comment"># 更新最近点</span></span><br><span class="line">            dist = temp2[<span class="number">1</span>]  <span class="comment"># 更新最近距离</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nearest, dist, nodes_visited</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> travel(tree.root, point, float(<span class="string">"inf"</span>))  <span class="comment"># 从根节点开始递归</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#先left后right，前序遍历</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preorder</span><span class="params">(root)</span>:</span></span><br><span class="line">    print(root.dome_point)</span><br><span class="line">    <span class="keyword">if</span> root.left:</span><br><span class="line">        preorder(root.left)</span><br><span class="line">    <span class="keyword">if</span> root.right:</span><br><span class="line">        preorder(root.right)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    data=[[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">5</span>,<span class="number">4</span>],[<span class="number">9</span>,<span class="number">6</span>],[<span class="number">4</span>,<span class="number">7</span>],[<span class="number">8</span>,<span class="number">1</span>],[<span class="number">7</span>,<span class="number">2</span>]]     <span class="comment">#书上P42例子</span></span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"data"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    print(data)</span><br><span class="line">    kd=KDtree(data)</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"先序遍历"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    preorder(kd.root)</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"search"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    print(search(kd,[<span class="number">5</span>,<span class="number">6</span>]))</span><br></pre></td></tr></table></figure><h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><p><img src="/2018/10/16/K近邻算法的原理及实现/QQ20181017-133411.png" alt="kd树寻找最邻近点的结果"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> python </tag>
            
            <tag> K近邻 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hexo中文章的两种创建方法及加密</title>
      <link href="/2018/10/15/Hexo%E4%B8%AD%E6%96%87%E7%AB%A0%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%88%9B%E5%BB%BA%E6%96%B9%E6%B3%95%E5%8F%8A%E5%8A%A0%E5%AF%86/"/>
      <url>/2018/10/15/Hexo%E4%B8%AD%E6%96%87%E7%AB%A0%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%88%9B%E5%BB%BA%E6%96%B9%E6%B3%95%E5%8F%8A%E5%8A%A0%E5%AF%86/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在每次写笔记的时候，一天肯定是写不完的，但是又不希望直接<code>clean g d</code>了，因此想使用草稿的功能，鸣谢<a href="https://blog.csdn.net/wizardforcel/article/details/40684575" target="_blank" rel="noopener">https://blog.csdn.net/wizardforcel/article/details/40684575</a>找到了使用草稿功能的方法。</p><h2 id="写文章的步骤"><a href="#写文章的步骤" class="headerlink" title="写文章的步骤"></a>写文章的步骤</h2><p>回顾一下正常写文章的步骤：<br>首先创建一个名叫<code>name</code>的文章<br><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="keyword">new</span> <span class="string">"name"</span></span><br></pre></td></tr></table></figure></p><p>接着系统会提示文章保存在<code>/sources/_post</code>里面，然后书写文章，直至完毕。<br>然后就是同步预览文章和提交github pages的时间了。<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo clean</span></span><br><span class="line"><span class="attribute">hexo g</span></span><br><span class="line"><span class="attribute">hexo d</span></span><br></pre></td></tr></table></figure></p><p>此刻就完成了写文章的标准步骤。</p><h2 id="写草稿的步骤"><a href="#写草稿的步骤" class="headerlink" title="写草稿的步骤"></a>写草稿的步骤</h2><p>接下来讲写草稿的步骤：<br>首先创建一个草稿<br><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="keyword">new</span> <span class="type">draft</span> <span class="string">"new draft"</span></span><br></pre></td></tr></table></figure></p><p>会在<code>source/_drafts</code>目录下生成一个<code>new-draft.md</code>文件。但是这个文件不被显示在页面上，链接也访问不到。也就是说如果你想把某一篇文章移除显示，又不舍得删除，可以把它移动到<code>_drafts</code>目录之中。<br>如果你希望强行预览草稿，更改配置文件：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">render_drafts:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>或者，如下方式启动server：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo<span class="built_in"> server </span>--drafts</span><br></pre></td></tr></table></figure></p><p>下面这条命令可以把草稿变成文章，或者页面：<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo publish <span class="string">[layout]</span> &lt;filename&gt;</span><br></pre></td></tr></table></figure></p><h2 id="加密文章的方法"><a href="#加密文章的方法" class="headerlink" title="加密文章的方法"></a>加密文章的方法</h2><p>我们所书写的一些文章，总会因为一些原因不想让大家看到，比如：文章未完成、涉及个人隐私等等。因此想要将它加密一下。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>在此，我们使用的是<code>hexo-blog-encrypt</code>，并且得在<code>2.0</code>版本以上,然后执行<code>npm install</code>命令。然后该插件会自动安装。</p><h3 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h3><h4 id="1、首先在-config-yml中启用该插件"><a href="#1、首先在-config-yml中启用该插件" class="headerlink" title="1、首先在_config.yml中启用该插件:"></a>1、首先在<code>_config.yml</code>中启用该插件:</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Security</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="attr">encrypt:</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h4 id="2、在你的文章的头部添加上对应的字段，如password-message"><a href="#2、在你的文章的头部添加上对应的字段，如password-message" class="headerlink" title="2、在你的文章的头部添加上对应的字段，如password,message"></a>2、在你的文章的头部添加上对应的字段，如<code>password</code>,<code>message</code></h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">加密测试</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="bullet">-10</span><span class="bullet">-15</span> <span class="number">18</span><span class="string">:48:35</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">加密测试</span></span><br><span class="line"><span class="attr">password:</span> <span class="string">passwd</span></span><br><span class="line"><span class="attr">message:</span> <span class="string">欢迎来到我的博客，由于一些原因，请输入密码查看。</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure><p>如果要取消加密，在<code>password</code>之前用<code>#</code>注释掉即可。</p><h4 id="3、加密效果"><a href="#3、加密效果" class="headerlink" title="3、加密效果"></a>3、加密效果</h4><p><img src="/2018/10/15/Hexo中文章的两种创建方法及加密/QQ20181015-191701.png" alt="加密效果展示"></p>]]></content>
      
      
      <categories>
          
          <category> 技术技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>更改hexo中的Mathjax引擎</title>
      <link href="/2018/10/14/%E6%9B%B4%E6%94%B9hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E7%9A%84Mathjax%E5%BC%95%E6%93%8E/"/>
      <url>/2018/10/14/%E6%9B%B4%E6%94%B9hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E7%9A%84Mathjax%E5%BC%95%E6%93%8E/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>搭建的博客在使用Math引擎构建数学公式的时候遇到了几个问题。</p><ul><li>显示公式出错，显示的是语法错误</li><li>不能使用<code>$...$</code>行内公式</li><li>显示的公式十分丑</li></ul><p>在网上也看到了很多的答案，但是都不是很靠谱啊。真的是个大坑啊啊啊，看<a href="https://docs.mathjax.org/en/latest/mathjax.html" target="_blank" rel="noopener">官方文档</a>看了好久，然后又去找的Hexo的实现方法。emmm，索性解决了。</p><h3 id="找到自己模板配置"><a href="#找到自己模板配置" class="headerlink" title="找到自己模板配置"></a>找到自己模板配置</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd themes<span class="regexp">/maupassant/</span></span><br><span class="line"><span class="comment"># 找到head的配置文件，我的模板中head等的配置文件是在这里，其他的可以自己找找</span></span><br><span class="line">cd layout<span class="regexp">/_partial/</span></span><br></pre></td></tr></table></figure><p>layout/_partial/下就是该模板的配置信息了，然后看一下是否有mathjax.pug（部分模板是html格式或其他格式，无所谓），如果有的话，选择方案一，如果没有，选择方案二。</p><p>优先推荐方案一，因为这样可以只在使用mathjax的页面进行渲染，速度更加快。</p><h2 id="解决方案一"><a href="#解决方案一" class="headerlink" title="解决方案一"></a>解决方案一</h2><h3 id="只需要修改mathjax-pug"><a href="#只需要修改mathjax-pug" class="headerlink" title="只需要修改mathjax.pug"></a>只需要修改mathjax.pug</h3><p>在某些模板中，有着自己的mathjax.pug，那么只需要将js相应部分改为以下的pug代码即可。</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">script(type=<span class="string">"text/x-mathjax-config"</span>).</span><br><span class="line">    <span class="symbol">MathJax</span>.<span class="symbol">Hub</span>.<span class="symbol">Config</span>(&#123;</span><br><span class="line">    extensions: [<span class="string">"tex2jax.js"</span>],</span><br><span class="line">    jax: [<span class="string">"input/TeX"</span>, <span class="string">"output/HTML-CSS"</span>],</span><br><span class="line">    tex2jax: &#123;</span><br><span class="line">        inlineMath: [ [<span class="string">'$'</span>,<span class="string">'$'</span>], [<span class="string">"\\("</span>,<span class="string">"\\)"</span>] ],</span><br><span class="line">        displayMath: [ [<span class="string">'$$'</span>,<span class="string">'$$'</span>], [<span class="string">"\\["</span>,<span class="string">"\\]"</span>] ],</span><br><span class="line">        processEscapes: true</span><br><span class="line">    &#125;,</span><br><span class="line">        <span class="string">"HTML-CSS"</span>: &#123; fonts: [<span class="string">"TeX"</span>] &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">script(type=<span class="string">"text/javascript"</span> src=<span class="string">"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"</span>)</span><br></pre></td></tr></table></figure><h2 id="解决方案二"><a href="#解决方案二" class="headerlink" title="解决方案二"></a>解决方案二</h2><p>若mathjax不是单独配置的话，按照以下步骤进行。</p><h3 id="关闭hexo本身的mathjax"><a href="#关闭hexo本身的mathjax" class="headerlink" title="关闭hexo本身的mathjax"></a>关闭hexo本身的mathjax</h3><p>关闭hexo本身的mathjax，也就是在HEXO目录下的_config.yml，改为<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mathjax:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></p><p>然后关闭相应文章的<code>front-matter</code>中的<code>mathjax: true</code></p><h3 id="修改主题模板"><a href="#修改主题模板" class="headerlink" title="修改主题模板"></a>修改主题模板</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 修改<span class="selector-tag">head</span><span class="selector-class">.pug</span>，为了让全局页面中加入<span class="selector-tag">mathjax</span></span><br><span class="line"><span class="selector-tag">vim</span> <span class="selector-tag">head</span><span class="selector-class">.pug</span></span><br></pre></td></tr></table></figure><p>然后就在其中加入我们需要的mathjax<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">script(type=<span class="string">"text/x-mathjax-config"</span>).</span><br><span class="line">    <span class="symbol">MathJax</span>.<span class="symbol">Hub</span>.<span class="symbol">Config</span>(&#123;</span><br><span class="line">    extensions: [<span class="string">"tex2jax.js"</span>],</span><br><span class="line">    jax: [<span class="string">"input/TeX"</span>, <span class="string">"output/HTML-CSS"</span>],</span><br><span class="line">    tex2jax: &#123;</span><br><span class="line">        inlineMath: [ [<span class="string">'$'</span>,<span class="string">'$'</span>], [<span class="string">"\\("</span>,<span class="string">"\\)"</span>] ],</span><br><span class="line">        displayMath: [ [<span class="string">'$$'</span>,<span class="string">'$$'</span>], [<span class="string">"\\["</span>,<span class="string">"\\]"</span>] ],</span><br><span class="line">        processEscapes: true</span><br><span class="line">    &#125;,</span><br><span class="line">        <span class="string">"HTML-CSS"</span>: &#123; fonts: [<span class="string">"TeX"</span>] &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">script(type=<span class="string">"text/javascript"</span> src=<span class="string">"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"</span>)</span><br></pre></td></tr></table></figure></p><p>因为pug格式和html相似，因此如果格式是html的话，可以自己更改一下。</p><h2 id="修改前后对比"><a href="#修改前后对比" class="headerlink" title="修改前后对比"></a>修改前后对比</h2><h3 id="修改前"><a href="#修改前" class="headerlink" title="修改前"></a>修改前</h3><p><img src="/2018/10/14/更改hexo博客中的Mathjax引擎/E7217D6653FB277C412019A200AE3D19.png" alt="修改前的0-1损失函数" title="修改前的0-1损失函数"></p><h3 id="修改后"><a href="#修改后" class="headerlink" title="修改后"></a>修改后</h3><p>$$L(Y,f(x))=\begin{cases}<br>    1,&amp; Y \ne f(X)\\<br>    0,&amp; Y = f(X)<br>\end{cases}$$</p><p>$$<br>\begin{matrix}<br>    1 &amp; x &amp; x^2 \\<br>    1 &amp; y &amp; y^2 \\<br>    1 &amp; z &amp; z^2 \\<br>\end{matrix}<br>$$</p><blockquote><p>PS:在该版本中，换行符是<code>\\\</code>，而不是两个<code>\</code>，请在测试的时候注意一下。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mac配置ssh免密登录腾讯云服务器</title>
      <link href="/2018/10/13/mac%E9%85%8D%E7%BD%AEssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
      <url>/2018/10/13/mac%E9%85%8D%E7%BD%AEssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我在前一段时间，斥巨资购买了一年的云服务器，但是一直落着吃灰，今天忽然想起来折腾一下。</p><p><br></p><h2 id="ssh直连"><a href="#ssh直连" class="headerlink" title="ssh直连"></a>ssh直连</h2><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -i ~/.ssh/<span class="string">[密钥名称]</span> <span class="string">[账户]</span>@<span class="string">[ip地址]</span></span><br></pre></td></tr></table></figure><p><img src="/2018/10/13/mac配置ssh免密登录腾讯云服务器/QQ20181013-165116.png" alt="连接上的效果" title="连接上的效果"></p><p>可以看到我们已经连接上了腾讯云服务器，但是是不是每次都需要输入很长的ip地址，这就很烦了，因此接下来我们设置一下hosts，用代号替换ip地址。</p><h2 id="设置hosts"><a href="#设置hosts" class="headerlink" title="设置hosts"></a>设置hosts</h2><p>以下以mac osx系统为例<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim <span class="regexp">/etc/</span>hosts</span><br></pre></td></tr></table></figure></p><p>然后直接在最后加上你的云服务器ip地址和想要表示的名字。<br>或者用下面的语句直接添加：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"[ip地址]  [替代名]"</span></span></span><br></pre></td></tr></table></figure></p><p><img src="/2018/10/13/mac配置ssh免密登录腾讯云服务器/QQ20181013-165910.png" alt="修改后的样子" title="修改后的样子"></p><h3 id="设置免密登录"><a href="#设置免密登录" class="headerlink" title="设置免密登录"></a>设置免密登录</h3><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>cd .ssh</span><br><span class="line"><span class="variable">$ </span>ssh-keygen</span><br><span class="line"><span class="variable">$ </span>ssh-copy-id ubuntu<span class="variable">@t_cloud</span></span><br></pre></td></tr></table></figure><p>首先进入.ssh目录，生成密钥，然后将公钥传给云服务器。注意：在腾讯云服务器中，默认的用户是ubuntu，而不是root。</p><p><img src="/2018/10/13/mac配置ssh免密登录腾讯云服务器/QQ20181013-170408.png" alt="密钥传送完毕" title="密钥传送完毕"></p><p>最后尝试免密登录吧</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ssh</span> ubuntu<span class="variable">@t_cloud</span></span><br></pre></td></tr></table></figure><p><img src="/2018/10/13/mac配置ssh免密登录腾讯云服务器/QQ20181013-170625.png" alt="免密登录成功" title="免密登录成功"></p><p>这样就免密登录成功了，再也不需要繁琐的输入密码了！</p>]]></content>
      
      
      <categories>
          
          <category> 技术技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssh配置 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>感知机的原理及实现</title>
      <link href="/2018/10/10/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"/>
      <url>/2018/10/10/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>经过第一章漫长的学习，终于进入到了算法的学习，也很激动地开始了对感知机的学习。</p><h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><h3 id="感知机的定义"><a href="#感知机的定义" class="headerlink" title="感知机的定义"></a>感知机的定义</h3><blockquote><p>假设输入空间（特征空间）是$\mathcal{X} \subseteq R^n$，输出空间是$\mathcal{Y}=\{+1,-1\}$，输入$x\in\mathcal{X}$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y\in\mathcal{Y}$表示实例的类别，由输入空间到输出空间的如下函数：<br>$$f(x)=sign(w\bullet x+b)$$<br>称为感知机，其中，w和b为感知机模型参数，$w\in R^n$叫作权值（weight）或权值向量（weight vector），$b\in R$叫作偏置（biss），$w\bullet x$表示w和x的内积，sign表示符号函数，即：<br>$$sign(x)=\begin{cases}<br> +1,x\ge 0 \\<br>-1,x&lt;0\end{cases}$$<br>感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型或线性分类器，即函数集合$ \{ f|f(x)=w\bullet x+b=0 \} $</p></blockquote><p>对于特征空间$R^n$中的一个超平面$S$，其中w是超平面的法向量，$b$是超平面的截距。这个超平面将特征空间划分为两个部分，位于两部分的点（特征向量）分别被分为正、负两类，因此超平面S被称为分离超平面。如下图所示:</p><p><img src="/2018/10/10/感知机的原理及实现/感知机模型.png" alt="感知机模型"></p><h2 id="感知机学习策略"><a href="#感知机学习策略" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h2><h3 id="数据集的线性可分性"><a href="#数据集的线性可分性" class="headerlink" title="数据集的线性可分性"></a>数据集的线性可分性</h3><blockquote><p>定义(数据集的线性可分性)：给定一个数据集$$T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$$<br>其中$x_i\in \mathcal{X}=R^n$,$y_i\in \mathcal{Y}=\{+1,-1\}$，$i=1,2,…,N$，如果存在某个超平面$S$$$w\bullet x_i +b &lt;0$$能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i=+1$的实例$i$，有$w\bullet x_i+b&gt;0$，对所有$y_i=1$的实例$i$，有$w\bullet x_i+b&lt;0$，则称数据集T为线性可分数据集，否则称数据集T线性不可分。</p></blockquote><p>在我的理解中，问题的关键在于存不存在一个超平面将所有的数据集按照正例负例分割开来，若存在，则线性可分；不存在，则线性不可分。</p><h3 id="感知机学习策略-1"><a href="#感知机学习策略-1" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h3><p>如果训练数据集是线性可分的，感知机学习的目标是找到能够将正、负实例点完全正确分开的<code>分离超平面</code>，为了找到这样的超平面，我们也就要找到感知机的参数，因此需要确定一个学习策略，即定义（经验）损失函数并将损失函数极小化。</p><p>损失函数的选择有很多，最简单的一个就是<code>误分点的个数</code>，但是这样的损失函数并不是参数w、b的连续可导函数，不容易优化。另一个选择则是<code>误分类点到超平面S的总距离</code>，这个是感知机所采用的。<br>输入空间$R^n$中任一点$x_0$到超平面S的距离：<br>$$\frac {1}{||w||} |w\bullet x_0+b|，其中||w||是w的L_2范数$$<br>其次，对于误分类的数据$(x_i,y_i)$，来说$$-y_i(w\bullet x_i+b)&gt;0$$成立,也意味着预测得到值和真实值符号相反。<br>这样，假设超平面S的误分类点集合为M，那么所有误分类点到超平面S的总距离为：$$-\frac{1}{||w||} \sum_{x_i\in M}{y_i(w\bullet x_i+b)}$$不考虑$\frac{1}{||w||}$，就得到感知机学习的损失函数.<br>$$L(w,b)=-\sum_{x_i\in M}{y_i(w\bullet x_i+b)}$$<br>其中，$M$是误分类点的集合，这个损失函数就是感知机学习的经验风险函数。<br>可以看出，损失函数$L(w,b)$是非负的，如果没有误分类点，损失函数值是0.而误分类点越少，误分类点距离超平面也就越近，损失函数值就越小。一个特定的样本点的损失函数：在误分类时是参数$w,b$的线性函数，在正确分类时为0，因此在给定训练数据T，损失函数$L(w,b)$是$w，b$的连续可导函数。</p><h2 id="感知机学习算法"><a href="#感知机学习算法" class="headerlink" title="感知机学习算法"></a>感知机学习算法</h2><h3 id="感知机学习算法的原始形式"><a href="#感知机学习算法的原始形式" class="headerlink" title="感知机学习算法的原始形式"></a>感知机学习算法的原始形式</h3><p>感知机算法其实就是求解损失函数极小化问题的解$$\min_{w,b} L(w,b)=-\sum_{x_i\in M} y_i(w\bullet x_i +b)$$<br>感知机学习算法是由误分类驱动的，具体采用随机梯度下降法，过程是首先，任意选取一个超平面$w_0,b_0$，然后用梯度下降法不断地极小化目标函数，一次随机选取一个误分类点使其梯度下降。<br>假设误分类点集合M是固定的，那么损失函数$L(w,b)$的梯度如下：</p><p>$$\nabla_w L(w,b)=-\sum_{x_i\in M} y_ix_i\\<br>\nabla_b L(w,b)=-\sum_{x_i\in M} y_i$$</p><p>随机选取一个误分类点$(x_i,y_i)$，对$w,b$进行更新：</p><p>$$w \gets w+\eta y_ix_i \\<br>b \gets b+\eta y_i$$</p><p>其中，式子中的$\eta$是步长，也就是我们熟知的学习率(learning rate)。</p><h3 id="感知机学习算法的原始形式算法"><a href="#感知机学习算法的原始形式算法" class="headerlink" title="感知机学习算法的原始形式算法"></a>感知机学习算法的原始形式算法</h3><blockquote><p>输入：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i \in \mathcal{X} = R^n$，$y_i \in \mathcal{Y}=\{-1,+1\}$，$i=1,2,…,N$，学习率$\eta(0&lt;\eta\le 1);$<br>输出：$w,b$，感知机模型$f(x)=sign(w\bullet x+b)$.</p><ol><li>选取初值$w_0,b_0$</li><li>在训练集中选取数据$(x_i,y_i)$</li><li>如果$y_i(w\bullet x_i+b)\le 0$ $$w \gets w+\eta y_ix_i$$$$b \gets b+\eta y_i$$</li><li>转至2步骤，直至训练集中没有误分类点。</li></ol></blockquote><p>这种学习方法在直观上有如下解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$w,b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该分类点使其被正确分类。</p><h3 id="感知机学习算法的原始形式的实现"><a href="#感知机学习算法的原始形式的实现" class="headerlink" title="感知机学习算法的原始形式的实现"></a>感知机学习算法的原始形式的实现</h3><h4 id="code"><a href="#code" class="headerlink" title="code"></a>code</h4><p>根据上述的原始形式算法，直接构建算法模型，数据是通过公式创造的，然后用于分类，实现如下：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">import <span class="built_in">random</span></span><br><span class="line">import numpy as <span class="built_in">np</span></span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"># 创建数据集</span><br><span class="line">def createData(t_w,t_b,num_size):</span><br><span class="line">    <span class="built_in">random</span>.seed(<span class="number">78</span>)</span><br><span class="line">    x,y=[],[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_size):</span><br><span class="line">        x.<span class="built_in">append</span>([<span class="built_in">random</span>.uniform(<span class="number">0</span>,<span class="number">10</span>),<span class="built_in">random</span>.uniform(<span class="number">0</span>,<span class="number">10</span>)])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">        sy=i[<span class="number">0</span>]*t_w[<span class="number">0</span>]+i[<span class="number">1</span>]*t_w[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> sy&gt;t_b:</span><br><span class="line">            y.<span class="built_in">append</span>(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y.<span class="built_in">append</span>(-<span class="number">1</span>)</span><br><span class="line">    x=<span class="built_in">np</span>.<span class="built_in">array</span>(x)</span><br><span class="line">    y=<span class="built_in">np</span>.<span class="built_in">array</span>(y)</span><br><span class="line">    <span class="built_in">return</span> x,y</span><br><span class="line"></span><br><span class="line">#训练过程</span><br><span class="line">def train(x,y,learning_rate):</span><br><span class="line">    w=<span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">    b=<span class="number">0</span></span><br><span class="line">    finish_flag=False</span><br><span class="line">    <span class="built_in">num</span>=<span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="keyword">not</span> finish_flag):</span><br><span class="line">        <span class="built_in">num</span>+=<span class="number">1</span></span><br><span class="line">        finish_flag=True</span><br><span class="line">        f_num=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len(y)):</span><br><span class="line">            <span class="keyword">if</span> y[i]*(<span class="built_in">np</span>.dot(w,x[i])+b)&lt;=<span class="number">0</span>:</span><br><span class="line">                w=w+learning_rate*y[i]*x[i]</span><br><span class="line">                b=b+learning_rate*y[i]</span><br><span class="line">                finish_flag=False</span><br><span class="line">                f_num+=<span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Traversing the &#123;&#125; times ,w= [&#123;:.2f&#125;,&#123;:.2f&#125;] b= &#123;:.2f&#125; accuracy= &#123;:.2f&#125;."</span>.format(<span class="built_in">num</span>,w[<span class="number">0</span>],w[<span class="number">1</span>],b,(<span class="number">10</span>-f_num)/<span class="number">10</span>))</span><br><span class="line">    <span class="built_in">return</span> w,b</span><br><span class="line"></span><br><span class="line">#预测出来的model</span><br><span class="line">def f(w,b):</span><br><span class="line">    x1 = <span class="built_in">np</span>.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">0.1</span>)</span><br><span class="line">    <span class="built_in">return</span> x1,[-(w[<span class="number">0</span>]*i+b)/w[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> x1]</span><br><span class="line"></span><br><span class="line">#图像展示</span><br><span class="line">def <span class="built_in">show</span>(x,y,w,b):</span><br><span class="line">    x1=<span class="built_in">np</span>.dot(x,[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">    x2=<span class="built_in">np</span>.dot(x,[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="built_in">color</span>=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">np</span>.<span class="built_in">array</span>([i==-<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> y]):</span><br><span class="line">        <span class="keyword">if</span> i:</span><br><span class="line">            <span class="built_in">color</span>.<span class="built_in">append</span>(<span class="number">255</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">color</span>.<span class="built_in">append</span>(<span class="number">0</span>)</span><br><span class="line">    plt.scatter(x1,x2,c=<span class="built_in">color</span>)</span><br><span class="line">    xx,yy=f(w,b)</span><br><span class="line">    plt.plot(xx,yy)</span><br><span class="line">    plt.<span class="built_in">xlabel</span>(<span class="string">"x0"</span>)</span><br><span class="line">    plt.<span class="built_in">ylabel</span>(<span class="string">"x1"</span>)</span><br><span class="line">    plt.xlim(<span class="number">0</span>,<span class="number">10</span>)</span><br><span class="line">    plt.ylim(<span class="number">0</span>,<span class="number">10</span>)</span><br><span class="line">    plt.<span class="built_in">title</span>(<span class="string">"Realization of perceptron"</span>)</span><br><span class="line">    plt.<span class="built_in">show</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == '__main__':</span><br><span class="line">    t_w=<span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">3</span>,-<span class="number">2</span>])    #学习方便，仅设计两个参数，两个特征值</span><br><span class="line">    t_b=<span class="number">4.3</span>                 #生成值的b</span><br><span class="line">    num_size=<span class="number">80</span>             #创造的训练数据量</span><br><span class="line">    learning_rate=<span class="number">0.1</span></span><br><span class="line">    x,y=createData(t_w,t_b,num_size)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"train data"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"&#123;&#125;*w0+&#123;&#125;*w1+b &gt;= 0? &#123;&#125;"</span>.format(x[i][<span class="number">0</span>],x[i][<span class="number">1</span>],y[i]))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"Answer"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    w,b=train(x,y,learning_rate)</span><br><span class="line">    <span class="built_in">show</span>(x,y,w,b)</span><br></pre></td></tr></table></figure></p><h4 id="实现结果"><a href="#实现结果" class="headerlink" title="实现结果"></a>实现结果</h4><p><img src="/2018/10/10/感知机的原理及实现/Figure_1.png" alt="感知机学习算法的原始形式的结果图"><br><img src="/2018/10/10/感知机的原理及实现/QQ20181016-090136.png" alt="感知机学习算法的原始形式的预测性能与结果"></p><h3 id="感知机学习算法的对偶形式"><a href="#感知机学习算法的对偶形式" class="headerlink" title="感知机学习算法的对偶形式"></a>感知机学习算法的对偶形式</h3><blockquote><p>输入：线性可分的数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i \in R^n$，$x_i \in R^n,y_i \in \{-1,+1\},i=1,2,…,N$，$学习率\eta(0&lt;\eta\le 1)$<br>输出：$\alpha,b$；感知机模型$f(x)=sign(\sum_{j=1}^{N}\alpha_j y_j x_j+b)$，其中$\alpha = (\alpha_1,\alpha_2,…,\alpha_N)^T.$</p><ol><li>$\alpha\gets 0,b\gets 0$</li><li>在训练集中选取数据$(x_i,y_i)$</li><li>如果$y_i\lgroup\sum_{j=1} ^N{\alpha_j y_j x_i +b}\rgroup\le 0$ $$\alpha_i \gets \alpha_i+\eta$$ $$b\gets b+\eta y_i$$</li><li>转至2直到没有误分类数据</li></ol><p>对偶形式中训练实例仅为内积的形式出现，为了方便，可以预先将训练集中实例的内积计算出来并用矩阵的形式存储，这个矩阵就是所谓的Gram矩阵$$G=[x_i\bullet x_j]_{N\times N}$$</p></blockquote><h3 id="感知机学习算法的对偶形式的实现"><a href="#感知机学习算法的对偶形式的实现" class="headerlink" title="感知机学习算法的对偶形式的实现"></a>感知机学习算法的对偶形式的实现</h3><h4 id="code-1"><a href="#code-1" class="headerlink" title="code"></a>code</h4><p>与原始形式不同的地方在于实例中的内积是通过求解Gram矩阵获得，详细代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Author   : yechenchen</span></span><br><span class="line"><span class="comment"># @Time     : 2018/10/9 下午5:19</span></span><br><span class="line"><span class="comment"># @File     : 感知机学习算法的对偶形式.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm Community Edition</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#感知机学习算法的对偶形式，P33</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">np.random.seed(<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CreateData</span><span class="params">(h_size,w_size)</span>:</span></span><br><span class="line">    x=np.random.rand(h_size,w_size)</span><br><span class="line">    <span class="keyword">return</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#给予对应的真实值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getY</span><span class="params">(X,feature_num)</span>:</span></span><br><span class="line">    w=np.random.rand(feature_num)</span><br><span class="line">    b=<span class="number">-0.2</span></span><br><span class="line">    print(w,b)</span><br><span class="line">    Y=np.array([])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> X:</span><br><span class="line">        <span class="keyword">if</span> np.dot(w,i)+b&gt;<span class="number">0</span>:</span><br><span class="line">            Y=np.append(Y,[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y=np.append(Y,[<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建Gram矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getG</span><span class="params">(X)</span>:</span></span><br><span class="line">    G=np.zeros((len(X),len(X)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(len(X)):</span><br><span class="line">            G[i][t]=np.dot(X[i],X[t])</span><br><span class="line">    <span class="keyword">return</span> G</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练过程，默认学习率为0.1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X,Y,G,feature_num,learning_rate=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    a=np.zeros(len(Y))</span><br><span class="line">    b=<span class="number">0</span></span><br><span class="line">    finish_flag=<span class="keyword">False</span></span><br><span class="line">    num=<span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> finish_flag:</span><br><span class="line">        finish_flag=<span class="keyword">True</span></span><br><span class="line">        num+=<span class="number">1</span></span><br><span class="line">        f_num=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(G)):</span><br><span class="line">            tmp=<span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(Y)):</span><br><span class="line">                tmp+=(a[j]*Y[j]*G[i][j])</span><br><span class="line">            ans=Y[i]*(tmp+b)</span><br><span class="line">            <span class="keyword">if</span> ans&lt;=<span class="number">0</span>:</span><br><span class="line">                a[i]+=<span class="number">1</span>*learning_rate   <span class="comment">#更新a值</span></span><br><span class="line">                b+=Y[i]*learning_rate   <span class="comment">#更新b值</span></span><br><span class="line">                finish_flag=<span class="keyword">False</span></span><br><span class="line">                f_num+=<span class="number">1</span></span><br><span class="line">        print(<span class="string">"Traversing the &#123;&#125; times , b= &#123;:.2f&#125; accuracy= &#123;:.2f&#125;."</span>.format(num,b,(len(Y)-f_num)/len(Y)))</span><br><span class="line">    w=np.zeros(feature_num)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        w+=a[i]*X[i]*Y[i]</span><br><span class="line">    <span class="keyword">return</span> w,b</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建预测到的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(w,b)</span>:</span></span><br><span class="line">    x0 = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> x0,[-(w[<span class="number">0</span>]*i+b)/w[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> x0]</span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化显示得到的函数以及散点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(x,y,w,b)</span>:</span></span><br><span class="line">    x1=np.dot(x,[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">    x2=np.dot(x,[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    color=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.array([i==<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> y]):</span><br><span class="line">        <span class="keyword">if</span> i:</span><br><span class="line">            color.append(<span class="number">255</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            color.append(<span class="number">0</span>)</span><br><span class="line">    plt.scatter(x1,x2,c=color)</span><br><span class="line">    xx,yy=f(w,b)</span><br><span class="line">    plt.plot(xx,yy)</span><br><span class="line">    plt.xlabel(<span class="string">"x0"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"x1"</span>)</span><br><span class="line">    plt.xlim(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    plt.ylim(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">"Realization of perceptron"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#主函数</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    feature_num=<span class="number">2</span>   <span class="comment">#每个样本具备的特征值的数量，设为2时可以可视化输出，否则需要用PCA降维</span></span><br><span class="line">    data_size=<span class="number">80</span>    <span class="comment">#训练数据量</span></span><br><span class="line">    X=CreateData(data_size,feature_num)    <span class="comment">#每组数据五个特征值，共80组数据</span></span><br><span class="line">    Y=getY(X,feature_num)</span><br><span class="line">    print(<span class="string">"*"</span>*<span class="number">10</span>,<span class="string">"train data"</span>,<span class="string">"*"</span>*<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(data_size):</span><br><span class="line">        print(X[i],Y[i])</span><br><span class="line">    print(<span class="string">"*"</span> * <span class="number">10</span>, <span class="string">"train"</span>, <span class="string">"*"</span> * <span class="number">10</span>)</span><br><span class="line">    G=getG(X)</span><br><span class="line">    w,b=train(X,Y,G,feature_num)</span><br><span class="line">    print(<span class="string">"*"</span> * <span class="number">10</span>, <span class="string">"ans"</span>, <span class="string">"*"</span> * <span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"w=&#123;&#125;,b=&#123;&#125;"</span>.format(w,b))</span><br><span class="line">    show(X,Y,w,b)</span><br></pre></td></tr></table></figure></p><h4 id="实现结果-1"><a href="#实现结果-1" class="headerlink" title="实现结果"></a>实现结果</h4><p><img src="/2018/10/10/感知机的原理及实现/Figure_2.png" alt="感知机学习算法的对偶形式的结果图"><br><img src="/2018/10/10/感知机的原理及实现/QQ20181016-092327.png" alt="感知机学习算法的原始形式的预测性能与结果"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> 感知机 </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>《统计学习方法》概论整理</title>
      <link href="/2018/10/04/2018-10-04-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%A6%82%E8%AE%BA%E6%95%B4%E7%90%86/"/>
      <url>/2018/10/04/2018-10-04-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%A6%82%E8%AE%BA%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="前记"><a href="#前记" class="headerlink" title="前记"></a>前记</h2><p>在导师的推荐下，本人开始阅读李航博士的<code>《统计学习方法》</code>，该书是以内容短小精悍著称，因此我也从此书开始正式进入了机器学习的学习阶段。当然在这一节中，本人只是对该章节知识点进行梳理，由于多是基本知识点，所以量很大，内容也很多。</p><h2 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h2><h3 id="统计学习的概念"><a href="#统计学习的概念" class="headerlink" title="统计学习的概念"></a>统计学习的概念</h3><p>统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行<code>预测与分析</code>的一门学科，统计学习也称为统计机器学习。<br></p><h3 id="统计学习的对象"><a href="#统计学习的对象" class="headerlink" title="统计学习的对象"></a>统计学习的对象</h3><p>统计学习的对象是<code>数据</code>，它从数据出发，提取数据的特征，抽象出数据的模型，从数据中发现知识，又回到对数据的分析与预测中去。</p><h3 id="统计学习的方法"><a href="#统计学习的方法" class="headerlink" title="统计学习的方法"></a>统计学习的方法</h3><p>统计学习由监督学习(supervised learning)、非监督学习(unsupervised learning)、半监督学习(semi-supervised learning)、强化学习(reinforcement learning)等组成。</p><h3 id="统计学习中的常用概念"><a href="#统计学习中的常用概念" class="headerlink" title="统计学习中的常用概念"></a>统计学习中的常用概念</h3><ul><li>我们认为要学习的模型属于某个函数的集合，该集合被称为<code>假设空间</code>。<br></li><li>训练数据、测试数据：意如其名。<br></li><li>统计学习方法的三要素为<code>模型(model)</code>、<code>策略(strategy)</code>、<code>算法(algorithm)</code>。</li></ul><h3 id="实现统计学习方法的步骤"><a href="#实现统计学习方法的步骤" class="headerlink" title="实现统计学习方法的步骤"></a>实现统计学习方法的步骤</h3><blockquote><p>(1) 得到一个有限的训练数据集合；<br><br>(2) 确定包含所有可能的模型的假设空间，即学习的模型的集合；<br><br>(3) 确定模型选择的准则，即学习的策略；<br><br>(4) 实现求解最优模型的算法，即学习的算法；<br><br>(5) 通过学习方法选择最优模型；<br><br>(6) 利用学习的最优模型对新数据进行预测或分析；</p></blockquote><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><h3 id="监督学习的概念"><a href="#监督学习的概念" class="headerlink" title="监督学习的概念"></a>监督学习的概念</h3><p>监督学习的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测（注意，这里的输入、输出是指某个系统的输入与输出，与学习的输入与输出不同）<br></p><p>本人认为，”<code>这里的输入、输出</code>“指的不是特征值，而仅仅只是指代系统需要输入输出样本。</p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li><strong>输入空间、特征空间与输出空间</strong><br><br>首先，输入空间与输出空间指的是输入输出所有的可能取值的集合。输入与输出空间可以是有限元素的集合，也可以是整个欧式空间，两者可以是指同一个空间，也可以是不同的空间，通常情况下，输出空间远远小于输入空间。<br><br>每个具体的输入是一个实例，通常由特征向量来表示，此时所有的特征向量存在的空间称为特征空间。特征空间的每一维对应于一个特征。</li><li><strong>联合概率分布</strong><br><br>监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)，P(X,Y)表示分布函数，或者称分布密度函数。</li><li><strong>假设空间</strong><br><br>模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。假设空间的确定意味着学习范围的确定。</li></ul><h2 id="统计学习三要素"><a href="#统计学习三要素" class="headerlink" title="统计学习三要素"></a>统计学习三要素</h2><p>$$方法=模型+策略+算法$$</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>在监督学习中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。<br><br>假设空间用$\mathcal{F}$表示，假设空间可以定义为决策函数的集合：<br></p><p>$$ \mathcal{F}=\{f|Y=f(X)\} $$</p><p>其中，X和Y是定义在输入空间$\mathcal{X}$和输出空间$\mathcal{Y}$上的变量，这时$\mathcal{F}$通常是由一个参数向量决定的函数族：<br></p><p>$$\mathcal{F}=\{ f|Y=f_\theta(X),\theta\in R^n \}$$</p><p>参数向量$\theta$取值与n维欧式空间$R^n$，称为参数空间。<br></p><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>统计学习的<code>目标</code>在于从假设空间中选取<strong>最优模型</strong>。首先需要引入损失函数与风险函数的概念，损失函数度量模型<code>一次预测</code>的好坏，风险函数度量<code>平均意义下</code>的模型预测的好坏。两者的区别就在于一次预测和平均意义。</p><h4 id="损失函数和风险函数（重要）"><a href="#损失函数和风险函数（重要）" class="headerlink" title="损失函数和风险函数（重要）"></a><strong>损失函数和风险函数（重要）</strong></h4><p>监督学习问题是在假设空间$\mathcal{F}$中选取模型$f$作为决策函数，对于给定的输入$X$,由$f(X)$给出相应的输出$Y$，这个输出的预测值$f(X)$与真实值$Y$可能一致也可能不一致，因此需要一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度，损失函数是$f(X)$和$Y$的非负实值函数，记作为$L(Y,f(X))$.<br>统计学习常用的损失函数有以下几种：</p><ul><li><p>0-1损失函数（0-1 loss function）<br></p><p>  $$L(Y,f(x))=\begin{cases}<br>  1,&amp; Y \ne f(X)\\<br>  0,&amp; Y = f(X)<br>  \end{cases}$$</p></li><li><p>平方损失函数（quadratic loss function）</p><p>$$L(Y,f(X))=(Y-f(X))^2$$</p></li><li><p>绝对损失函数（absolute loss function）</p><p>$$L(Y,f(X))=|Y-f(X)|$$</p></li><li><p>对数损失函数（logarithmic loss function）或对数似然损失函数（log-likelihood loss function）</p><p>$$L(Y,P(Y|X))=-log(P(Y|X))$$</p></li></ul><p>损失函数值越小，模型就越好，由于模型的输入、输出是随机变量，遵循联合分布$P(X,Y)$所以损失函数的期望是:<br></p><p>$$R_{exp}(f)=E_p[L(Y,f(X))]= \lmoustache_{x \times y}L(y,f(x))P(x,y)dxdy$$</p><p>这就是理论上模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数或期望损失。<br>由于联合分布是未知的，因此$R_exp(f)$不能直接计算，如果知道了联合分布那也就不需要学习了，一方面根据期望风险最小来学习模型要用到联合分布，而联合分布却是未知的，因此我们需要引入一个<code>经验风险</code>的概念，其实模型f(x)关于训练数据集的平均损失，记作$R_{emp}$</p><p>$$R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))   \qquad$$</p><p>注：上式子中，N代表着训练集中的样本数量</p><h4 id="经验风险最小化与结构风险最小化"><a href="#经验风险最小化与结构风险最小化" class="headerlink" title="经验风险最小化与结构风险最小化"></a><strong>经验风险最小化与结构风险最小化</strong></h4><p>在经验风险最小化的策略中，我们需要的是求解上述经验风险的最小化，就是求解最优化问题：<br></p><p>$$min_{f\in \mathcal{F}}  \frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\qquad$$</p><p>当样本容量足够大的时候，经验风险最小化有着很好的效果，因此被广泛采用。<br>当样本容量很小的时候，经验风险最小化学习的效果就未必很好，可能还会有过拟合的情况发生。结构风险最小化就是为了防止过拟合而提出来的策略，结构风险最小化等价于<strong>正则化</strong>，结构风险是在经验风险上加了表示模型复杂度的正则化项（或者称为罚项），在假设空间、损失函数、训练数据集确定的时候，<strong>结构风险</strong>的定义是</p><p>$$R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)$$</p><ul><li>$J(f)$为模型的复杂度，是定义在假设空间$\mathcal{F}$上的反函数，模型$f$越复杂，复杂度$J(f)$就越大。</li><li>$\lambda$是系数，泳衣权衡经验风险和模型复杂度。</li></ul><p>结构风险最小化策略认为结构风险最小的模型是最优的模型，所以最优模型，就是求解最优化的问题:<br></p><p>$$min_{f\in \mathcal{F}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)$$</p><p>在这时，监督学习问题就变成了经验风险或结构风险函数的最优化问题。</p><h4 id="公式整理"><a href="#公式整理" class="headerlink" title="公式整理"></a><strong>公式整理</strong></h4><table><thead><tr><th>风险名称</th><th>损失函数</th><th>最优化</th></tr></thead><tbody><tr><td>期望风险</td><td>$R_{exp}(f)=E_p[L(Y,f(X))]= \lmoustache_{x \times y}L(y,f(x))P(x,y)dxdy$</td><td>不可求</td></tr><tr><td>经验风险</td><td>$ R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))   \qquad $</td><td>$min_{f\in \mathcal{F}}R_{emp}(f)  $</td></tr><tr><td>结构风险</td><td>$R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)$</td><td>$min_{f\in \mathcal{F}}R_{srm}(f)  $</td></tr></tbody></table><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>算法是指学习模型的具体计算方法。按照以上所讲，统计学习问题归结为最优化问题，统计学习的算法称为求解最优化问题的算法，通过数值计算的方法求解，来保证找到全局最优解。</p><p>统计学习方法之间的不同，主要来自其模型、策略、算法的不同，若三者确定，统计学习的方法也就随之确定，因此称其为统计学习的三要素。</p><h2 id="模型评估与模型选择"><a href="#模型评估与模型选择" class="headerlink" title="模型评估与模型选择"></a>模型评估与模型选择</h2><h3 id="训练误差与测试误差"><a href="#训练误差与测试误差" class="headerlink" title="训练误差与测试误差"></a>训练误差与测试误差</h3><p>当损失函数给定时，基于损失函数的模型的训练误差和模型的测试误差就自然成为学习方法评估的标准，注意，统计学习方法具体采用的损失函数未必是评估时使用的损失函数，当然，让两者一致是比较理想的。</p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>如果一味地追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高，这种情况呗称为过拟合。过拟合是指学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但是对未知数据预测得很差的现象。</p><h2 id="正则化与交叉验证"><a href="#正则化与交叉验证" class="headerlink" title="正则化与交叉验证"></a>正则化与交叉验证</h2><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>正则化是结构风险最小化策略的实现，是在经验风险上加上正则化项和罚项。正则化项一般是模型复杂度的<code>单调递增函数</code>，也就是模型越复杂，正则化值越大，LOSS值也就越大。</p><p>正则化的一般形式：</p><p>$$min_{f\in \mathcal{F}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)$$</p><p>正则化项有着不同的形式，一般是与参数向量有关，经常使用范数来表示。</p><h3 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h3><p>这里就需要引入一个范数的概念了。</p><p>L0范数：实际上表示的为向量中非零元素的个数</p><p>$$L0(x)= ^0 \sqrt{\sum_{i=1}^n|x_i|^0}$$</p><ul><li>L1范数：又被称为曼哈顿距离、最小绝对误差等</li></ul><p>$$L1(X)=\sum_{i=1}^n|x_i|$$</p><ul><li>L2范数：又被称为欧式距离，是用的最多的距离度量</li></ul><p>$$L2(X)=\sqrt{\sum_{i=1}^n|x_i|^2}$$</p><p>范数在numpy中有具体的实现方法：<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下列语句可以实现求解a向量与b向量之间的距离</span></span><br><span class="line">np.linalg.norm(<span class="keyword">a</span>-b)</span><br></pre></td></tr></table></figure></p><h2 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h2><h3 id="生成方法"><a href="#生成方法" class="headerlink" title="生成方法"></a>生成方法</h3><p>生成方法是由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(X|Y)$作为预测的模型。这就是生成模型：</p><p>$$P(X|Y)=\frac{P(X,Y)}{P(X)}$$</p><p>该方法之所以称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。</p><h3 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h3><p>判别方法是数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。判别模型关心的是对给定的输入，应该预测什么样的输出。</p><h3 id="生成方法和判别方法的特点比较"><a href="#生成方法和判别方法的特点比较" class="headerlink" title="生成方法和判别方法的特点比较"></a>生成方法和判别方法的特点比较</h3><p>生成方法的特点：</p><ul><li>生成方法可以还原出联合概率分布$P(X,Y)$，而判别方法不能；</li><li>生成方法的学习收敛速度更快，模型可以更快地收敛于真实模型</li><li>当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。</li></ul><p>判别方法的特点：</p><ul><li>判别方法直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$</li><li>直接面对预测，往往学习的准确率更高。</li><li>由于直接学习$P(Y|X)或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</li></ul><h2 id="针对分类问题的指标（重要）"><a href="#针对分类问题的指标（重要）" class="headerlink" title="针对分类问题的指标（重要）"></a>针对分类问题的指标（重要）</h2><p>对二分类问题常用的评价指标是精准率和召回率。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4中情况出现的总数分别记作：</p><ul><li>TP——将正类预测为正类的数量</li><li>FN——将正类预测为负类的数量</li><li>FP——将负类预测为正类的数量</li><li>TN——将负类预测为负类的数量</li></ul><p>精准率的定义为$P=\frac{TP}{TP+FP}$，可以记为判断为正类的样本中的真实正类的比重。</p><p>召回率的定义为$R=\frac{TP}{TP+FN}$，可以记为真实的正类被判定为正类的比重。</p><p>$F_1$值，是精确率和召回率的调和均值，即$\frac{2}{F_1}=\frac{1}{P}\frac{1}{R}$，$F_1=\frac{2TP}{2TP+FP+FN}$，因此可以看出，当精准率和召回率都高的时候，$F1$值才会高。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本章节主要讲的都是统计学习中的一些基本概念，这份整理也花了不少的时间，应该理解深刻。从下一章开始，进入了十个算法的学习，并且每章都会在理论的基础上配上代码实现，尽请期待了。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>网络实践课程设计——五子棋</title>
      <link href="/2018/09/18/2018-09-18-%E7%BD%91%E7%BB%9C%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/"/>
      <url>/2018/09/18/2018-09-18-%E7%BD%91%E7%BB%9C%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="游戏形式"><a href="#游戏形式" class="headerlink" title="游戏形式"></a>游戏形式</h2><p>本游戏是通过socket来进行五子棋游戏，我们的形式是<code>一台server</code>开启之后可以支持尽可能多的<code>客户端</code>运行。</p><h2 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h2><ul><li><p><code>server</code></p><ul><li><code>net</code>：socket通信相关功能<ul><li>Action:解析socket传来的指令后进行的执行操作</li><li>EndDeal:游戏结束后的相关处理</li><li>Resolve：对收到的指令（字符串）进行解析处理</li><li>ServerThread：针对多个客户端，开启多个线程支持</li></ul></li><li><code>tool</code><ul><li>FightManager：下棋对战时的相关信息</li><li>HashMapManager：存储对战时配对上的map组合</li><li>MessageManager：用于进行消息管理，包括信息的发送等等</li><li>Player：存储玩家的个人信息</li><li><code>check</code>:进行游戏状态的检测<ul><li>check:检测胜负</li><li>checkX&amp;Y:横纵检测状态</li><li>checkM&amp;N:斜着检测状态</li></ul></li></ul></li><li><code>ui</code>：ui界面<ul><li>ClientPanel:客户端列表版面</li><li>MatchsPanel：配对连接版面</li><li>MessagePanel：消息显示版面</li><li>ServerFrame：服务器主窗口</li></ul></li><li>Server.java:主程序</li></ul></li><li><p><code>client</code></p><ul><li><code>data</code><ul><li>Data:存储玩家自己的相关信息，包括ID、昵称、配对对象等等</li></ul></li><li><code>image</code>：相关的图片，用于表示棋子等</li><li><code>listener</code>：用于监听相关的操作<ul><li>BackListener：悔棋操作</li><li>ChallengeListener：挑战操作</li><li>ConnectListener：登录操作</li><li>ListListener：列表双击操作</li><li>MapListener：监听棋盘</li><li>MessageListener：监听消息发送</li><li>NameListener：重命名操作</li><li>QuitListener：退出操作</li><li>RestartListener：重新开始操作</li><li>StartListener：游戏开始</li></ul></li><li><code>manager</code><ul><li>IOManager:输入输出流</li><li>ListManager:管理玩家列表</li><li>MessageManager：管理消息</li></ul></li><li><code>net</code><ul><li>Connect：登录服务器</li><li>PlayChess：游戏落字传输</li><li>Receive：接收数据线程</li><li>Resolve：解析数据</li></ul></li><li><code>ui</code><ul><li>ChessBoardCanvas：棋盘画板</li><li>FunctionPanel：功能区</li><li>GameFrame：游戏主界面</li><li>GamePanel：左边游戏区</li><li>MessagePanel：消息面板</li><li>OperationPanel：操作面板</li><li>PlayerPanel：玩家面板</li></ul></li><li>QuinterGame：客户端启动</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 课设记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Jekyll框架搭建</title>
      <link href="/2018/09/16/2018-09-16-MAC_OSX%E4%B8%8A%E5%AE%89%E8%A3%85jekyll/"/>
      <url>/2018/09/16/2018-09-16-MAC_OSX%E4%B8%8A%E5%AE%89%E8%A3%85jekyll/</url>
      
        <content type="html"><![CDATA[<p>1.安装ruby(mac上自带，此步可以跳过)</p><p>2.安装jekyll</p><ul><li><p>安装jekyll</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gem <span class="keyword">install</span> jekyll</span><br></pre></td></tr></table></figure></li><li><p>顺利地安装完成之后，可以生成自己的一个博客</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jekyll <span class="keyword">new</span> myBlog</span><br></pre></td></tr></table></figure></li><li><p>接下来尝试进入博客运行一下</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd myBlog</span><br><span class="line">jekyll <span class="keyword">server</span></span><br></pre></td></tr></table></figure></li><li><p>hin难受，果然报了个错！！！错误输出如下：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/System/</span>Library<span class="regexp">/Frameworks/</span>Ruby.framework<span class="regexp">/Versions/</span><span class="number">2.3</span><span class="regexp">/usr/</span>lib<span class="regexp">/ruby/</span><span class="number">2.3</span><span class="number">.0</span><span class="regexp">/rubygems/</span>core_ext/kernel_require.<span class="string">rb:</span><span class="number">55</span>:<span class="keyword">in</span> <span class="string">'require'</span>: cannot load such file -- bundler (LoadError)</span><br><span class="line">from <span class="regexp">/System/</span>Library<span class="regexp">/Frameworks/</span>Ruby.framework<span class="regexp">/Versions/</span><span class="number">2.3</span><span class="regexp">/usr/</span>lib<span class="regexp">/ruby/</span><span class="number">2.3</span><span class="number">.0</span><span class="regexp">/rubygems/</span>core_ext/kernel_require.<span class="string">rb:</span><span class="number">55</span>:<span class="keyword">in</span> <span class="string">'require'</span></span><br><span class="line">from <span class="regexp">/Library/</span>Ruby<span class="regexp">/Gems/</span><span class="number">2.3</span><span class="number">.0</span><span class="regexp">/gems/</span>jekyll<span class="number">-3.8</span><span class="number">.3</span><span class="regexp">/lib/</span>jekyll/plugin_manager.<span class="string">rb:</span><span class="number">48</span>:<span class="keyword">in</span> <span class="string">'require_from_bundler'</span></span><br><span class="line">from <span class="regexp">/Library/</span>Ruby<span class="regexp">/Gems/</span><span class="number">2.3</span><span class="number">.0</span><span class="regexp">/gems/</span>jekyll<span class="number">-3.8</span><span class="number">.3</span><span class="regexp">/exe/</span><span class="string">jekyll:</span><span class="number">11</span>:<span class="keyword">in</span> <span class="string">'&lt;top (required)&gt;'</span></span><br><span class="line">from <span class="regexp">/usr/</span>local<span class="regexp">/bin/</span><span class="string">jekyll:</span><span class="number">22</span>:<span class="keyword">in</span> <span class="string">'load'</span></span><br><span class="line">from <span class="regexp">/usr/</span>local<span class="regexp">/bin/</span><span class="string">jekyll:</span><span class="number">22</span>:<span class="keyword">in</span> <span class="string">'&lt;main&gt;'</span></span><br></pre></td></tr></table></figure></li><li><p>接下来就是上网找了找如何解决该问题，因为笔者也是小白，第一次搭博客，所幸找到了解决方案，感激！解决办法如下：</p></li><li><p>安装bundle</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gem <span class="keyword">install </span><span class="keyword">bundle</span></span><br><span class="line"><span class="keyword">gem </span><span class="keyword">install </span>minima</span><br><span class="line">gem <span class="keyword">install </span><span class="keyword">jekyll-feed</span></span><br></pre></td></tr></table></figure></li><li><p>本以为完成了，就开心的开启了一下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bundle </span>exec <span class="keyword">jekyll </span>serve</span><br></pre></td></tr></table></figure></li><li><p>但是！但是！但是！它又报了一个错！</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Could <span class="keyword">not</span> <span class="builtin-name">find</span> public_suffix-3.0.0 <span class="keyword">in</span> any of the sources</span><br><span class="line"><span class="builtin-name">Run</span> `bundle install` <span class="keyword">to</span> install missing gems.</span><br></pre></td></tr></table></figure></li><li><p>好的是，只要我们认真听话的执行一下提示代码就可以了！</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bundle </span>install</span><br></pre></td></tr></table></figure></li><li><p>然后等待全部安装完成</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">Bundle complete! <span class="number">4</span> Gemfile dependencies, <span class="number">23</span> gems now installed.</span><br><span class="line">Use `bundle info [gemname]` to see <span class="keyword">where</span> a bundled gem is installed.</span><br></pre></td></tr></table></figure></li><li><p>最后启动！</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bundle </span>exec <span class="keyword">jekyll </span>serve</span><br></pre></td></tr></table></figure></li><li><p>然后打开<a href="http://127.0.0.1:4000" target="_blank" rel="noopener">http://127.0.0.1:4000</a> ，就能完美的看到自己的博客啦，jekyll就已经安装完成咯。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Jekyll </tag>
            
            <tag> blog </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>2018华为秋招笔试</title>
      <link href="/2018/08/30/2018%E5%8D%8E%E4%B8%BA%E7%A7%8B%E6%8B%9B%E7%AC%94%E8%AF%95/"/>
      <url>/2018/08/30/2018%E5%8D%8E%E4%B8%BA%E7%A7%8B%E6%8B%9B%E7%AC%94%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>徘徊在保研与不保研之间，看了考研书感觉自己根本看不下去，看实验室同学都去了华为，因此就像报名了华为的秋招，报名了大数据开发岗，<del>由于之前参加比赛学过hadoop等大数据框架</del>，所以报了试试看，前几天通知了笔试，于是就去水了一波。</p><h2 id="笔试介绍"><a href="#笔试介绍" class="headerlink" title="笔试介绍"></a>笔试介绍</h2><p>华为今年的笔试和去年形式差不多，三道编程题，分值分别是100、200、300，语言基本上都可以使用，题目自我感觉很简单，因为都是一遍AC了，一个小时就解决了，hhh，就不吹了，下面记录记录题目和我的代码。</p><h3 id="字符串处理题"><a href="#字符串处理题" class="headerlink" title="字符串处理题"></a>字符串处理题</h3><p>题目是从一个字符串中找到所有存在的整数，包括负号，且负号可累计，然后将寻找到的整数相加即可，下面是代码：<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">import sys</span><br><span class="line">numlist= [str(i) <span class="keyword">for</span> i <span class="keyword">in</span> (range(<span class="number">10</span>))]</span><br><span class="line"><span class="comment"># print(numlist)</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">line</span> <span class="keyword">in</span> sys.<span class="keyword">stdin</span>:</span><br><span class="line">    <span class="keyword">a</span> = <span class="built_in">line</span>.<span class="built_in">split</span>()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">a</span>:</span><br><span class="line">        flag=True<span class="comment">#当前数字为正</span></span><br><span class="line">        <span class="built_in">sum</span>=<span class="number">0</span><span class="comment">#当前和</span></span><br><span class="line">        <span class="built_in">num</span>=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> <span class="keyword">char</span> <span class="keyword">in</span> i:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">char</span> == <span class="string">'-'</span>:</span><br><span class="line">                flag=<span class="keyword">not</span> flag</span><br><span class="line">            elif <span class="keyword">char</span> <span class="keyword">in</span> numlist:</span><br><span class="line">                <span class="comment">#数字处理</span></span><br><span class="line">                    <span class="built_in">num</span>=<span class="built_in">num</span>*<span class="number">10</span>+int(<span class="keyword">char</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> flag:</span><br><span class="line">                    <span class="built_in">sum</span>+=<span class="built_in">num</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">sum</span>-=<span class="built_in">num</span></span><br><span class="line">                <span class="comment"># sum+=num</span></span><br><span class="line">                <span class="comment"># print(sum)</span></span><br><span class="line">                <span class="built_in">num</span>=<span class="number">0</span></span><br><span class="line">                flag=True</span><br><span class="line">        <span class="keyword">if</span> flag:</span><br><span class="line">            <span class="built_in">sum</span>+=<span class="built_in">num</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">sum</span>-=<span class="built_in">num</span></span><br><span class="line">        print(<span class="built_in">sum</span>)</span><br></pre></td></tr></table></figure></p><h3 id="卷积计算"><a href="#卷积计算" class="headerlink" title="卷积计算"></a>卷积计算</h3><p>题目是需要按照给定的规则，来计算带有复数的卷积，卷积定义网上也有，我就不赘述了，直接开模拟，代码如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line">import sys</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">fuNum</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>,x,y)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.r=x</span><br><span class="line">        <span class="keyword">self</span>.im=y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(<span class="keyword">self</span>,b)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> fuNum(<span class="keyword">self</span>.r + b.r, <span class="keyword">self</span>.im + b.im)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sub</span><span class="params">(<span class="keyword">self</span>,b)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> fuNum(<span class="keyword">self</span>.r - b.r, <span class="keyword">self</span>.im - b.im)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mult</span><span class="params">(<span class="keyword">self</span>,b)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> fuNum(<span class="keyword">self</span>.r * b.r - <span class="keyword">self</span>.im * b.im, b.r * <span class="keyword">self</span>.im + <span class="keyword">self</span>.r * b.im)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name_<span class="number">_</span> == <span class="string">"__main__"</span><span class="symbol">:</span></span><br><span class="line">    duoa=[]</span><br><span class="line">    duob=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>)<span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 读取每一行</span></span><br><span class="line">        r = eval(sys.stdin.readline().strip())</span><br><span class="line">        im= eval(sys.stdin.readline().strip())</span><br><span class="line">        duoa.append(fuNum(r,im))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>)<span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 读取每一行</span></span><br><span class="line">        r = eval(sys.stdin.readline().strip())</span><br><span class="line">        im= eval(sys.stdin.readline().strip())</span><br><span class="line">        duob.append(fuNum(r,im))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)<span class="symbol">:</span></span><br><span class="line">        duoa.append(fuNum(<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">        duob.append(fuNum(<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    <span class="comment"># n=4</span></span><br><span class="line">    duoc=[]</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">9</span>)<span class="symbol">:</span></span><br><span class="line">        temp=fuNum(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(n+<span class="number">1</span>)<span class="symbol">:</span></span><br><span class="line">            <span class="comment"># print(k,n)</span></span><br><span class="line">            temp=temp.add(duoa[k].mult(duob[n-k]))</span><br><span class="line">        <span class="comment"># print(temp.r,temp.im)</span></span><br><span class="line">        duoc.append(temp)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(duoc)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="symbol">duoc:</span></span><br><span class="line">        print(i.r)</span><br><span class="line">        print(i.im)</span><br></pre></td></tr></table></figure></p><h3 id="牛生小牛的问题"><a href="#牛生小牛的问题" class="headerlink" title="牛生小牛的问题"></a>牛生小牛的问题</h3><p>传说中300分的题目，题目具体数据记不清了，是牛生小牛的问题，不过好像和经典例题的递归形式不同，这题我在思考的时候加上了dp的思想，<del>因为用递归模拟，发现过不了样例</del>，代码如下：<br><figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 读取第一行的n</span></span><br><span class="line">    n = <span class="built_in">int</span>(sys.<span class="literal">stdin</span>.readline().strip())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="comment"># 每一个例子</span></span><br><span class="line">        M = eval(sys.<span class="literal">stdin</span>.readline().strip())</span><br><span class="line">        N = eval(sys.<span class="literal">stdin</span>.readline().strip())</span><br><span class="line">        cheng=[M,M,M,M,M]</span><br><span class="line">        wei=[<span class="number">0</span>,M,<span class="number">2</span>*M,<span class="number">3</span>*M,<span class="number">4</span>*M]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>,N+<span class="number">1</span>):</span><br><span class="line">            cheng.append(cheng[i-<span class="number">1</span>]+(wei[i-<span class="number">3</span>]-wei[i-<span class="number">4</span>]))</span><br><span class="line">            wei.append(wei[i-<span class="number">1</span>]+cheng[i]-(wei[i-<span class="number">3</span>]-wei[i-<span class="number">4</span>]))</span><br><span class="line">            <span class="comment"># print(i,cheng[i],wei[i])</span></span><br><span class="line">        print(cheng[N]+wei[N])</span><br></pre></td></tr></table></figure></p><p>就这样三道题目就全部AC了，难度不是太大，感觉打过ACM的全AC是很正常的事情，所以对ACMer菊厂也是相当欢迎的。</p><hr><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>九月初拿到学校保研资格的那天，放弃了菊厂的面试，菊厂打了好几个电话来问，给人的感觉是相当负责的，当时和他们说等我研究生毕业再去菊厂了，哈哈哈，又装了13，溜了溜了看书了。</p>]]></content>
      
      
      <categories>
          
          <category> 刷题记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 笔试 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python实践——塔防游戏</title>
      <link href="/2018/07/20/python%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%A1%94%E9%98%B2%E6%B8%B8%E6%88%8F/"/>
      <url>/2018/07/20/python%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%A1%94%E9%98%B2%E6%B8%B8%E6%88%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这仅仅只是一个入门级游戏，一个小toy，使用的也就是贴图形式的pygame，因为需要过多的访存次数，图片的精度也较高，所以对内存以及硬盘的速度有着较高的要求。<del>当时答辩的辣鸡主机上，我们跑的就相当卡。</del></p><h2 id="环境要求"><a href="#环境要求" class="headerlink" title="环境要求"></a>环境要求</h2><p>语言版本：python 3.6.2<br><br>依赖：pygame<br><br>系统：皆可<br></p><h2 id="游戏按键"><a href="#游戏按键" class="headerlink" title="游戏按键"></a>游戏按键</h2><p>暂停/开始：Space<br><br>退出：Esc<br></p><h2 id="截图"><a href="#截图" class="headerlink" title="截图"></a>截图</h2><p><img src="/2018/07/20/python实践——塔防游戏/开始界面.png" alt="游戏开成动画" title="游戏开成动画"><br><img src="/2018/07/20/python实践——塔防游戏/游戏截图.png" alt="游戏中截图" title="游戏截图"></p><h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><p><a href="https://github.com/netycc/Tower-defense-game" title="塔防游戏" target="_blank" rel="noopener">https://github.com/netycc/Tower-defense-game</a></p>]]></content>
      
      
      <categories>
          
          <category> 课设记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
