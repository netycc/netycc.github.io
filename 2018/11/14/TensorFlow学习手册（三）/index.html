<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="用于日常笔记"><meta name="baidu-site-verification" content="31u13chEy5"><title>TensorFlow学习手册（三） | Netycc's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '83d60cd5e215de54f53db5b26853c623';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
  </script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">TensorFlow学习手册（三）</h1><a id="logo" href="/.">Netycc's blog</a><p class="description">每天进步一点点，吃吃喝喝的single dog.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/timeline"><i class="fa fa-history"> 时间线</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">TensorFlow学习手册（三）</h1><div class="post-meta">Nov 14, 2018<span> | </span><span class="category"><a href="/categories/学习笔记/">学习笔记</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.9k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 13</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" href="/2018/11/14/TensorFlow学习手册（三）/#vcomment"><span class="valine-comment-count" data-xid="/2018/11/14/TensorFlow学习手册（三）/"></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorFlow模型持久化"><span class="toc-number">2.</span> <span class="toc-text">TensorFlow模型持久化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorFlow最佳实践样例程序"><span class="toc-number">3.</span> <span class="toc-text">TensorFlow最佳实践样例程序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#mnist-inference-py"><span class="toc-number">3.1.</span> <span class="toc-text">mnist_inference.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mnist-train-py"><span class="toc-number">3.2.</span> <span class="toc-text">mnist_train.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mnist-eval-py"><span class="toc-number">3.3.</span> <span class="toc-text">mnist_eval.py</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积神经网络"><span class="toc-number">4.</span> <span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#卷积层"><span class="toc-number">4.1.</span> <span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#池化层"><span class="toc-number">4.2.</span> <span class="toc-text">池化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN"><span class="toc-number">4.3.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LeNet-5"><span class="toc-number">4.3.1.</span> <span class="toc-text">LeNet-5</span></a></li></ol></li></ol></li></ol></div></div><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;本节将叙述一下如何将训练好的模型保存下来，并且在使用的时候还原出来，以及卷积神经网络的构建方法以及TensorFlow实现。</p>
<h2 id="TensorFlow模型持久化"><a href="#TensorFlow模型持久化" class="headerlink" title="TensorFlow模型持久化"></a>TensorFlow模型持久化</h2><p>&emsp;&emsp;在TensorFlow中，提供了tf.train.Saver类用来保存TensorFlow计算图，以下代码给出了保存TensorFlow计算图的方法。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line"></span><br><span class="line"><span class="attr">v1</span> = tf.Variable(tf.constant(<span class="number">1.0</span>, <span class="attr">shape=[1]),</span> <span class="attr">name</span> = <span class="string">"v1"</span>)</span><br><span class="line"><span class="attr">v2</span> = tf.Variable(tf.constant(<span class="number">2.0</span>, <span class="attr">shape=[1]),</span> <span class="attr">name</span> = <span class="string">"v2"</span>)</span><br><span class="line"><span class="attr">result</span> = v1 + v2</span><br><span class="line"></span><br><span class="line"><span class="attr">init_op</span> = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># 可以指定saver保存什么变量，如果只保存v1，那么下边要恢复result就会出错，因为缺少v2</span></span><br><span class="line"><span class="attr">saver</span> = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() as sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    saver.save(sess, <span class="string">"Saved_model/model.ckpt"</span>)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;以上代码实现了持久化一个简单的TensorFlow模型的功能。在这段代码中，saver.save函数将TensorFlow模型保存到了<code>Saved_model/model.ckpt</code>文件中， 但是在该文件夹下，会产生三个文件，第一个文件为<code>model.ckpt.meta</code>他保存了计算图的结构；第二个文件为<code>model.ckpt</code>，这个文件保存了TensorFlow程序中每一个变量的取值；最后一个文件为<code>checkpoint</code>文件，这个文件中保存了一个目录下所有的模型文件列表。<br>&emsp;&emsp;以下代码给出了加载这个已经保存的TensorFlow模型的方法。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line"></span><br><span class="line">v1 = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name = <span class="string">"v1"</span>)</span><br><span class="line">v2 = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name = <span class="string">"v2"</span>)</span><br><span class="line">result = v1 + v2</span><br><span class="line"></span><br><span class="line">saver = <span class="keyword">tf</span>.train.Saver()</span><br><span class="line"></span><br><span class="line">with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></span><br><span class="line">    saver.restore(sess, <span class="string">"Saved_model/model.ckpt"</span>)</span><br><span class="line">    <span class="keyword">print</span>(result.<span class="built_in">eval</span>())</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;可以看出这个代码与保存模型的代码差不多。在加载模型的代码中没有运行变量的初始化过程，而是将变量的值通过已经保存的模型加载下来。如果不希望重复定义图上的运算，也可以直接加载已经持久化的图。如下：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line"></span><br><span class="line">saver = <span class="keyword">tf</span>.train.import_meta_graph(<span class="string">"Saved_model/model.ckpt.meta"</span>)</span><br><span class="line">with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></span><br><span class="line">    saver.restore(sess, <span class="string">"Saved_model/model.ckpt"</span>)</span><br><span class="line">    <span class="keyword">print</span>(sess.run(<span class="keyword">tf</span>.get_default_graph().get_tensor_by_name(<span class="string">"add:0"</span>)))</span><br></pre></td></tr></table></figure></p>
<h2 id="TensorFlow最佳实践样例程序"><a href="#TensorFlow最佳实践样例程序" class="headerlink" title="TensorFlow最佳实践样例程序"></a>TensorFlow最佳实践样例程序</h2><h3 id="mnist-inference-py"><a href="#mnist-inference-py" class="headerlink" title="mnist_inference.py"></a>mnist_inference.py</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line"></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line">LAYER1_NODE = <span class="number">500</span></span><br><span class="line"></span><br><span class="line">def get_weight_variable(shape, regularizer):</span><br><span class="line">    weights = <span class="keyword">tf</span>.get_variable(<span class="string">"weights"</span>, shape, initializer=<span class="keyword">tf</span>.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">    <span class="keyword">if</span> regularizer != None: <span class="keyword">tf</span>.add_to_collection(<span class="string">'losses'</span>, regularizer(weights))</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line">def inference(input_tensor, regularizer):</span><br><span class="line">    with <span class="keyword">tf</span>.variable_scope(<span class="string">'layer1'</span>):</span><br><span class="line"></span><br><span class="line">        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)</span><br><span class="line">        biases = <span class="keyword">tf</span>.get_variable(<span class="string">"biases"</span>, [LAYER1_NODE], initializer=<span class="keyword">tf</span>.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        layer1 = <span class="keyword">tf</span>.<span class="keyword">nn</span>.relu(<span class="keyword">tf</span>.matmul(input_tensor, weights) + biases)</span><br><span class="line"></span><br><span class="line">    with <span class="keyword">tf</span>.variable_scope(<span class="string">'layer2'</span>):</span><br><span class="line">        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)</span><br><span class="line">        biases = <span class="keyword">tf</span>.get_variable(<span class="string">"biases"</span>, [OUTPUT_NODE], initializer=<span class="keyword">tf</span>.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        layer2 = <span class="keyword">tf</span>.matmul(layer1, weights) + biases</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> layer2</span><br></pre></td></tr></table></figure>
<h3 id="mnist-train-py"><a href="#mnist-train-py" class="headerlink" title="mnist_train.py"></a>mnist_train.py</h3><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist <span class="built_in">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> mnist_inference</span><br><span class="line"></span><br><span class="line"><span class="attr">BATCH_SIZE</span> = <span class="number">100</span></span><br><span class="line"><span class="attr">LEARNING_RATE_BASE</span> = <span class="number">0.8</span></span><br><span class="line"><span class="attr">LEARNING_RATE_DECAY</span> = <span class="number">0.99</span></span><br><span class="line"><span class="attr">REGULARIZATION_RATE</span> = <span class="number">0.0001</span></span><br><span class="line"><span class="attr">TRAINING_STEPS</span> = <span class="number">30000</span></span><br><span class="line"><span class="attr">MOVING_AVERAGE_DECAY</span> = <span class="number">0.99</span></span><br><span class="line"><span class="attr">MODEL_SAVE_PATH="MNIST_model/"</span></span><br><span class="line"><span class="attr">MODEL_NAME="mnist_model"</span></span><br><span class="line"></span><br><span class="line">def train(mnist):</span><br><span class="line"></span><br><span class="line">    <span class="attr">x</span> = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], <span class="attr">name='x-input')</span></span><br><span class="line">    <span class="attr">y_</span> = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], <span class="attr">name='y-input')</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">regularizer</span> = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)</span><br><span class="line">    <span class="attr">y</span> = mnist_inference.inference(x, regularizer)</span><br><span class="line">    <span class="attr">global_step</span> = tf.Variable(<span class="number">0</span>, <span class="attr">trainable=False)</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">variable_averages</span> = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    <span class="attr">variables_averages_op</span> = variable_averages.apply(tf.trainable_variables())</span><br><span class="line">    <span class="attr">cross_entropy</span> = tf.nn.sparse_softmax_cross_entropy_with_logits(<span class="attr">logits=y,</span> <span class="attr">labels=tf.argmax(y_,</span> <span class="number">1</span>))</span><br><span class="line">    <span class="attr">cross_entropy_mean</span> = tf.reduce_mean(cross_entropy)</span><br><span class="line">    <span class="attr">loss</span> = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))</span><br><span class="line">    <span class="attr">learning_rate</span> = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,</span><br><span class="line">        <span class="attr">staircase=True)</span></span><br><span class="line">    <span class="attr">train_step</span> = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, <span class="attr">global_step=global_step)</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, variables_averages_op]):</span><br><span class="line">        <span class="attr">train_op</span> = tf.no_op(<span class="attr">name='train')</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">saver</span> = tf.train.Saver()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() as sess:</span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        <span class="built_in">import</span> time</span><br><span class="line">        <span class="attr">start_time=time.time()</span></span><br><span class="line">        for i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">            xs, <span class="attr">ys</span> = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            _, loss_value, <span class="attr">step</span> = sess.run([train_op, loss, global_step], <span class="attr">feed_dict=&#123;x:</span> xs, y_: ys&#125;)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># print("spend"+str(time.time()-start_time))</span></span><br><span class="line">                print(<span class="string">"Spend %.2f s,after %d training step(s), loss on training batch is %g."</span> % (time.time()-start_time,step, loss_value))</span><br><span class="line">                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), <span class="attr">global_step=global_step)</span></span><br><span class="line">                <span class="attr">start_time=time.time()</span></span><br><span class="line"></span><br><span class="line">def main(<span class="attr">argv=None):</span></span><br><span class="line">    <span class="attr">mnist</span> = input_data.read_data_sets(<span class="string">"data"</span>, <span class="attr">one_hot=True)</span></span><br><span class="line">    train(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attr">__name__</span> == '__main__':</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>
<h3 id="mnist-eval-py"><a href="#mnist-eval-py" class="headerlink" title="mnist_eval.py"></a>mnist_eval.py</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line"></span><br><span class="line">import mnist_train</span><br><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">import mnist_inference</span><br><span class="line"></span><br><span class="line"># 加载的时间间隔。</span><br><span class="line">EVAL_INTERVAL_SECS = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">def evaluate(mnist):</span><br><span class="line">    with <span class="keyword">tf</span>.Graph().as_default() <span class="keyword">as</span> <span class="variable">g:</span></span><br><span class="line">        <span class="keyword">x</span> = <span class="keyword">tf</span>.placeholder(<span class="keyword">tf</span>.float32, [None, mnist_inference.INPUT_NODE], name=<span class="string">'x-input'</span>)</span><br><span class="line">        y_ = <span class="keyword">tf</span>.placeholder(<span class="keyword">tf</span>.float32, [None, mnist_inference.OUTPUT_NODE], name=<span class="string">'y-input'</span>)</span><br><span class="line">        validate_feed = &#123;<span class="keyword">x</span>: mnist.validation.images, y_: mnist.validation.labels&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">y</span> = mnist_inference.inference(<span class="keyword">x</span>, None)</span><br><span class="line">        correct_prediction = <span class="keyword">tf</span>.equal(<span class="keyword">tf</span>.argmax(<span class="keyword">y</span>, <span class="number">1</span>), <span class="keyword">tf</span>.argmax(y_, <span class="number">1</span>))</span><br><span class="line">        accuracy = <span class="keyword">tf</span>.reduce_mean(<span class="keyword">tf</span>.cast(correct_prediction, <span class="keyword">tf</span>.float32))</span><br><span class="line"></span><br><span class="line">        variable_averages = <span class="keyword">tf</span>.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)</span><br><span class="line">        variables_to_restore = variable_averages.variables_to_restore()</span><br><span class="line">        saver = <span class="keyword">tf</span>.train.Saver(variables_to_restore)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> True:</span><br><span class="line">            with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></span><br><span class="line">                ckpt = <span class="keyword">tf</span>.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)</span><br><span class="line">                <span class="keyword">if</span> ckpt <span class="built_in">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">                    saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">                    <span class="keyword">for</span> v in <span class="keyword">tf</span>.global_variables():</span><br><span class="line">                        <span class="keyword">print</span>(v.name, <span class="string">":"</span>, v.<span class="built_in">eval</span>())</span><br><span class="line">                    <span class="keyword">print</span>(<span class="string">"#####################"</span>)</span><br><span class="line">                    global_step = ckpt.model_checkpoint_path.<span class="keyword">split</span>(<span class="string">'/'</span>)[-<span class="number">1</span>].<span class="keyword">split</span>(<span class="string">'-'</span>)[-<span class="number">1</span>]</span><br><span class="line">                    accuracy_score = sess.run(accuracy, feed_dict=validate_feed)</span><br><span class="line">                    <span class="keyword">print</span>(<span class="string">"After %s training step(s), validation accuracy = %g"</span> % (global_step, accuracy_score))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">print</span>(<span class="string">'No checkpoint file found'</span>)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">            time.<span class="keyword">sleep</span>(EVAL_INTERVAL_SECS)</span><br><span class="line"></span><br><span class="line">def main(<span class="built_in">argv</span>=None):</span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"data"</span>, one_hot=True)</span><br><span class="line">    evaluate(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>&emsp;&emsp;理论知识不再赘述，该文只叙述如何使用TensorFlow构建CNN结构，一般构建卷积层是通过以下几个步骤：</p>
<ol>
<li><p>构建过滤器参数尺寸，参数为四维，前两个代表了过滤器的尺寸，第三个表示当前层的深度，第四个表示过滤器的深度。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">filter_weight = tf.get_variable('weights',[<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">16</span>],</span><br><span class="line">                    initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建偏执项，偏执项尺寸为过滤器的深度。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">biases</span> = tf.get_variable(<span class="string">'biases'</span>,[<span class="number">16</span>],initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>利用tf.nn.conv2d()创建卷积层，参数依次为：输入的batch，过滤器权值，各维度步长，填充方法。<strong>其中在各维度步长的参数中，第一位和最后一维的数字一定要是1，因为卷积的步长只与矩阵的长宽有效。</strong>填充参数有两种选择，分别是SAME（全0填充）、VALID（表示不添加）。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv = tf<span class="selector-class">.nn</span><span class="selector-class">.conv2d</span>(<span class="selector-tag">input</span>,filter_weight,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],<span class="attribute">padding</span>=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>利用tf.nn.bias_add()完成加偏执项，第一个参数是该卷积层，第二个参数是偏执。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bias = tf<span class="selector-class">.nn</span><span class="selector-class">.bias_add</span>(conv,biases)</span><br></pre></td></tr></table></figure>
</li>
<li><p>将计算结果通过激活函数去线性化。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">actived_conv = tf<span class="selector-class">.nn</span><span class="selector-class">.relu</span>(bias)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>&emsp;&emsp;池化层可以有效地缩小矩阵的尺寸，从而减少最后全连接层的参数。使用池化层既可以加快计算速度也有防止过拟合问题的作用。<br>&emsp;&emsp;池化层采用更加简单的最大值或者平均值计算，也叫作最大池化层(max pooling)或者平均池化层(average pooling。卷积层与池化层中过滤器移动的方式是相似的，唯一的区别在于卷积层使用的过滤器是横跨整个深度的，而池化层使用的过滤器只影响一个深度的节点。以下TensorFlow程序实现了最大池化层的前向传播算法：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pool = tf.nn.max_pool(actived_conv,ksize=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding='SAME')</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;在<code>tf.nn.max_pool</code>函数中，首先需要传入当前层的节点矩阵，第二个参数为过滤器的尺寸，且第一个和最后一个数字必须为1，第三个参数为步长信息且第一个和最后一个数字必须为1，padding也是填充选项。</p>
<h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p>&emsp;&emsp;可以用以下正则表达式来表示一些经典的用于图片分类的问题的卷积神经网络结构：$$输入层\to（卷积层+ \to 池化层？)+\to 全连接层+$$<br>&emsp;&emsp;在以上公式中，“卷积层+”表示一层或多层卷积层，大部分卷积神经网络中一般最多连续使用三层卷积层。“池化层？”表示没有或者一层池化层。LeNet-5模型就可以表示为以下结构:$$输入层\to卷积层\to池化层\to卷积层\to池化层\to全连接层\to全连接层\to输出层$$</p>
<h4 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h4><p>&emsp;&emsp;解决MNIST手写数字分类的源代码：</p>
<p><strong>LeNet5_infernece.py:</strong><br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入为28*28的图像[28, 28]</span></span><br><span class="line"><span class="attr">INPUT_NODE</span> = <span class="number">784</span></span><br><span class="line"><span class="comment"># 输出为1~10的可能性[10]</span></span><br><span class="line"><span class="attr">OUTPUT_NODE</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像尺寸</span></span><br><span class="line"><span class="attr">IMAGE_SIZE</span> = <span class="number">28</span></span><br><span class="line"><span class="comment"># 图像的颜色通道数，这里只有黑白一种通道</span></span><br><span class="line"><span class="attr">NUM_CHANNELS</span> = <span class="number">1</span></span><br><span class="line"><span class="comment"># 标签的数量</span></span><br><span class="line"><span class="attr">NUM_LABELS</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一层卷积的深度</span></span><br><span class="line"><span class="attr">CONV1_DEEP</span> = <span class="number">32</span></span><br><span class="line"><span class="comment"># 第一层卷积的过滤器尺寸</span></span><br><span class="line"><span class="attr">CONV1_SIZE</span> = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二层卷积的深度</span></span><br><span class="line"><span class="attr">CONV2_DEEP</span> = <span class="number">64</span></span><br><span class="line"><span class="comment"># 第二层卷积的过滤器尺寸</span></span><br><span class="line"><span class="attr">CONV2_SIZE</span> = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全连接层的节点个数</span></span><br><span class="line"><span class="attr">FC_SIZE</span> = <span class="number">512</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 常见的卷积模型</span></span><br><span class="line"><span class="comment"># 本例子卷积模型 输入 -&gt; 卷积层 -&gt; 池化层 -&gt; 卷积层 -&gt; 池化层 -&gt; 全连接层 -&gt; 全连接层</span></span><br><span class="line"><span class="comment"># 输入 -&gt; (卷积层+ -&gt; 池化层?)+ -&gt; 全连接层+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def inference(input_tensor, train, regularizer):</span><br><span class="line">    <span class="comment"># 第一层卷积1</span></span><br><span class="line">    <span class="comment"># 输入为[x-size=28, y-size=28, channel=1]的图像</span></span><br><span class="line">    <span class="comment"># 过滤器尺寸[x-size=5, y-size=5, channel=1, deep=32]</span></span><br><span class="line">    <span class="comment"># 过滤器步长=1</span></span><br><span class="line">    <span class="comment"># 输出为[x-size=28, y-size=28, deep=32]的矩阵</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope('layer1-conv1'):</span><br><span class="line">        <span class="attr">conv1_weights</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="weight",</span></span><br><span class="line">            <span class="attr">shape=[CONV1_SIZE,</span> CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],</span><br><span class="line">            <span class="attr">initializer=tf.truncated_normal_initializer(stddev=0.1)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="attr">conv1_biases</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="bias",</span></span><br><span class="line">            <span class="attr">shape=[CONV1_DEEP],</span></span><br><span class="line">            <span class="attr">initializer=tf.constant_initializer(0.0)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="attr">conv1</span> = tf.nn.conv2d(input_tensor, conv1_weights, <span class="attr">strides=[1,</span> <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="attr">padding='SAME')</span></span><br><span class="line">        <span class="attr">relu1</span> = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二层池化1</span></span><br><span class="line">    <span class="comment"># 输入为[x-size=28, y-size=28, deep=32]的矩阵</span></span><br><span class="line">    <span class="comment"># 过滤器尺寸[x-size=2, y-size=2]</span></span><br><span class="line">    <span class="comment"># 过滤器步长=2</span></span><br><span class="line">    <span class="comment"># 输出为[x-size=14, y-size=14, deep=32]的矩阵</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"layer2-pool1"</span>):</span><br><span class="line">        <span class="attr">pool1</span> = tf.nn.max_pool(relu1, <span class="attr">ksize</span> = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],<span class="attr">strides=[1,2,2,1],padding="SAME")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第三层卷积2</span></span><br><span class="line">    <span class="comment"># 输入为[x-size=14, y-size=14, deep=32]的矩阵</span></span><br><span class="line">    <span class="comment"># 过滤器尺寸[x-size=5, y-size=5, channel=1, deep=64]</span></span><br><span class="line">    <span class="comment"># 过滤器步长=1</span></span><br><span class="line">    <span class="comment"># 输出为[x-size=14, y-size=14, deep=64]的矩阵</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer3-conv2"</span>):</span><br><span class="line">        <span class="attr">conv2_weights</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="weight",</span></span><br><span class="line">            <span class="attr">shape=[CONV2_SIZE,</span> CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],</span><br><span class="line">            <span class="attr">initializer=tf.truncated_normal_initializer(stddev=0.1)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="attr">conv2_biases</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="bias",</span></span><br><span class="line">            <span class="attr">shape=[CONV2_DEEP],</span></span><br><span class="line">            <span class="attr">initializer=tf.constant_initializer(0.0)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="attr">conv2</span> = tf.nn.conv2d(pool1, conv2_weights, <span class="attr">strides=[1,</span> <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="attr">padding='SAME')</span></span><br><span class="line">        <span class="attr">relu2</span> = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第四层池化2</span></span><br><span class="line">    <span class="comment"># 输入为[x-size=14, y-size=14, deep=64]的矩阵</span></span><br><span class="line">    <span class="comment"># 过滤器尺寸[x-size=2, y-size=2]</span></span><br><span class="line">    <span class="comment"># 过滤器步长=2</span></span><br><span class="line">    <span class="comment"># 输出为[x-size=7, y-size=7, deep=64]的矩阵</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"layer4-pool2"</span>):</span><br><span class="line">        <span class="attr">pool2</span> = tf.nn.max_pool(relu2, <span class="attr">ksize=[1,</span> <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], <span class="attr">strides=[1,</span> <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], <span class="attr">padding='SAME')</span></span><br><span class="line">        <span class="comment"># 把[batch, x-size, y-size, deep]4维矩阵转化为[batch, vector]2维矩阵，长*宽*深度转换为1维向量</span></span><br><span class="line">        <span class="attr">pool_shape</span> = pool2.get_shape().as_list()</span><br><span class="line">        <span class="attr">nodes</span> = pool_shape[<span class="number">1</span>] * pool_shape[<span class="number">2</span>] * pool_shape[<span class="number">3</span>]</span><br><span class="line">        <span class="attr">reshaped</span> = tf.reshape(pool2, [pool_shape[<span class="number">0</span>], nodes])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全连接层</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope('layer5-fc1'):</span><br><span class="line">        <span class="attr">fc1_weights</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="weight",</span></span><br><span class="line">            <span class="attr">shape=[nodes,</span> FC_SIZE],</span><br><span class="line">            <span class="attr">initializer=tf.truncated_normal_initializer(stddev=0.1)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 只有全连接的权重需要加入正则化</span></span><br><span class="line">        <span class="keyword">if</span> regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))</span><br><span class="line">        <span class="attr">fc1_biases</span> = tf.get_variable(<span class="string">"bias"</span>, [FC_SIZE], <span class="attr">initializer=tf.constant_initializer(0.1))</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">fc1</span> = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)</span><br><span class="line">        <span class="comment"># dropout在训练数据的时候，会随机把部分输出改为0</span></span><br><span class="line">        <span class="comment"># dropout可以避免过度拟合，dropout一般只在全连接层，而不是在卷积层或者池化层使用</span></span><br><span class="line">        <span class="keyword">if</span> train: <span class="attr">fc1</span> = tf.nn.dropout(fc1, <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全连接层</span></span><br><span class="line">    <span class="comment"># 输入为[512]的向量</span></span><br><span class="line">    <span class="comment"># 输出为[10]的向量</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope('layer6-fc2'):</span><br><span class="line">        <span class="attr">fc2_weights</span> = tf.get_variable(</span><br><span class="line">            <span class="attr">name="weight",</span></span><br><span class="line">            <span class="attr">shape=[FC_SIZE,</span> NUM_LABELS],</span><br><span class="line">            <span class="attr">initializer=tf.truncated_normal_initializer(stddev=0.1)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))</span><br><span class="line">        <span class="attr">fc2_biases</span> = tf.get_variable(<span class="string">"bias"</span>, [NUM_LABELS], <span class="attr">initializer=tf.constant_initializer(0.1))</span></span><br><span class="line">        <span class="attr">logit</span> = tf.matmul(fc1, fc2_weights) + fc2_biases</span><br><span class="line"></span><br><span class="line">    return logit</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>LeNet5_train.py</strong><br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist <span class="built_in">import</span> input_data</span><br><span class="line"><span class="built_in">import</span> LeNet5_infernece</span><br><span class="line"><span class="built_in">import</span> os</span><br><span class="line"><span class="built_in">import</span> numpy as np</span><br><span class="line"></span><br><span class="line"><span class="attr">BATCH_SIZE</span> = <span class="number">1000</span></span><br><span class="line"><span class="attr">LEARNING_RATE_BASE</span> = <span class="number">0.01</span></span><br><span class="line"><span class="attr">LEARNING_RATE_DECAY</span> = <span class="number">0.99</span></span><br><span class="line"><span class="attr">REGULARIZATION_RATE</span> = <span class="number">0.0001</span></span><br><span class="line"><span class="attr">TRAINING_STEPS</span> = <span class="number">6000</span></span><br><span class="line"><span class="attr">MOVING_AVERAGE_DECAY</span> = <span class="number">0.99</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train(mnist):</span><br><span class="line">    <span class="comment"># 定义输出为4维矩阵的placeholder</span></span><br><span class="line">    <span class="attr">x</span> = tf.placeholder(tf.float32, [</span><br><span class="line">        BATCH_SIZE,</span><br><span class="line">        LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">        LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">        LeNet5_infernece.NUM_CHANNELS],</span><br><span class="line">                       <span class="attr">name='x-input')</span></span><br><span class="line">    <span class="attr">y_</span> = tf.placeholder(tf.float32, [None, LeNet5_infernece.OUTPUT_NODE], <span class="attr">name='y-input')</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">regularizer</span> = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)</span><br><span class="line">    <span class="attr">y</span> = LeNet5_infernece.inference(x, False, regularizer)</span><br><span class="line">    <span class="attr">global_step</span> = tf.Variable(<span class="number">0</span>, <span class="attr">trainable=False)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义损失函数、学习率、滑动平均操作以及训练过程。</span></span><br><span class="line">    <span class="attr">variable_averages</span> = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    <span class="attr">variables_averages_op</span> = variable_averages.apply(tf.trainable_variables())</span><br><span class="line">    <span class="attr">cross_entropy</span> = tf.nn.sparse_softmax_cross_entropy_with_logits(<span class="attr">logits=y,</span> <span class="attr">labels=tf.argmax(y_,</span> <span class="number">1</span>))</span><br><span class="line">    <span class="attr">cross_entropy_mean</span> = tf.reduce_mean(cross_entropy)</span><br><span class="line">    <span class="attr">loss</span> = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))</span><br><span class="line">    <span class="attr">learning_rate</span> = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,</span><br><span class="line">        <span class="attr">staircase=True)</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">train_step</span> = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, <span class="attr">global_step=global_step)</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, variables_averages_op]):</span><br><span class="line">        <span class="attr">train_op</span> = tf.no_op(<span class="attr">name='train')</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化TensorFlow持久化类。</span></span><br><span class="line">    <span class="attr">saver</span> = tf.train.Saver()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() as sess:</span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        <span class="built_in">import</span> time</span><br><span class="line">        <span class="attr">start_time=time.time()</span></span><br><span class="line">        for i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">            xs, <span class="attr">ys</span> = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">            <span class="attr">reshaped_xs</span> = np.reshape(xs, (</span><br><span class="line">                BATCH_SIZE,</span><br><span class="line">                LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">                LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">                LeNet5_infernece.NUM_CHANNELS))</span><br><span class="line">            _, loss_value, <span class="attr">step</span> = sess.run([train_op, loss, global_step], <span class="attr">feed_dict=&#123;x:</span> reshaped_xs, y_: ys&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Spend %.2f s, after %d training step(s), loss on training batch is %g."</span> % (time.time()-start_time,step, loss_value))</span><br><span class="line">                <span class="attr">start_time=time.time()</span></span><br><span class="line"></span><br><span class="line">def main(<span class="attr">argv=None):</span></span><br><span class="line">    <span class="attr">mnist</span> = input_data.read_data_sets(<span class="string">"data"</span>, <span class="attr">one_hot=True)</span></span><br><span class="line">    train(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attr">__name__</span> == '__main__':</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>netycc</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/11/14/TensorFlow学习手册（三）/">https://netycc.com/2018/11/14/TensorFlow学习手册（三）/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://netycc.com/2018/11/14/TensorFlow学习手册（三）/" data-id="cjol8k6ku000qh182raqafirs" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACIklEQVR42u3aQZLCMAwEQP7/afa8xRJm5CxVcdonKgmOmoOwZT0e8Xi+jNfryZPtbCcPDAyMyzKeh+M43HNDz+/+QcXAwLgB410Ge/dMmyijUIL3vr2OgYGBETDybyUDAwMDY4XRkvL0+tWEi4GBcUFGnviSctgxILn7j3txDAyMCzLyqvv3P//L+QYGBsalGM9yJPO0yXcWya/ZMDAwtmbMlnorm8+8gaONBwMDY1fGWS873qa2i8LjH+gR1d4wMDD2YczaJtr02jZ+zY5RMTAwdmW0rauztNvC2uNVDAyMuzHypV5edGvbL05os8DAwNiOMWtxmLVE5HMWbAwMjJsxZqX/2eYzb6r48F4MDIytGW0RP1kCtkeS69tgDAyMOzCKY8L4+WQLOkv3xTIRAwNjI0Y7Xf6adUyRiDEwMG7AaBeIZyXZPNCoZwQDA2M7RlvEn11PjjzbAhwGBsZ9GG0SXGm8aJeDdXkOAwNja0YbdLLpnc22Pj8GBsaujLZAtt5+0Zb2iv8NDAyMrRntIUEeVv4z5ek1Ot/AwMDYlDFLl3nRbaW49iEpY2BgbMp4lmOWXs+iFmtbDAyMjRhtmmtLY3nQZ22MMTAwdmXkKbJYb5Yhzir8GBgYd2PUbQ2zZojv9IxgYGBglG0T+TZ46YATAwMDY9Rssd7AkS9eMTAw7sDIS/ltQG1qnqVjDAyMvRmzFoe20NYGPWwOw8DA2I3xA/MInTtMtKIcAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/深度学习/">深度学习</a><a href="/tags/TensorFlow/">TensorFlow</a></div><div class="post-nav"><a class="pre" href="/2018/11/14/安装cuda9-0和cudnn7-4以及tensorflow-gpu-1-11-0/">安装cuda9.0和cudnn7.4以及tensorflow-gpu==1.11.0</a><a class="next" href="/2018/11/10/TensorFlow学习手册（二）/">TensorFlow学习手册（二）</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'true' == true ? true : false;
var verify = 'true' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'dCzd3ozN5OlrLCT1FcrMc8D7-gzGzoHsz',
  appKey:'UmIi39gBjdh1Aria4ssShR31',
  placeholder:'欢迎讨论~',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/刷题记录/">刷题记录</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习笔记/">学习笔记</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术技巧/">技术技巧</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/课设记录/">课设记录</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/感知机/" style="font-size: 15px;">感知机</a> <a href="/tags/Jekyll/" style="font-size: 15px;">Jekyll</a> <a href="/tags/blog/" style="font-size: 15px;">blog</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/笔试/" style="font-size: 15px;">笔试</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/K近邻/" style="font-size: 15px;">K近邻</a> <a href="/tags/统计学习方法/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/显卡驱动配置/" style="font-size: 15px;">显卡驱动配置</a> <a href="/tags/ssh配置/" style="font-size: 15px;">ssh配置</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/HMM/" style="font-size: 15px;">HMM</a> <a href="/tags/CWS/" style="font-size: 15px;">CWS</a> <a href="/tags/osx/" style="font-size: 15px;">osx</a> <a href="/tags/ssr/" style="font-size: 15px;">ssr</a> <a href="/tags/proxychains/" style="font-size: 15px;">proxychains</a> <a href="/tags/朴素贝叶斯/" style="font-size: 15px;">朴素贝叶斯</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/11/17/TensorFlow学习手册（四）/">TensorFlow学习手册（四）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/16/通过proxychains在终端使用ss代理/">通过proxychains在终端使用ss代理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/安装cuda9-0和cudnn7-4以及tensorflow-gpu-1-11-0/">安装cuda9.0和cudnn7.4以及tensorflow-gpu==1.11.0</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/TensorFlow学习手册（三）/">TensorFlow学习手册（三）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/10/TensorFlow学习手册（二）/">TensorFlow学习手册（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/09/TensorFlow学习手册（一）/">TensorFlow学习手册（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/31/ubuntu16-04显卡驱动安装及环境配置/">Ubuntu16.04显卡驱动安装及环境配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/28/隐马尔可夫模型及分词上的实现/">隐马尔可夫模型及分词上的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/25/2019科大讯飞算法岗校招笔试/">2019科大讯飞算法岗校招笔试</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/19/决策树的原理及实现/">决策树的原理及实现</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.haomwei.com/technology/maupassant-hexo.html" title="博客模板" target="_blank">博客模板</a><ul></ul><a href="https://hyxxsfwy.github.io/2016/01/15/Hexo-Markdown-%E7%AE%80%E6%98%8E%E8%AF%AD%E6%B3%95%E6%89%8B%E5%86%8C/" title="HEXO markdown简明语法" target="_blank">HEXO markdown简明语法</a><ul></ul><a href="http://www.mohu.org/info/symbols/symbols.htm" title="mathjax使用的查阅手册" target="_blank">mathjax使用的查阅手册</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Netycc's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a><br/><span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
extensions: ["tex2jax.js"],
jax: ["input/TeX", "output/HTML-CSS"],
tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
},
    "HTML-CSS": { fonts: ["TeX"] }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>