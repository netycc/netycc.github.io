<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="用于日常笔记"><meta name="baidu-site-verification" content="31u13chEy5"><title>TensorFlow学习手册（二） | Netycc's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '83d60cd5e215de54f53db5b26853c623';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
  </script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">TensorFlow学习手册（二）</h1><a id="logo" href="/.">Netycc's blog</a><p class="description">每天进步一点点，吃吃喝喝的single dog.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/timeline"><i class="fa fa-history"> 时间线</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">TensorFlow学习手册（二）</h1><div class="post-meta">Nov 10, 2018<span> | </span><span class="category"><a href="/categories/学习笔记/">学习笔记</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 4.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 18</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" href="/2018/11/10/TensorFlow学习手册（二）/#vcomment"><span class="valine-comment-count" data-xid="/2018/11/10/TensorFlow学习手册（二）/"></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#深度学习与深层神经网络"><span class="toc-number">2.</span> <span class="toc-text">深度学习与深层神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性模型的局限性"><span class="toc-number">2.1.</span> <span class="toc-text">线性模型的局限性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#激活函数实现去线性化"><span class="toc-number">2.2.</span> <span class="toc-text">激活函数实现去线性化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多层网络解决异或运算"><span class="toc-number">2.3.</span> <span class="toc-text">多层网络解决异或运算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#损失函数定义"><span class="toc-number">3.</span> <span class="toc-text">损失函数定义</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#经典损失函数"><span class="toc-number">3.1.</span> <span class="toc-text">经典损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#分类问题"><span class="toc-number">3.1.1.</span> <span class="toc-text">分类问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#回归问题"><span class="toc-number">3.1.2.</span> <span class="toc-text">回归问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#自定义损失函数"><span class="toc-number">3.2.</span> <span class="toc-text">自定义损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络优化"><span class="toc-number">4.</span> <span class="toc-text">神经网络优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络进一步优化"><span class="toc-number">5.</span> <span class="toc-text">神经网络进一步优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#学习率的设置"><span class="toc-number">5.1.</span> <span class="toc-text">学习率的设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#过拟合问题（正则化项）"><span class="toc-number">5.2.</span> <span class="toc-text">过拟合问题（正则化项）</span></a></li></ol></li></ol></div></div><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;在本部分中，将从深度学习与深层神经网络概念介绍、如何设定神经网络的优化目标、更加详细的介绍神经网络的反向传播算法等方面来进一步介绍如何运用TensorFlow来构建神经网络。</p>
<h2 id="深度学习与深层神经网络"><a href="#深度学习与深层神经网络" class="headerlink" title="深度学习与深层神经网络"></a>深度学习与深层神经网络</h2><p>&emsp;&emsp;在维基百科中，深度学习的定义为“一类通过多层非线性变换对高复杂性数据建模算法的合集”。因为神经网络是实现“多层非线性变换”最常用的一种方法，所以在实际中基本可以认为深度学习就是深层神经网络的代名词。从以上可以看出，深度学习的两个重要特性——<strong>多层、非线性</strong></p>
<h3 id="线性模型的局限性"><a href="#线性模型的局限性" class="headerlink" title="线性模型的局限性"></a>线性模型的局限性</h3><p>&emsp;&emsp;在线性模型中，模型的输出为输入的加权和。假设一个模型的输出y和输入$x_i$满足以下关系，那么这个模型就是一个线性模型。$$y=\sum_i{w_ix_i}+b$$<br>&emsp;&emsp;其中$w_i,b\in R$为模型的参数。上面的公式就是一个线性变换，即便深层神经网络有着多层结构，也只是多个W进行矩阵乘法，与单层网络没有区别。只通过线性变换，<strong>任意层的全连接网络和单层神经网络模型的表达能力没有任何区别，而且他们都是线性模型</strong>。</p>
<h3 id="激活函数实现去线性化"><a href="#激活函数实现去线性化" class="headerlink" title="激活函数实现去线性化"></a>激活函数实现去线性化</h3><p>&emsp;&emsp;一般的线性神经元构成的模型都是线性模型，如果将每一个神经元的输出通过一个非线性函数，那么整个神经网络的模型也就不再是线性的了。这个非线性函数就是激活函数，下图显示了加入激活函数和偏置项之后的神经元结构。<br><img src="/2018/11/10/TensorFlow学习手册（二）/QQ20181110-132040.png" alt=""><br>&emsp;&emsp;以下公式给出了加上激活函数和偏置项后的前向传播算法的数学定义$$A_1=[a_{11},a_{12},a_{13}]=f(xW^{(1)}+b)$$<br>&emsp;&emsp;相对于之前的定义，新的公式增加了偏置项（bias），偏置项是神经网络中非常常用的一种结构；其次就是每个节点的取值不再是单纯的甲醛和。每个节点在加权和的基础上还做了一个<strong>非线性变换</strong>。下图显示了几种常用的非线性激活函数的函数图像。<br><img src="/2018/11/10/TensorFlow学习手册（二）/QQ20181110-133205.png" alt=""><br>&emsp;&emsp;目前TensorFlow提供了7种不同的非线性激活函数，<code>tf.nn.relu</code>、<code>tf.sigmoid</code>、<code>tf.tanh</code>是比较常用的几个，当然TensorFlow也支持使用自己定义的激活函数。以下代码展示了TensorFlow实现神经网络中的前向算法。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">a</span> = tf<span class="selector-class">.nn</span><span class="selector-class">.relu</span>(tf.matmul(x, w1) + biases1)</span><br><span class="line">y = tf<span class="selector-class">.nn</span><span class="selector-class">.relu</span>(tf.matmul(<span class="selector-tag">a</span>, w2) + biases2)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;TensorFlow可以很好地支持使用了激活函数和偏置项的神经网络。</p>
<h3 id="多层网络解决异或运算"><a href="#多层网络解决异或运算" class="headerlink" title="多层网络解决异或运算"></a>多层网络解决异或运算</h3><p>&emsp;&emsp;感知机可以简单的理解为单层的神经网络，这在我之前的博客里也<a href="https://netycc.com/2018/10/10/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/">实现</a>了。感知机会先将输入进行加权和，然后使用激活函数最后得到输出。这个结构就是一个没有隐藏层的神经网络。<br>&emsp;&emsp;深层神经网络其实是有<strong>组合特征提取</strong>的功能的，这个特性对于解决不易提取特征向量的问题有很大帮助，也是深度学习在这些问题上更加容易取得突破性进展的原因。</p>
<h2 id="损失函数定义"><a href="#损失函数定义" class="headerlink" title="损失函数定义"></a>损失函数定义</h2><p>&emsp;&emsp;神经网络的效果以及优化的目标是通过损失函数(loss function)来定义的。</p>
<h3 id="经典损失函数"><a href="#经典损失函数" class="headerlink" title="经典损失函数"></a>经典损失函数</h3><p>&emsp;&emsp;分类问题和回归问题是监督学习的两大种类。本节将会分别介绍分类问题和回归问题中使用到的经典损失函数。</p>
<h4 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h4><p>&emsp;&emsp;通过神经网络解决多分类问题最常用的方法是设置n个输出节点，其中n为类别的个数。对于每一个样例，神经网络可以得到一个n维数组作为输出结果。在理想情况下，如果一个样本输入类别k，那么这个类别所对应的输出节点的值应该为1，而其他节点的输出都为0.<br>&emsp;&emsp;判断一个输出的向量和期望向量有多接近有什么方法？交叉熵(cross entropy)是常用的评判方法之一，<strong>交叉熵</strong>刻画了两个概率分布之间的距离，它是分类问题中使用比较广的一种损失函数。<br>&emsp;&emsp;交叉熵是一个信息论中的概念，它原本是用来估算平均编码长度的。给定两个概率分布p和q，通过q来表示p的交叉熵为：$$H(p,q)=-\sum_x{p(x)log&ensp;{q(x)}}$$<br>&emsp;&emsp;注意交叉熵刻画的是<strong>两个概率分布之间的距离</strong>，然而神经网络的输出却不一定是一个概率分布。概率分布刻画了不同事件发生的概率。因此可以通过Softmax将神经网络的输出值转化为概率:$$softmax(y)_i=y_i`=\frac{e^{y_i}}{\sum_{j=1}^{n}{e^{y_j}}}$$<br>&emsp;&emsp;从以上公式可以看出，原始神经网络的输出被用作置信度来生成新的输出。这样就把神经网络的输出也变成了一个概率分布，从而可以通过交叉熵来计算预测的概率分布和真实答案的概率分布之间的距离了。<br>&emsp;&emsp;交叉熵函数<strong>不是对称的</strong>（$H(p,q)\ne H(q,p)$），它刻画的是通过概率分布q来表达概率分布p的困难程度。因为正确答案是希望得到的结果，所以当交叉熵作为神经网络的损失函数时，p代表的是正确答案，q代表的是预测值。<br>&emsp;&emsp;在TensorFlow中实现交叉熵代码为:<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = -<span class="keyword">tf</span>.reduce_mean(</span><br><span class="line">                    y_ * <span class="keyword">tf</span>.<span class="built_in">log</span>(<span class="keyword">tf</span>.clip_by_value(<span class="keyword">y</span>, <span class="number">1</span><span class="keyword">e</span>-<span class="number">10</span>, <span class="number">1.0</span>)))</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;其中y_代表正确结果，y代表预测结果，这一行代码包含了4个不同的TensorFlow运算。通过<code>tf.clip_by_value</code>函数可以将一个张量中的数值限制在一个范围内。如上就是限制在(1e-10,1.0)之内。<br>&emsp;&emsp;因为交叉熵一般会和softmax回归一起使用，所以TensorFlow对这两个功能进行了封装，并提供了<code>tf.nn.softmax_cross_entropy_with_logits</code>函数。比如可以直接通过以下代码来实现先softmax回归然后交叉熵的损失函数：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf<span class="selector-class">.nn</span><span class="selector-class">.softmax_cross_entropy_with_logits</span>(labels=y_,logits=y)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;其中y代表了原始神经网络的输出结果，而y_给出了标准答案。这样通过一个命令可以得到使用了Softmax回归之后的交叉熵。而在只有一个正确答案的分类问题中，TensorFlow提供了<code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>函数来进一步加速计算过程。</p>
<h4 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h4><p>&emsp;&emsp;与分类问题不同，回归问题解决的是对具体数值的预测。这些问题需要预测的不是一个事先定义好的类别，而是一个任意实数。解决回归问题的神经网络一般只有一个输出节点，这个节点的输出值就是预测值。对于回归问题，最常用的损失函数是均方误差（MSE）。它的定义如下：$$MSE(y,y’)=\frac{\sum_{i=1}^{n}{(y_i-y_i’)^2}}{n}$$<br>&emsp;&emsp;其中$y_i$为一个batch中第i个数据的正确答案，而$y_i’$为神经网络给出的预测值，以下代码展示了TensorFlow实现均方误差损失函数：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mse</span> = tf.reduce_mean(tf.square(y_ - y))</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;其中y代表了神经网络的输出答案，y_代表了标准答案。这里的减法运算符代表两个矩阵中对应元素的减法。</p>
<h3 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h3><p>&emsp;&emsp;在TensorFlow中也支持自定义损失函数，它可以使得神经网络优化的结果更加接近实际问题的需求。例如如下损失函数：<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">loss</span> = tf.reduce_sum(tf.where(tf.greater(<span class="built_in">v1</span>,<span class="built_in">v2</span>),</span><br><span class="line">                        (<span class="built_in">v1</span> - <span class="built_in">v2</span>) * a,(<span class="built_in">v2</span> - <span class="built_in">v1</span>) * <span class="keyword">b))</span></span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;以上代码用到了<code>tf.greater</code>和<code>tf.where</code>来实现选择操作。<code>tf.greater</code>的输入是两个张量，此函数会比较这两个输入张量中的每一个元素的大小，并返回比较结果。当<code>tf.greater</code>的输入张量维度不一样时，TensorFlow会进行类似numpy的广播操作的处理，<code>tf.where</code>函数有三个参数，第一个为选择条件根据，当选择条件为True时，<code>tf.where</code>函数会选择第二个参数中的值，否则使用第三个参数中的值。注意这两个操作都是元素级别进行。</p>
<h2 id="神经网络优化"><a href="#神经网络优化" class="headerlink" title="神经网络优化"></a>神经网络优化</h2><p>&emsp;&emsp;本部分将讨论通过<code>反向传播算法</code>和<code>梯度下降算法</code>调整神经网络中参数的取值。梯度下降法主要用于优化单个参数的取值，而反向传播算法给出了一个高效的方式在所有参数上使用梯度下降法，从而使神经网络模型在训练数据上的损失函数尽可能小。<br>&emsp;&emsp;反向传播算法是训练神经网络的核心算法，它可以根据定义好的损失函数优化神经网络中参数的取值，从而使神经网络模型在训练数据集上的损失函数达到一个较小值。神经网络模型中参数的优化过程直接决定了模型的质量，是使用神经网络时非常重要的一步。<br>&emsp;&emsp;假设用$\theta$表示神经网络中的参数，$J(\theta)$表示在给定的参数取值下，训练数据集上损失函数的大小，那么整个优化过程可以抽象为寻找一个参数$\theta$，使得$J(\theta)$最小。梯度下降算法会迭代式更新参数$\theta$，不断沿着梯度的反方向让参数朝着总损失更小的方向更新。<br>&emsp;&emsp;参数的梯度可以通过求偏导的方式计算，对于参数$\theta$，其梯度为$$\frac{\delta}{\delta\theta}J(\theta)$$<br>&emsp;&emsp;神经网络的优化过程可以分为两个阶段，第一个阶段先通过前向传播算法计算得到预测值，并将预测值和真实值做对比得出两者之间的差距。然后在第二个阶段通过反向传播算法计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数。<br>&emsp;&emsp;在训练神经网络时，参数的初始值会很大程度影响最后得到的结果。只有损失函数为凸函数时，梯度下降算法才能保证达到全局最优解。<br>&emsp;&emsp;除了不一定能达到全局最优，梯度下降算法的另一个问题就是<strong>计算时间太长</strong>，因为需要计算全部训练数据的损失函数。<br>&emsp;&emsp;为了加速训练算法，可以使用<code>随机梯度下降</code>算法。这个算法优化的不是在全部训练数据上的损失函数，而是在每一轮迭代中，随机优化某一条训练数据上的损失函数；但是随机梯度下降优化的神经网络可能无法达到局部最优。<br>&emsp;&emsp;为了综合梯度下降算法和随机梯度下降算法的优缺点，在实际应用中一般采用这两个算法的折中——每次计算一小部分训练数据的损失函数。这一小部分被称之为<code>**batch**</code>。通过矩阵运算，每次在一个<strong>batch</strong>上优化神经网络的参数并不会比单个数据慢太多。另一方面，每次使用一个<strong>batch</strong>可以大大减小收敛所需要的迭代次数，同时可以使收敛的结果更加接近梯度下降的效果。以下代码给出了在TensorFlow中如何实现神经网络的训练过程。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">batch_size = n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次读取一小部分作为当前的训练数据来执行反向传播算法。</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(batch_size, 2),name=<span class="string">"x_input"</span>)</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(batch_size, 1),name=<span class="string">"y_input"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络结构和优化算法</span></span><br><span class="line">loss = ......</span><br><span class="line">train_step = tf.train.AdamOptimizer(0.001).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练神经网络</span></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># 迭代的更新参数</span></span><br><span class="line">    current_X, current_Y = ...</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: current_X, y_: current_Y&#125;)</span><br></pre></td></tr></table></figure></p>
<h2 id="神经网络进一步优化"><a href="#神经网络进一步优化" class="headerlink" title="神经网络进一步优化"></a>神经网络进一步优化</h2><p>&emsp;&emsp;本部分将介绍神经网络优化过程中的可能遇到的问题，比如设置梯度下降法中的<strong>学习率</strong>、<strong>过拟合问题</strong>、<strong>滑动平均模型</strong>。</p>
<h3 id="学习率的设置"><a href="#学习率的设置" class="headerlink" title="学习率的设置"></a>学习率的设置</h3><p>&emsp;&emsp;<strong>学习率决定了参数每次更新的幅度。如果幅度过大，那么可能导致参数在极优值的两侧来回移动。</strong><br>&emsp;&emsp;学习率既不能过大，也不能过小。为了解决设定学习率的问题，TensorFlow提供了一种更加灵活的学习率设置方法——指数衰减法。<code>tf.train.exponential_decay</code>函数实现了指数衰减学习率。通过这个函数，可以先使用较大的学习率获得一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后期更加稳定。<code>exponential_decay</code>函数会指数级地减小学习率，它实现了以下代码的功能：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># decayed_learning_rate为每一轮batch优化时使用的学习率，learning_rate为事先规定的初始学习率</span></span><br><span class="line"><span class="comment"># decay_rate为衰减系数，decay_steps衰减速度</span></span><br><span class="line"><span class="attr">decayed_learning_rate</span> = learning_rate * decay_rate ^ (global_step / decay_steps)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;在<code>tf.train.exponential_decay</code>函数中提供了<code>staircase</code>这一参数，默认时为False，此时学习率为连续衰减形式，设置为True时为阶梯状衰减学习率。以下就是一段代码示范如何在TensorFlow中使用<code>tf.train.exponential_decay</code>函数：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过exponential_decay函数生成学习率</span></span><br><span class="line">learning_rate = tf.train.exponential_decay(0.1, global_step, 100, 0.96, staircase=True)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用指数衰减的学习率。在minimize函数中传入global_step将自动更新</span></span><br><span class="line"><span class="comment"># global_step参数，从而使得学习率也得到相应更新。</span></span><br><span class="line">learning_step = tf.train.GradientDescentOptimizer(learning_rate)\</span><br><span class="line">                    .minimize(myloss,global_step=global_step)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;以上代码设定了初始学习率为0.1，因为制定了<code>staircase=True</code>，所以每训练100轮后学习率乘以0.96。若<code>staircase=False</code>，则训练每条数据的时候学习率都会乘以0.96。</p>
<h3 id="过拟合问题（正则化项）"><a href="#过拟合问题（正则化项）" class="headerlink" title="过拟合问题（正则化项）"></a>过拟合问题（正则化项）</h3><p>&emsp;&emsp;之前的博客中也多次讲到了过拟合问题，一般是将loss函数后加上正则化项，常用的刻画模型复杂度的正则化项有两种，分别为L1和L2，这里就不再赘述其数学原理，在TensorFlow中，可以实现如下一个简单的带L2正则化的损失函数定义：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置初始参数，2行1列，标准差为1，随机种子1，正态分布</span></span><br><span class="line"><span class="attr">w</span> = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 设置w为从x到y中间的隐藏层</span></span><br><span class="line"><span class="attr">y</span> = tf.matmul(x,w)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置loss为y_与y的平方差均值加上w的L2范数*lambda系数,tf.square函数是为求平方</span></span><br><span class="line"><span class="attr">loss</span> = tf.reduce_mean(tf.square(y_ - y)) + tf.contrib.layers.l2_regularizer(lambda)(w)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;类似的，<code>tf.contrib.layers.l1_regularizer</code>函数可以计算L1正则化项的值。但是当网络结构复杂之后定义网络结构的部分和计算损失函数的部分可能不在同一个函数中，这样通过变量这种方式计算损失函数就不行了，在TensorFlow中可以是集合（collection），它可以在一个计算图中保存一组实体，下面代码实现了计算一个5层神经网络带L2正则化的损失函数的计算方法：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</span><br><span class="line"><span class="built_in">import</span> numpy as np</span><br><span class="line"></span><br><span class="line"><span class="attr">dataset_size</span> = <span class="number">200</span></span><br><span class="line"><span class="attr">data</span> = []</span><br><span class="line"><span class="attr">label</span> = []</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以原点为圆心，半径为1的圆把散点划分成红蓝两部分，并加入随机噪音。</span></span><br><span class="line">for i <span class="keyword">in</span> range(dataset_size):</span><br><span class="line">    <span class="attr">x1</span> = np.random.uniform(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="attr">x2</span> = np.random.uniform(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> x1 ** <span class="number">2</span> + x2 ** <span class="number">2</span> &lt;= <span class="number">1</span>:</span><br><span class="line">        data.append([np.random.normal(x1, <span class="number">0.1</span>), np.random.normal(x2, <span class="number">0.1</span>)])</span><br><span class="line">        label.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data.append([np.random.normal(x1, <span class="number">0.1</span>), np.random.normal(x2, <span class="number">0.1</span>)])</span><br><span class="line">        label.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.hstack()函数是将list在水平方向上平铺，然后.reshape将维度更改</span></span><br><span class="line"><span class="attr">data</span> = np.hstack(data).reshape(-<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="attr">label</span> = np.hstack(label).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 绘制原散点图</span></span><br><span class="line"><span class="comment"># plt.scatter(data[:, 0], data[:, 1], c=np.squeeze(label),</span></span><br><span class="line"><span class="comment">#             cmap="RdBu", vmin=-0.2, vmax=1.2, edgecolor="white")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置layer参数，入口为矩阵维度和正则化项系数</span></span><br><span class="line">def get_weight(shape, var_lambda):</span><br><span class="line">    <span class="comment"># 声明一层网络</span></span><br><span class="line">    <span class="attr">w</span> = tf.Variable(tf.random_normal(shape), <span class="attr">dtype=tf.float32)</span></span><br><span class="line">    <span class="comment"># 添加L2正则化项到losses集合中</span></span><br><span class="line">    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(var_lambda)(w))</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明batch</span></span><br><span class="line"><span class="attr">x</span> = tf.placeholder(tf.float32, <span class="attr">shape=(None,</span> <span class="number">2</span>))</span><br><span class="line"><span class="attr">y_</span> = tf.placeholder(tf.float32, <span class="attr">shape=(None,</span> <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每层节点的个数</span></span><br><span class="line"><span class="attr">layer_dimension</span> = [<span class="number">2</span>,<span class="number">10</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 声明网络结构层数</span></span><br><span class="line"><span class="attr">n_layers</span> = len(layer_dimension)</span><br><span class="line"><span class="comment"># 前一个layer</span></span><br><span class="line"><span class="attr">cur_layer</span> = x</span><br><span class="line"><span class="comment"># 前一个layer的节点数</span></span><br><span class="line"><span class="attr">in_dimension</span> = layer_dimension[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环生成网络结构，输入层X[None, 2]，隐藏层W1[2, 10]、W2[10, 5]、W3[5, 3]，输出层Y[None, 1]</span></span><br><span class="line">for i <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">    <span class="attr">out_dimension</span> = layer_dimension[i]  <span class="comment"># 该layer输出的节点数量</span></span><br><span class="line">    <span class="attr">weight</span> = get_weight([in_dimension, out_dimension], <span class="number">0.003</span>)   <span class="comment"># 设置layer，并且正则化项系数为0.003</span></span><br><span class="line">    <span class="attr">bias</span> = tf.Variable(tf.constant(<span class="number">0.1</span>, <span class="attr">shape=[out_dimension]))</span> <span class="comment"># 设置偏执项，维度按照输出维度提供</span></span><br><span class="line">    <span class="attr">cur_layer</span> = tf.nn.relu(tf.matmul(cur_layer, weight) + bias)  <span class="comment"># 该层的输出为relu(w_i*x+b)</span></span><br><span class="line">    <span class="attr">in_dimension</span> = layer_dimension[i]   <span class="comment"># 设置下层的输入节点数量</span></span><br><span class="line"></span><br><span class="line"><span class="attr">y=</span> cur_layer    <span class="comment"># 最后的输出层</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数的定义。</span></span><br><span class="line"><span class="attr">mse_loss</span> = tf.reduce_sum(tf.pow(y_ - y, <span class="number">2</span>)) / dataset_size</span><br><span class="line">tf.add_to_collection('losses', mse_loss)    <span class="comment"># 正常的损失函数加入losses集合中</span></span><br><span class="line"><span class="attr">loss</span> = tf.add_n(tf.get_collection('losses'))    <span class="comment"># 获得losses集合所有项并求和</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练的目标函数loss，训练次数及训练模型</span></span><br><span class="line"><span class="attr">train_op</span> = tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"><span class="attr">TRAINING_STEPS</span> = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    for i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">        sess.run(train_op, <span class="attr">feed_dict=&#123;x:</span> data, y_: label&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">1000</span> - <span class="number">1</span>:</span><br><span class="line">            print(<span class="string">"After %d steps, loss: %f"</span> % (i, sess.run(loss, <span class="attr">feed_dict=&#123;x:</span> data, y_: label&#125;)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 画出训练后的分割曲线</span></span><br><span class="line">    xx, <span class="attr">yy</span> = np.mgrid[-<span class="number">1</span>:<span class="number">1</span>:.<span class="number">01</span>, <span class="number">0</span>:<span class="number">2</span>:.<span class="number">01</span>]    <span class="comment"># 分别创建两个密集型网格，起始：终点：步长</span></span><br><span class="line">    print(<span class="string">"xx.ravel()"</span>+<span class="string">"*"</span>*<span class="number">100</span>,'\n',xx.ravel(),<span class="string">"yy.ravel()"</span>+'*'*<span class="number">100</span>,yy.ravel())</span><br><span class="line">    <span class="attr">grid</span> = np.c_[xx.ravel(), yy.ravel()]    <span class="comment"># 形成多个n*2的array，每组里是(x,y)</span></span><br><span class="line">                                            <span class="comment"># np.c_()函数是将两个array按照行连接起来，np.r_()是按照列连接</span></span><br><span class="line">                                            <span class="comment"># array.ravel()函数是将array变成一维，然后返回一维数据</span></span><br><span class="line">    <span class="attr">probs</span> = sess.run(y, <span class="attr">feed_dict=&#123;x:grid&#125;)</span> <span class="comment"># 将x作为输入去运行y这个计算图</span></span><br><span class="line">    <span class="attr">probs</span> = probs.reshape(xx.shape)         <span class="comment"># 按照网格重新排列</span></span><br><span class="line"></span><br><span class="line">plt.scatter(data[:,<span class="number">0</span>], data[:,<span class="number">1</span>], <span class="attr">c=np.squeeze(label),</span></span><br><span class="line">           <span class="attr">cmap="RdBu",</span> <span class="attr">vmin=-.2,</span> <span class="attr">vmax=1.2,</span> <span class="attr">edgecolor="white")</span></span><br><span class="line"><span class="comment"># 填充网格中的等高线</span></span><br><span class="line">plt.contour(xx, yy, probs, <span class="attr">levels=[.5],</span> <span class="attr">cmap="Greys",</span> <span class="attr">vmin=0,</span> <span class="attr">vmax=.1)</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/2018/11/10/TensorFlow学习手册（二）/jieguo.png" alt="分类结果"></p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>netycc</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/11/10/TensorFlow学习手册（二）/">https://netycc.com/2018/11/10/TensorFlow学习手册（二）/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://netycc.com/2018/11/10/TensorFlow学习手册（二）/" data-id="cjpavht56000uzm82fbd4xgws" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACHUlEQVR42u3azbKCMAwGUN//pblb70LMlyAztIeVI9hyXGTy09erfB1v1/s36d30mYsvDAyMxzKO06uyZX2Fa3+FgYGxG+NTBDv//Cng1vH1fb+8MwYGBkb4QsH25fQRAwMDo8JIN0jD600BFwMD47GMejFZSQrTlDFNKDEwMHZj1Lvu93/+yXwDAwPjUYxesz5N6X4xbPi3GgYGxtKM3ghzUnxWWmxp6omBgbE2I13o2pHAeUlcKVYxMDD2YVSWmLS6JiuXgj4GBsbSjDTw9Rr66XA0bf9hYGDswKinZen2lbv1P2g0j8XAwFiOMd+s90yv6YaBgbE2o1fETg5nXLvaqBbHwMB4CKMecHtBsxLEe69bKmIxMDCWYJxvWS9QJ422XlqJgYGxDyOtBHuBNW20TVbGwMBYlTEJtWlLLj1gEWS4GBgYizLSsWWl9J0fGkt3x8DAWJsR9+TC7SctuXjkgIGBsQ1jMrxMU7re0YpR7xADA+PhjMmhrrQ9N0krvwRcDAyMDRhp+75+bKI+Wrisa4iBgbEQo9fe6h2qiA9PnD5fmsdiYGA8nHGEV2+EWW+9NX+FgYGxNKM3aEyD5iTtq4AxMDB2YNSDbCU1rJPSgBvU4hgYGIsy0pJ10hS76cwIBgYGRpgOBqHz2mEABgbGloz0CEWahqY8DAyMfRiTIjZttNW/Cdp/GBgYSzMmpWMK6zHSoIyBgbEQ4w/YrY50fN6ruAAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/深度学习/">深度学习</a><a href="/tags/TensorFlow/">TensorFlow</a></div><div class="post-nav"><a class="pre" href="/2018/11/14/TensorFlow学习手册（三）/">TensorFlow学习手册（三）</a><a class="next" href="/2018/11/09/TensorFlow学习手册（一）/">TensorFlow学习手册（一）</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'true' == true ? true : false;
var verify = 'true' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'dCzd3ozN5OlrLCT1FcrMc8D7-gzGzoHsz',
  appKey:'UmIi39gBjdh1Aria4ssShR31',
  placeholder:'欢迎讨论~',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/刷题记录/">刷题记录</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习笔记/">学习笔记</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术技巧/">技术技巧</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/课设记录/">课设记录</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/tags/Jekyll/" style="font-size: 15px;">Jekyll</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/统计学习方法/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/感知机/" style="font-size: 15px;">感知机</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/笔试/" style="font-size: 15px;">笔试</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/K近邻/" style="font-size: 15px;">K近邻</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/ssh配置/" style="font-size: 15px;">ssh配置</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/blog/" style="font-size: 15px;">blog</a> <a href="/tags/Bert/" style="font-size: 15px;">Bert</a> <a href="/tags/WordEmbedding/" style="font-size: 15px;">WordEmbedding</a> <a href="/tags/cosine/" style="font-size: 15px;">cosine</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/显卡驱动配置/" style="font-size: 15px;">显卡驱动配置</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/tags/osx/" style="font-size: 15px;">osx</a> <a href="/tags/ssr/" style="font-size: 15px;">ssr</a> <a href="/tags/proxychains/" style="font-size: 15px;">proxychains</a> <a href="/tags/朴素贝叶斯/" style="font-size: 15px;">朴素贝叶斯</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/HMM/" style="font-size: 15px;">HMM</a> <a href="/tags/CWS/" style="font-size: 15px;">CWS</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/12/05/利用bert构建词向量并计算相似度/">利用Bert构建词向量并计算相似度</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/04/正则表达式总结/">正则表达式记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/02/LeetCode-1-3/">[LeetCode]Problem 1-3</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/17/TensorFlow学习手册（四）/">TensorFlow学习手册（四）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/16/通过proxychains在终端使用ss代理/">通过proxychains在终端使用ss代理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/安装cuda9-0和cudnn7-4以及tensorflow-gpu-1-11-0/">安装cuda9.0和cudnn7.4以及tensorflow-gpu==1.11.0</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/TensorFlow学习手册（三）/">TensorFlow学习手册（三）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/10/TensorFlow学习手册（二）/">TensorFlow学习手册（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/09/TensorFlow学习手册（一）/">TensorFlow学习手册（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/31/ubuntu16-04显卡驱动安装及环境配置/">Ubuntu16.04显卡驱动安装及环境配置</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.haomwei.com/technology/maupassant-hexo.html" title="博客模板" target="_blank">博客模板</a><ul></ul><a href="https://hyxxsfwy.github.io/2016/01/15/Hexo-Markdown-%E7%AE%80%E6%98%8E%E8%AF%AD%E6%B3%95%E6%89%8B%E5%86%8C/" title="HEXO markdown简明语法" target="_blank">HEXO markdown简明语法</a><ul></ul><a href="http://www.mohu.org/info/symbols/symbols.htm" title="mathjax使用的查阅手册" target="_blank">mathjax使用的查阅手册</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Netycc's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a><br/><span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
extensions: ["tex2jax.js"],
jax: ["input/TeX", "output/HTML-CSS"],
tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
},
    "HTML-CSS": { fonts: ["TeX"] }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>