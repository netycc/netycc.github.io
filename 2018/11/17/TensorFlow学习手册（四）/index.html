<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="用于日常笔记"><meta name="baidu-site-verification" content="31u13chEy5"><title>TensorFlow学习手册（四） | Netycc's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '83d60cd5e215de54f53db5b26853c623';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
  </script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">TensorFlow学习手册（四）</h1><a id="logo" href="/.">Netycc's blog</a><p class="description">每天进步一点点，吃吃喝喝的single dog.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/timeline"><i class="fa fa-history"> 时间线</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">TensorFlow学习手册（四）</h1><div class="post-meta">Nov 17, 2018<span> | </span><span class="category"><a href="/categories/学习笔记/">学习笔记</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 7</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" href="/2018/11/17/TensorFlow学习手册（四）/#vcomment"><span class="valine-comment-count" data-xid="/2018/11/17/TensorFlow学习手册（四）/"></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#循环神经网络（CNN）"><span class="toc-number">2.</span> <span class="toc-text">循环神经网络（CNN）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#循环神经网络的经典形式"><span class="toc-number">2.1.</span> <span class="toc-text">循环神经网络的经典形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#循环神经网络的展开形式"><span class="toc-number">2.2.</span> <span class="toc-text">循环神经网络的展开形式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#长短期记忆网络（LSTM）"><span class="toc-number">3.</span> <span class="toc-text">长短期记忆网络（LSTM）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#循环神经网络的变种"><span class="toc-number">4.</span> <span class="toc-text">循环神经网络的变种</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#双向循环神经网络"><span class="toc-number">4.1.</span> <span class="toc-text">双向循环神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#深层循环神经网络"><span class="toc-number">4.2.</span> <span class="toc-text">深层循环神经网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#循环神经网络样例程序"><span class="toc-number">5.</span> <span class="toc-text">循环神经网络样例程序</span></a></li></ol></div></div><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;本节将描述一下循环神经网络与长短期记忆网络两种模型。</p>
<h2 id="循环神经网络（CNN）"><a href="#循环神经网络（CNN）" class="headerlink" title="循环神经网络（CNN）"></a>循环神经网络（CNN）</h2><p>&emsp;&emsp;循环神经网络主要用于处理和预测序列化数据的。循环神经网络的来源就是为了刻画一个序列当前的输出和之前信息的关系，它的隐藏层之间的结点是有连接的，隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出。</p>
<h3 id="循环神经网络的经典形式"><a href="#循环神经网络的经典形式" class="headerlink" title="循环神经网络的经典形式"></a>循环神经网络的经典形式</h3><p><img src="/2018/11/17/TensorFlow学习手册（四）/WX20181117-145027.png" alt="图1 循环神经网络经典结构示意图"></p>
<p>&emsp;&emsp;图1展示的是一个典型的循环神经网络，在每一时刻t，循环神经网络会针对该时刻的输入结合当前模型的状态给出一个输出，并更新模型的状态。在每一个时刻，循环神经网络的模块A在读取了$x_t$和$h_{t-1}$之后会产生本时刻的输出$O_t$。<strong>由于模块A中的运算和变量在不同时刻是相同的</strong>，因此循环神经网络理论上可以被看作是同一神经网络结构被无限复制的结果。循环神经网络是在不同时间位置共享参数，从而能够使用有限的参数处理任意长度的序列。</p>
<h3 id="循环神经网络的展开形式"><a href="#循环神经网络的展开形式" class="headerlink" title="循环神经网络的展开形式"></a>循环神经网络的展开形式</h3><p>&emsp;&emsp;如果将完整的输入输出序列展开，就可以看到如图2所示的结构。从下图可以看出循环神经网络当前状态$h_t$和当前的输入$x_t$共同决定的。在时刻t，状态$h_{t-1}$浓缩了前面序列$x_0,x_1,…,x_{t-1}$的信息，用于作为输出$o_t$的参考。由于序列的长度可以无限延长，维度有限的h状态不可能将序列的全部信息都保存下来，因此模型必须学习只保留与后面任务$o_t,o_{t+1},…$相关的最重要的信息。</p>
<p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_2.jpg" alt="图2 循环神经网络按时间展开后的结构"></p>
<p>&emsp;&emsp;循环神经网络对长度为N的序列展开之后，可以视为一个<strong>有N个中间层的前馈神经网络</strong>。这个前馈神经网络没有训练链接，可以使用反向传播算法进行训练。循环神经网络要求每一个时刻都有一个输入，但是不一定每个时刻都需要有输出。</p>
<p>&emsp;&emsp;在循环神经网络中，被复制多次的结构称为循环体。图3是一个最简单的循环体结构。循环神经网络中的状态是通过一个向量来表示的，这个向量的维度也称为循环神经网络隐藏层的大小，假设其为n。从下图可以看出循环体中的神经网络的输入有两部分，一部分为上一时刻的状态，一部分为当前时刻的输入样本。对于时间序列数据来说，每一时刻的输入样例可以是该时刻的数值或者是单词向量。</p>
<p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_3_gaitubao_com_289x278.jpg" alt="图3 使用单层全连接神经网络作为循环体的循环神经网络结构图"></p>
<p>&emsp;&emsp;循环神经网络唯一的区别在于因为它每个时刻都有一个输出，所以循环神经网络的总损失为所有时刻（或者部分时刻上）的损失总和。</p>
<h2 id="长短期记忆网络（LSTM）"><a href="#长短期记忆网络（LSTM）" class="headerlink" title="长短期记忆网络（LSTM）"></a>长短期记忆网络（LSTM）</h2><p>&emsp;&emsp;与单一tanh循环体结构不同，LSTM是一种拥有三个“门”结构的特殊网络结构，如图4所示。为了使循环神经网络更有效的保存长期记忆，“遗忘门”和“输入门”至关重要，它们是LSTM结构的核心。“遗忘门”会根据当前的输入$x_t$和上一时刻输出$h_{t-1}$决定哪一部分记忆需要被遗忘。“输入门”会根据$x_t$和$h_{t-1}$决定哪些信息加入到状态$c_{t-1}$中生成新的状态$c_t$。</p>
<p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_4.jpg" alt="图4 LSTM单元结构示意图"></p>
<p>&emsp;&emsp;图5是用流程图形式表示了LSTM单元的细节。</p>
<p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_5.jpg" alt="图5 LSTM单元细节图"></p>
<h2 id="循环神经网络的变种"><a href="#循环神经网络的变种" class="headerlink" title="循环神经网络的变种"></a>循环神经网络的变种</h2><h3 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h3><p>&emsp;&emsp;在之前介绍的经典的循环神经网络中，状态的传输都是从前往后单向的，然而在有些问题中，当前时刻的输出不仅和之前的状态有关，也和之后的状态有关。双向循环神经网络是由两个独立的循环神经网络叠加在一起组成的。输出由这两个循环神经网络的输出拼接而成。如下图所示：</p>
<p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_6.jpg" alt="图6 双向循环神经网络"></p>
<h3 id="深层循环神经网络"><a href="#深层循环神经网络" class="headerlink" title="深层循环神经网络"></a>深层循环神经网络</h3><p>&emsp;&emsp;深层循环神经网络是循环神经网络的另外一种变种。为了增强模型的表达能力，可以在网络中设置多个循环层，将每层循环网络的输出传给下一层进行处理。</p>
<p><img src="/2018/11/17/TensorFlow学习手册（四）/新文档 2018-11-17 14.20.45_7.jpg" alt="图7 深层循环神经网络"></p>
<h2 id="循环神经网络样例程序"><a href="#循环神经网络样例程序" class="headerlink" title="循环神经网络样例程序"></a>循环神经网络样例程序</h2><p>&emsp;&emsp;以时序预测为例，利用循环神经网络实现对函数sin x取值的预测。在以下程序中每隔SAMPLE_ITERVAL对sin函数进行一次采样，采样得到的序列就是sin函数离散化之后的结果。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Author   : yechenchen</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/16 8:44 PM</span></span><br><span class="line"><span class="comment"># @File     : 循环神经网络样例.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm Community Edition</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> numpy as np</span><br><span class="line"><span class="built_in">import</span> tensorflow as tf</span><br><span class="line"></span><br><span class="line"><span class="built_in">import</span> matplotlib as mpl</span><br><span class="line">mpl.use(<span class="string">"Agg"</span>)</span><br><span class="line">from matplotlib <span class="built_in">import</span> pyplot as plt</span><br><span class="line"></span><br><span class="line"><span class="attr">HIDDEN_SIZE=30</span>  <span class="comment">#LSTM中的隐藏节点数</span></span><br><span class="line"><span class="attr">NUM_LAYERS=2</span>    <span class="comment">#LSTM的层数</span></span><br><span class="line"></span><br><span class="line"><span class="attr">TIMESTEPS=10</span>    <span class="comment">#循环神经网络的训练长度</span></span><br><span class="line"><span class="attr">TRAINING_STEPS=10000</span>    <span class="comment">#训练轮数</span></span><br><span class="line"><span class="attr">BATCH_SIZE=32</span>   <span class="comment">#batch大小</span></span><br><span class="line"></span><br><span class="line"><span class="attr">TRAINING_EXAMPLES=10000</span> <span class="comment">#训练数据个数</span></span><br><span class="line"><span class="attr">TESTING_EXAMPLES=1000</span>   <span class="comment">#测试数据个数</span></span><br><span class="line"><span class="attr">SAMPLE_GAP=0.01</span>         <span class="comment">#采样间隔</span></span><br><span class="line"></span><br><span class="line">def generate_data(seq):</span><br><span class="line">    <span class="attr">X=[]</span></span><br><span class="line">    <span class="attr">y=[]</span></span><br><span class="line"></span><br><span class="line">    for i <span class="keyword">in</span> range(len(seq)-TIMESTEPS):</span><br><span class="line">        X.append([seq[i:i+TIMESTEPS]])  <span class="comment">#i:i+TIMESTEPS这组数据作为训练数据</span></span><br><span class="line">        y.append([seq[i+TIMESTEPS]])    <span class="comment">#为了预测第i+TIMESTEPS这一数据</span></span><br><span class="line">    return np.array(X,<span class="attr">dtype=np.float32),np.array(y,dtype=np.float32)</span></span><br><span class="line"></span><br><span class="line">def lstm_model(X,y,is_training):</span><br><span class="line">    <span class="comment"># 多层LSTM</span></span><br><span class="line">    <span class="attr">cell</span> = tf.nn.rnn_cell.MultiRNNCell([</span><br><span class="line">        tf.nn.rnn_cell.LSTMCell(HIDDEN_SIZE)   for _ <span class="keyword">in</span> range(NUM_LAYERS)])</span><br><span class="line"></span><br><span class="line">    outputs,<span class="attr">_=tf.nn.dynamic_rnn(cell,X,dtype=tf.float32)</span></span><br><span class="line">    <span class="attr">output=outputs[:,-1,:]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对LSTM网络的输出再加一层全连接层并计算损失</span></span><br><span class="line">    <span class="attr">predictions</span> = tf.contrib.layers.fully_connected(output,<span class="number">1</span>,<span class="attr">activation_fn=None)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> not is_training:</span><br><span class="line">        return predictions,None,None</span><br><span class="line"></span><br><span class="line">    <span class="attr">loss=tf.losses.mean_squared_error(labels=y,predictions=predictions)</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">train_op=tf.contrib.layers.optimize_loss(</span></span><br><span class="line">        loss,tf.train.get_global_step(),<span class="attr">optimizer="Adagrad",learning_rate=0.1)</span></span><br><span class="line"></span><br><span class="line">    return predictions,loss,train_op</span><br><span class="line"></span><br><span class="line">def train(sess,train_X,train_y):</span><br><span class="line">    <span class="attr">ds=tf.data.Dataset.from_tensor_slices((train_X,train_y))</span></span><br><span class="line">    <span class="attr">ds=ds.repeat().shuffle(1000).batch(BATCH_SIZE)</span></span><br><span class="line">    X,<span class="attr">y=ds.make_one_shot_iterator().get_next()</span></span><br><span class="line">    <span class="comment"># 调用模型</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>):</span><br><span class="line">        predictions,loss,<span class="attr">train_op=lstm_model(X,y,True)</span></span><br><span class="line"></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    for i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">        _,<span class="attr">l=sess.run([train_op,loss])</span></span><br><span class="line">        <span class="keyword">if</span> i %<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"train step:"</span>+str(i),<span class="string">", loss:"</span>+str(l))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def run_eval(sess,test_X,test_y):</span><br><span class="line">    <span class="attr">ds=tf.data.Dataset.from_tensor_slices((test_X,test_y))</span></span><br><span class="line">    <span class="attr">ds=ds.batch(1)</span></span><br><span class="line">    X,<span class="attr">y=ds.make_one_shot_iterator().get_next()</span></span><br><span class="line">    <span class="comment"># 调用模型计算结果</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>,<span class="attr">reuse=True):</span></span><br><span class="line">        prediction,_,<span class="attr">_=lstm_model(X,[0.0],False)</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">predictions=[]</span></span><br><span class="line">    <span class="attr">labels=[]</span></span><br><span class="line">    for i <span class="keyword">in</span> range(TESTING_EXAMPLES):</span><br><span class="line">        p,<span class="attr">l=sess.run([prediction,y])</span></span><br><span class="line">        predictions.append(p)</span><br><span class="line">        labels.append(l)</span><br><span class="line"></span><br><span class="line">    <span class="attr">predictions=np.array(predictions).squeeze()</span></span><br><span class="line">    <span class="attr">labels=np.array(labels).squeeze()</span></span><br><span class="line">    <span class="attr">rmse=np.sqrt(((predictions-labels)**2).mean(axis=0))</span></span><br><span class="line">    print(<span class="string">"Mean Square Error is :%.2f"</span>%rmse)</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(predictions,<span class="attr">label="predictions")</span></span><br><span class="line">    plt.plot(labels,<span class="attr">label='real_sin')</span></span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attr">__name__</span> == '__main__':</span><br><span class="line">    <span class="attr">test_start=(TRAINING_EXAMPLES+TIMESTEPS)*SAMPLE_GAP</span></span><br><span class="line">    <span class="attr">test_end=test_start+(TESTING_EXAMPLES+TIMESTEPS)*SAMPLE_GAP</span></span><br><span class="line">    train_X,<span class="attr">train_y=generate_data(np.sin(np.linspace(</span></span><br><span class="line">        <span class="number">0</span>,test_start,TRAINING_EXAMPLES+TIMESTEPS,<span class="attr">dtype=np.float32</span></span><br><span class="line">    )))</span><br><span class="line">    test_X,<span class="attr">test_y=generate_data(np.sin(np.linspace(</span></span><br><span class="line">        test_start,test_end,TESTING_EXAMPLES+TIMESTEPS,<span class="attr">dtype=np.float32</span></span><br><span class="line">    )))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() as sess:</span><br><span class="line">        train(sess,train_X,train_y)</span><br><span class="line">        run_eval(sess,test_X,test_y)</span><br></pre></td></tr></table></figure></p>
<p><img src="/2018/11/17/TensorFlow学习手册（四）/Figure_1.png" alt="预测结果"></p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>netycc</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/11/17/TensorFlow学习手册（四）/">https://netycc.com/2018/11/17/TensorFlow学习手册（四）/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://netycc.com/2018/11/17/TensorFlow学习手册（四）/" data-id="cjol64zcf000mz382h4nxwo18" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACL0lEQVR42u3awY7DIAxF0f7/T2e2laaQ+wypClxWVUITThYWxn698LjeBp/5Pv//79Yc8pbikCFDxrKMqzv6c/rL4rz07oe3yJAh4wBG68WtOSSkko+Vvrd5XYYMGTIayxrB80AvQ4YMGU8zeHj96YArQ4aMH2BMC3wAQO4+mIvLkCFjQQY/df/+70fqGzJkyFiKcYUjDawjc4JVyZAhY2sGD3Dp9nFWW0bQ/CFDhoxNGbUEkh+Z1TaF/GgvOLGTIUPGsgx+6J8e9/M2i/TorblOGTJkbMrggXLC18KwtFAqQ4aMMxlpYWCkJMl3ekG3iAwZMrZjkLDI76ZtFiRk37BlyJCxNaNWbhw5aCMfqNbqIUOGjF0ZtbA4tyQ5ngbLkCHjBEb6uFkpaP+ZwTZRhgwZWzP4otOF8uP+WrOFDBkyTmPUNmojwbq2KQwwMmTI2I5x4UHYs9rF0mM4GTJk7Mrg5cm5SWmaEt+8UYYMGVsz0gXVtn21Amd/fry3lSFDxuIM3jDBr/AUlBc7bz6WDBkyjmGMLK7/L94OWysVyJAh4wRG2rzFl5UWOIOTQhkyZBzAuMLBN3b9J/AnoyRZhgwZWzPSMJfCRpLbWkOGDBkydmWQIMsT1FpRM90+frgrQ4aMAxhpUZPH77RIOa2+IUOGjOMZ46EzDak3VBkyZMgopaz8+bwto/lpZMiQcQCjdpTPr6cf5cFcXIYMGQsyRlLHdMPHF11sDpMhQ8ZujD8+GQPxl3mSDwAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/TensorFlow/">TensorFlow</a><a href="/tags/RNN/">RNN</a><a href="/tags/LSTM/">LSTM</a></div><div class="post-nav"><a class="next" href="/2018/11/16/通过proxychains在终端使用ss代理/">通过proxychains在终端使用ss代理</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'true' == true ? true : false;
var verify = 'true' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'dCzd3ozN5OlrLCT1FcrMc8D7-gzGzoHsz',
  appKey:'UmIi39gBjdh1Aria4ssShR31',
  placeholder:'欢迎讨论~',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/刷题记录/">刷题记录</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习笔记/">学习笔记</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术技巧/">技术技巧</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/课设记录/">课设记录</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/感知机/" style="font-size: 15px;">感知机</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/笔试/" style="font-size: 15px;">笔试</a> <a href="/tags/Jekyll/" style="font-size: 15px;">Jekyll</a> <a href="/tags/blog/" style="font-size: 15px;">blog</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/K近邻/" style="font-size: 15px;">K近邻</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/统计学习方法/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/显卡驱动配置/" style="font-size: 15px;">显卡驱动配置</a> <a href="/tags/ssh配置/" style="font-size: 15px;">ssh配置</a> <a href="/tags/朴素贝叶斯/" style="font-size: 15px;">朴素贝叶斯</a> <a href="/tags/osx/" style="font-size: 15px;">osx</a> <a href="/tags/ssr/" style="font-size: 15px;">ssr</a> <a href="/tags/proxychains/" style="font-size: 15px;">proxychains</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/HMM/" style="font-size: 15px;">HMM</a> <a href="/tags/CWS/" style="font-size: 15px;">CWS</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/11/17/TensorFlow学习手册（四）/">TensorFlow学习手册（四）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/16/通过proxychains在终端使用ss代理/">通过proxychains在终端使用ss代理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/安装cuda9-0和cudnn7-4以及tensorflow-gpu-1-11-0/">安装cuda9.0和cudnn7.4以及tensorflow-gpu==1.11.0</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/TensorFlow学习手册（三）/">TensorFlow学习手册（三）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/10/TensorFlow学习手册（二）/">TensorFlow学习手册（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/09/TensorFlow学习手册（一）/">TensorFlow学习手册（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/31/ubuntu16-04显卡驱动安装及环境配置/">Ubuntu16.04显卡驱动安装及环境配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/28/隐马尔可夫模型及分词上的实现/">隐马尔可夫模型及分词上的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/25/2019科大讯飞算法岗校招笔试/">2019科大讯飞算法岗校招笔试</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/19/决策树的原理及实现/">决策树的原理及实现</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.haomwei.com/technology/maupassant-hexo.html" title="博客模板" target="_blank">博客模板</a><ul></ul><a href="https://hyxxsfwy.github.io/2016/01/15/Hexo-Markdown-%E7%AE%80%E6%98%8E%E8%AF%AD%E6%B3%95%E6%89%8B%E5%86%8C/" title="HEXO markdown简明语法" target="_blank">HEXO markdown简明语法</a><ul></ul><a href="http://www.mohu.org/info/symbols/symbols.htm" title="mathjax使用的查阅手册" target="_blank">mathjax使用的查阅手册</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Netycc's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a><br/><span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
extensions: ["tex2jax.js"],
jax: ["input/TeX", "output/HTML-CSS"],
tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
},
    "HTML-CSS": { fonts: ["TeX"] }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>