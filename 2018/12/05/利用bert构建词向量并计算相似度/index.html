<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="用于日常笔记"><meta name="baidu-site-verification" content="31u13chEy5"><title>利用Bert构建词向量并计算相似度 | Netycc's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '83d60cd5e215de54f53db5b26853c623';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
  </script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">利用Bert构建词向量并计算相似度</h1><a id="logo" href="/.">Netycc's blog</a><p class="description">每天进步一点点，吃吃喝喝的single dog.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/timeline"><i class="fa fa-history"> 时间线</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">利用Bert构建词向量并计算相似度</h1><div class="post-meta">Dec 5, 2018<span> | </span><span class="category"><a href="/categories/技术技巧/">技术技巧</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.3k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 4</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" href="/2018/12/05/利用bert构建词向量并计算相似度/#vcomment"><span class="valine-comment-count" data-xid="/2018/12/05/利用bert构建词向量并计算相似度/"></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bert简介"><span class="toc-number">2.</span> <span class="toc-text">Bert简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bert-as-service"><span class="toc-number">3.</span> <span class="toc-text">Bert-as-service</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#服务器上下载该框架"><span class="toc-number">3.1.</span> <span class="toc-text">服务器上下载该框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#下载预训练模型"><span class="toc-number">3.2.</span> <span class="toc-text">下载预训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#服务器端启动服务"><span class="toc-number">3.3.</span> <span class="toc-text">服务器端启动服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#客户端使用服务"><span class="toc-number">3.4.</span> <span class="toc-text">客户端使用服务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cosine相似度计算"><span class="toc-number">4.</span> <span class="toc-text">cosine相似度计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cos相似度的数学推倒"><span class="toc-number">4.1.</span> <span class="toc-text">cos相似度的数学推倒</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cos相似度的实现"><span class="toc-number">4.2.</span> <span class="toc-text">cos相似度的实现</span></a></li></ol></li></ol></div></div><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&emsp;&emsp;在Bert刚出来的时候就想要尝试一下，但是由于时间的原因，也只是看了一下<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Google的Bert论文</a>，因为水平有限，看的也只是模模糊糊的，对transformer也没有更深入的了解。随着时间的流逝，Google在近期也开源了自己的<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">代码</a>。上周看到了一个dalao写一个可以简单调用的bert框架<a href="https://github.com/hanxiao/bert-as-service" target="_blank" rel="noopener"><code>bert-as-server</code></a>（CS架构），由此可以进行WordEmbedding步骤。感觉很好玩，就尝试了一下并用其去计算语句相似度。</p>
<h2 id="Bert简介"><a href="#Bert简介" class="headerlink" title="Bert简介"></a>Bert简介</h2><blockquote>
<p>以下是本人在看完论文之后的理解，并不保证理解的一定是对的，欢迎指正错误！</p>
</blockquote>
<p>&emsp;&emsp;首先在刚拿到这篇论文的时候，发现好多单词看不懂，比如<code>Transformer</code>，都不知道应该怎么去翻译，然后通过上网查一些资料，知道了它是谷歌创造的一种学习文本中单词之间的上下文关系的注意力机制。BERT模型中，该模型是基于其周围环境（双向）来学习单词的上下文的，它的顺序是双向的。在预训练中，该模型有两个训练任务。<br>&emsp;&emsp;第一个任务是<strong>MLM（掩蔽语言模型）</strong>，它是从完形填空中获得的灵感，在向BERT输入单词序列之前，每个序列中有随机15%的单词被[MASK]令牌替换。然后，该模型试图根据序列中其他non-masked提供的上下文来预测掩蔽词的原始值。它的执行过程是80%的时间，用[MASK]标记单词；10%的时间，用随机词替换单词；10%的时间，保持单词不变。<br>&emsp;&emsp;第二个任务是<strong>NSP（下一句话预测）</strong>具体地，当为每个预训练示例选择句子A和B时，50％的时间B是跟随A的实际下一句子，并且50％的时间是来自语料库的随机句子。</p>
<p><strong>两个步骤</strong></p>
<ul>
<li>第一步骤就是以上所说的<strong>预训练</strong>，通过预训练获得预训练模型，有意思的是，预训练过程是无监督的，也就是说语料库可以说是近乎无限的了，在这一步骤中，google也指出了需要消耗庞大的计算资源，按照google的TPU租赁价格，预训练一次可能会花费几万刀。_(:з」∠)_(我真是个穷人)</li>
<li>第二步是根据我们需要的功能进行<strong>微调</strong>，这一步需要花费的资源很小。</li>
</ul>
<p><strong>PS</strong>：从近期的google的动作可以看出，预训练已经是越来越流行了。</p>
<h2 id="Bert-as-service"><a href="#Bert-as-service" class="headerlink" title="Bert-as-service"></a>Bert-as-service</h2><p>&emsp;&emsp;hanxiao大佬开源出来的<a href="https://github.com/hanxiao/bert-as-service" target="_blank" rel="noopener">bert-as-service框架</a>很适合初学者，因为可以很简单的配置，然后使用上最先进的Bert模型。</p>
<h3 id="服务器上下载该框架"><a href="#服务器上下载该框架" class="headerlink" title="服务器上下载该框架"></a>服务器上下载该框架</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone http<span class="variable">s:</span>//github.<span class="keyword">com</span>/hanxiao/bert-<span class="keyword">as</span>-service</span><br><span class="line"><span class="keyword">cd</span> bert-<span class="keyword">as</span>-service</span><br></pre></td></tr></table></figure>
<h3 id="下载预训练模型"><a href="#下载预训练模型" class="headerlink" title="下载预训练模型"></a>下载预训练模型</h3><p>&emsp;&emsp;在这次实验中，我使用的是</p>
<ul>
<li><a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" target="_blank" rel="noopener"><strong>BERT-Base, Uncased</strong></a>: 12-layer, 768-hidden, 12-heads, 110M parameters</li>
</ul>
<h3 id="服务器端启动服务"><a href="#服务器端启动服务" class="headerlink" title="服务器端启动服务"></a>服务器端启动服务</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python app.py -model_dir uncased_L<span class="number">-12</span>_H<span class="number">-768</span>_A<span class="number">-12</span>/ -num_worker=<span class="number">4</span> &gt; run.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在此介绍一下nohub命令，他可以将进程静默运行，并且不会死掉，而且可以生成日志，很方便，训练过程需要挂载服务器上的时候，亲测很好用！</p>
</blockquote>
<h3 id="客户端使用服务"><a href="#客户端使用服务" class="headerlink" title="客户端使用服务"></a>客户端使用服务</h3><p>&emsp;&emsp;我们所下载到的bert-as-service中的service/，我们还是需要用在客户端上的，因为需要它来构成通信。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from service.client import BertClient</span><br><span class="line">&gt;&gt;&gt; bc = BertClient(ip=<span class="string">"XXXX"</span>)</span><br><span class="line">&gt;&gt;&gt; bc.encode(['First do it', 'then do it right'])</span><br><span class="line">array([[ <span class="number">0.06414033</span>,  <span class="number">0.59846574</span>, <span class="number">-0.1504644</span> , ...,  <span class="number">0.24284995</span>,</span><br><span class="line">        <span class="number">-0.93878424</span>,  <span class="number">0.35514495</span>],</span><br><span class="line">       [ <span class="number">0.22535315</span>,  <span class="number">0.16440475</span>,  <span class="number">0.13435377</span>, ..., <span class="number">-0.23758584</span>,</span><br><span class="line">        <span class="number">-0.7612017</span> ,  <span class="number">0.21298395</span>]], dtype=float32)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;可以看出我们的句子已经转化为向量了，不过这是句子向量，如果需要更加细致的token信息或者词向量需要根据作者思路进行修改了。</p>
<h2 id="cosine相似度计算"><a href="#cosine相似度计算" class="headerlink" title="cosine相似度计算"></a>cosine相似度计算</h2><p>&emsp;&emsp;在我们获得了两个句子的向量之后，我们就可以进行相似度计算的操作了。在此我们使用的cos相似度。</p>
<blockquote>
<p>余弦相似度，又称为余弦相似性，是通过计算两个向量的夹角余弦值来评估他们的相似度。余弦相似度将向量根据坐标值，绘制到向量空间中，如最常见的二维空间。</p>
</blockquote>
<h3 id="cos相似度的数学推倒"><a href="#cos相似度的数学推倒" class="headerlink" title="cos相似度的数学推倒"></a>cos相似度的数学推倒</h3><p>两个向量间的余弦值可以通过使用欧几里得点积公式求出：$$a\bullet b=||a|| ||b||cos\theta$$<br>给定两个属性向量，A和B，其余弦相似性θ由点积和向量长度给出，如下所示：<br><img src="https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D394/sign=20b5db49b7a1cd1101b674298d13c8b0/ac4bd11373f0820282c6ae4646fbfbedab641b76.jpg" alt="cos相似度的计算公式"></p>
<h3 id="cos相似度的实现"><a href="#cos相似度的实现" class="headerlink" title="cos相似度的实现"></a>cos相似度的实现</h3><p><strong>实例</strong><br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from service.client import <span class="type">BertClient</span></span><br><span class="line">import numpy <span class="keyword">as</span> np</span><br><span class="line">bc = <span class="type">BertClient</span>(ip=<span class="string">"XXX"</span>)</span><br><span class="line"></span><br><span class="line">def cosine(a,b):</span><br><span class="line">    return a.dot(b)/(np.linalg.norm(a)*np.linalg.norm(b))</span><br><span class="line"></span><br><span class="line">emb=np.<span class="built_in">array</span>(bc.encode([<span class="symbol">'First</span> <span class="keyword">do</span> it', <span class="symbol">'then</span> <span class="keyword">do</span> it right']))</span><br><span class="line"></span><br><span class="line">print([<span class="symbol">'First</span> <span class="keyword">do</span> it', <span class="symbol">'then</span> <span class="keyword">do</span> it right'],<span class="string">":"</span>,cosine(emb[<span class="number">0</span>],emb[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></p>
<p><strong>结果</strong><br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="symbol">'First</span> <span class="keyword">do</span> it', <span class="symbol">'then</span> <span class="keyword">do</span> it right'] : <span class="number">0.92645866</span></span><br></pre></td></tr></table></figure></p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>netycc</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/12/05/利用bert构建词向量并计算相似度/">https://netycc.com/2018/12/05/利用bert构建词向量并计算相似度/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://netycc.com/2018/12/05/利用bert构建词向量并计算相似度/" data-id="cjpb0tesp001d1z82g8r2h04o" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACx0lEQVR42u3aQXLCMAwF0N7/0nSmq86UhP9lu7B4WTFAgp8XlpD09RVfj5/r9+u/19VdV9/5+/7Vc5JPly48PDy80dLvF33PSJZyv0HJvckWP1khHh4e3jFeGwzW37//leQ5UfDAw8PDeysvObhzcJ5MJ2vAw8PD+3xeDp6VMPDw8PA+n5cf3O0hngeG5K6DtRY8PDy8mDdrgL339fH+Hh4eHt6oq563qfJjPS9GPJYvPDw8vBO8WZOpLROsb1MbBaLJLzw8PLwRb6VtP/s0YSRjWElAwsPDwzvB21uibVtZeUOrLePi4eHhneMlB3QSKmbHd552txuEh4eH9z+8tmSQjwW0QaUNG8MGGB4eHt4CLymqro9Dtbx2+y4DAx4eHt4B3qzwmqTIK2NbeVjaFvfw8PDwVqYJ4pGCWWtqNkZQ34WHh4d3mDcrIswS7pbUFoW31Ujw8PDwglR19rhZ0WGlTByFCjw8PLxjvJVgkKfa+SDCrCg8HLrCw8PDK3n5I5LjuE2mZ2l0u0F4eHh4e3nt0ttyRpuyr2xlVKXGw8PDW+blgHwwq3j06Go3CA8PD+8ELx91Wmnnz5bbDlo9+SYeHh7eVl7Sppolym3CPRsseHEXHh4e3gHerqXMltWm6SsBAw8PD28vrx23yttjbVDJRxmKFB8PDw9vK+++/LpeJkgO93ywoB0ywMPDwzvNyw/uWW+tTanbDX1RgMDDw8PbxFsvH+Q/Njv626bX5RPw8PDwNvEe5ZWXXNsiQrJ9SQHissqCh4eHt4m3fvi2ASBPppNUOw9jeHh4eHt5eTCYlXrbo389qODh4eH9D689stuRqVkheBbNnvxvwMPDw3sTL29E5UWEdvvqYjEeHh7eB/Dapa88Od+yF7VqPDw8vAO8vBjRfnMWcpLkO/oDgIeHh3eAN2uArRdtZ4n75qEBPDw8vI73DdN8AydgBFx0AAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/Bert/">Bert</a><a href="/tags/WordEmbedding/">WordEmbedding</a><a href="/tags/cosine/">cosine</a></div><div class="post-nav"><a class="next" href="/2018/12/04/正则表达式总结/">正则表达式记录</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'true' == true ? true : false;
var verify = 'true' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'dCzd3ozN5OlrLCT1FcrMc8D7-gzGzoHsz',
  appKey:'UmIi39gBjdh1Aria4ssShR31',
  placeholder:'欢迎讨论~',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/刷题记录/">刷题记录</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习笔记/">学习笔记</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术技巧/">技术技巧</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/课设记录/">课设记录</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/ssh配置/" style="font-size: 15px;">ssh配置</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/感知机/" style="font-size: 15px;">感知机</a> <a href="/tags/Jekyll/" style="font-size: 15px;">Jekyll</a> <a href="/tags/blog/" style="font-size: 15px;">blog</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/笔试/" style="font-size: 15px;">笔试</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/K近邻/" style="font-size: 15px;">K近邻</a> <a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/tags/统计学习方法/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/显卡驱动配置/" style="font-size: 15px;">显卡驱动配置</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/Bert/" style="font-size: 15px;">Bert</a> <a href="/tags/WordEmbedding/" style="font-size: 15px;">WordEmbedding</a> <a href="/tags/cosine/" style="font-size: 15px;">cosine</a> <a href="/tags/朴素贝叶斯/" style="font-size: 15px;">朴素贝叶斯</a> <a href="/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/HMM/" style="font-size: 15px;">HMM</a> <a href="/tags/CWS/" style="font-size: 15px;">CWS</a> <a href="/tags/osx/" style="font-size: 15px;">osx</a> <a href="/tags/ssr/" style="font-size: 15px;">ssr</a> <a href="/tags/proxychains/" style="font-size: 15px;">proxychains</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/12/05/利用bert构建词向量并计算相似度/">利用Bert构建词向量并计算相似度</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/04/正则表达式总结/">正则表达式记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/02/LeetCode-1-3/">[LeetCode]Problem 1-3</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/17/TensorFlow学习手册（四）/">TensorFlow学习手册（四）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/16/通过proxychains在终端使用ss代理/">通过proxychains在终端使用ss代理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/安装cuda9-0和cudnn7-4以及tensorflow-gpu-1-11-0/">安装cuda9.0和cudnn7.4以及tensorflow-gpu==1.11.0</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/TensorFlow学习手册（三）/">TensorFlow学习手册（三）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/10/TensorFlow学习手册（二）/">TensorFlow学习手册（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/09/TensorFlow学习手册（一）/">TensorFlow学习手册（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/31/ubuntu16-04显卡驱动安装及环境配置/">Ubuntu16.04显卡驱动安装及环境配置</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.haomwei.com/technology/maupassant-hexo.html" title="博客模板" target="_blank">博客模板</a><ul></ul><a href="https://hyxxsfwy.github.io/2016/01/15/Hexo-Markdown-%E7%AE%80%E6%98%8E%E8%AF%AD%E6%B3%95%E6%89%8B%E5%86%8C/" title="HEXO markdown简明语法" target="_blank">HEXO markdown简明语法</a><ul></ul><a href="http://www.mohu.org/info/symbols/symbols.htm" title="mathjax使用的查阅手册" target="_blank">mathjax使用的查阅手册</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Netycc's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a><br/><span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
extensions: ["tex2jax.js"],
jax: ["input/TeX", "output/HTML-CSS"],
tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
},
    "HTML-CSS": { fonts: ["TeX"] }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>